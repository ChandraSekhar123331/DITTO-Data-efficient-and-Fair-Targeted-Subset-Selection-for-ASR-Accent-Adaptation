{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv, pathlib, ast, os.path\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "\n",
    "accent_short_forms = {\"hindi\":\"HIN\", \"korean\":\"KOR\", \"vietnamese\":\"VTN\", \"arabic\":\"ARB\", \"chinese\":\"CHN\", \"spanish\":\"ESP\"}\n",
    "accent_map = {\"ABA\":\"arabic\",\"SKA\":\"arabic\",\"YBAA\":\"arabic\",\"ZHAA\":\"arabic\",\n",
    "              \"BWC\":\"chinese\",\"LXC\":\"chinese\",\"NCC\":\"chinese\",\"TXHC\":\"chinese\",\n",
    "              \"ASI\":\"hindi\",\"RRBI\":\"hindi\",\"SVBI\":\"hindi\",\"TNI\":\"hindi\",\n",
    "              \"HJK\":\"korean\",\"HKK\":\"korean\",\"YDCK\":\"korean\",\"YKWK\":\"korean\",\n",
    "              \"EBVS\":\"spanish\",\"ERMS\":\"spanish\",\"MBMPS\":\"spanish\",\"NJS\":\"spanish\",\n",
    "              \"HQTV\":\"vietnamese\",\"PNV\":\"vietnamese\",\"THV\":\"vietnamese\",\"TLV\":\"vietnamese\"\n",
    "              }\n",
    "raw_string=\"\"\"|ABA|M|Arabic|1129|150|\\n|SKA|F|Arabic|974|150|\\n|YBAA|M|Arabic|1130|149|\\n|ZHAA|F|Arabic|1132|150|\\n|BWC|M|Chinese|1130|150|\\n|LXC|F|Chinese|1131|150|\\n|NCC|F|Chinese|1131|150|\\n|TXHC|M|Chinese|1132|150|\\n|ASI|M|Hindi|1131|150|\\n|RRBI|M|Hindi|1130|150|\\n|SVBI|F|Hindi|1132|150|\\n|TNI|F|Hindi|1131|150|\\n|HJK|F|Korean|1131|150|\\n|HKK|M|Korean|1131|150|\\n|YDCK|F|Korean|1131|150|\\n|YKWK|M|Korean|1131|150|\\n|EBVS|M|Spanish|1007|150|\\n|ERMS|M|Spanish|1132|150|\\n|MBMPS|F|Spanish|1132|150|\\n|NJS|F|Spanish|1131|150|\\n|HQTV|M|Vietnamese|1132|150|\\n|PNV|F|Vietnamese|1132|150|\\n|THV|F|Vietnamese|1132|150|\\n|TLV|M|Vietnamese|1132|150|\"\"\"\n",
    "raw_strings=raw_string.split('\\n')\n",
    "gender_map={}\n",
    "for lne in raw_strings:\n",
    "    attrs=lne.split('|')\n",
    "    gender_map[attrs[1]]=attrs[2]\n",
    "\n",
    "composed_accent_map = {k: accent_short_forms.get(v) for k, v in accent_map.items()}\n",
    "\n",
    "def replace_with_short_forms(s):\n",
    "    for key, value in accent_short_forms.items():\n",
    "        s = s.replace(key, value)\n",
    "    return s\n",
    "\n",
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "def get_each_run(lne):\n",
    "    return list(map(float, re.findall(': (.+?) -> ', lne)[0].split(' ')))\n",
    "\n",
    "def get_selection_counts(s):\n",
    "    return list(map(replace_with_short_forms, re.findall('Counter\\\\((.+?)\\\\)', s)))\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9]+\"run_{}/output/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "def get_test(stats_file_path):\n",
    "    return stats_file_path[:-9]+\"run_1/output/test_infer_log.txt\"\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    txt_file = open(test_file, 'r')\n",
    "    lines = txt_file.readlines()\n",
    "    matched = \"\"\n",
    "    for line in lines:\n",
    "        if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "            txt_file.close()\n",
    "            return float(line.rstrip().split(\": \")[1])\n",
    "    txt_file.close()\n",
    "    return \"\"\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    return \"-n:\"+str(float(eta[4:]))\n",
    "\n",
    "def clean_cond(fun):\n",
    "    if 'conditional' not in fun:\n",
    "        return fun\n",
    "#     if fun==\"conditional_FLMI_spanish\":\n",
    "#         print(\"reached\")\n",
    "    _,f,acc=fun.split('_')\n",
    "    if acc!='native':\n",
    "        acc=accent_short_forms[acc]\n",
    "    f=f.split('MI')[0]\n",
    "#     if fun==\"conditional_FLMI_spanish\":\n",
    "#         print(\"{}_cond_against_{}\".format(f,acc))\n",
    "    return \"{}_cond_against_{}\".format(f,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 100\n",
    "# target = 50\n",
    "target = 10\n",
    "\n",
    "# budget = 200\n",
    "# target = 80\n",
    "features = '39'\n",
    "# features = 'TRILL'\n",
    "csv_name = \"report_{}_{}_{}.csv\".format(budget, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./hindi/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./hindi/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./hindi/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./chinese/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./chinese/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./chinese/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./spanish/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./spanish/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./spanish/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./arabic/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./arabic/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./arabic/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./korean/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./korean/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./korean/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./vietnamese/manifests/TSS_output_NEW/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./vietnamese/manifests/TSS_output_NEW/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./vietnamese/manifests/TSS_output_NEW/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "no WER's in file ./hindi/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n",
      "no WER's in file ./chinese/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n",
      "no WER's in file ./spanish/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n",
      "no WER's in file ./arabic/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n",
      "no WER's in file ./korean/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n",
      "no WER's in file ./vietnamese/manifests/TSS_output_NEW/all/budget_100/target_10/random/run_1/output/test_infer_log.txt\n"
     ]
    }
   ],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/accent/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "\n",
    "\n",
    "cols = ['accent', 'ground', 'function', 'similarity', 'duration', 'samples', \n",
    "        'WER-r1', 'WER-r2', 'WER-r3', 'WER-mean', 'WER-std-dev', 'accents_run1', 'accents_run2', 'accents_run3']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "\n",
    "accents = [f.name for f in os.scandir('./') if f.is_dir() and f.name != '.ipynb_checkpoints' and f.name != 'reserved_TSS_output']\n",
    "\n",
    "# # within submodular\n",
    "# for accent in accents:\n",
    "# # for accent in ['chinese']:\n",
    "#     if not(pathlib.Path('./{}/manifests/TSS_output/'.format(accent)).is_dir()):\n",
    "#         print(\"no results for accent {}\".format(accent))\n",
    "#         continue\n",
    "#     if 'all' not in get_dirs('./{}/manifests/TSS_output/'.format(accent)):\n",
    "#         print(\"no all results for {}\".format(accent))\n",
    "#         continue\n",
    "#     if not(os.path.isdir('./{}/manifests/TSS_output/within/budget_{}/target_{}/'.format(accent, budget, target))):\n",
    "#         continue\n",
    "#     for function in get_dirs('./{}/manifests/TSS_output/within/budget_{}/target_{}/'.format(accent, budget, target)):\n",
    "# #     for budget in [100,200,300,400,600,800]:\n",
    "# #         function = \"GCMI\"\n",
    "#         if function == \"equal_random\":\n",
    "#             continue\n",
    "# #         for eta in get_dirs('./{}/manifests/TSS_output/within/budget_{}/target_{}/{}/'.format(accent, budget, target, function)):\n",
    "#         for similarity in get_dirs('./{}/manifests/TSS_output/within/budget_{}/target_{}/{}/'.format(accent, budget, target, function)):\n",
    "#             stats_file_path = './{}/manifests/TSS_output/within/budget_{}/target_{}/{}/{}/{}/stats.txt'.format(accent, budget, target, function, similarity, features)  \n",
    "#             stats_file = open(stats_file_path, 'r')\n",
    "#             lines = stats_file.readlines()\n",
    "#             df_selections = get_selection_counts(lines[5])\n",
    "#             total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "#             sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "#             sample_total = mean(total_selections)\n",
    "#             duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "#             duration_total = mean(total_durations)\n",
    "#             df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "#             df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "#             try:\n",
    "#                 wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "#                 df_wer_mean = round(mean(wers), 2)\n",
    "#                 df_wer_var = round(variance(wers), 3)\n",
    "#             except:\n",
    "#                 print(stats_file_path+\" corresponding WER test file not found\")\n",
    "#                 continue\n",
    "#             df = df.append(dict(zip(cols, [accent, \"within\", clean_cond(function), similarity, df_duration, df_samples]+\n",
    "#                                            wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), \n",
    "#                            ignore_index=True)\n",
    "#             stats_file.close()\n",
    "\n",
    "# normal all\n",
    "for accent in accents:\n",
    "# for accent in ['chinese']:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output_NEW/'.format(accent)).is_dir()):\n",
    "        print(\"no results for accent {}\".format(accent))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output_NEW/'.format(accent)):\n",
    "        print(\"no all results for {}\".format(accent))\n",
    "        continue\n",
    "    if not(os.path.isdir('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/'.format(accent, budget, target))):\n",
    "        continue\n",
    "    for function in get_dirs('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/'.format(accent, budget, target)):\n",
    "        if function == \"random\":\n",
    "            continue\n",
    "        for eta in get_dirs('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/{}/'.format(accent, budget, target, function)):\n",
    "            for similarity in get_dirs('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/{}/{}/'.format(accent, budget, target, function, eta)):\n",
    "                stats_file_path = './{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/{}/{}/{}/{}/stats.txt'.format(accent, budget, target, function, eta, similarity, features)  \n",
    "                stats_file = open(stats_file_path, 'r')\n",
    "                # print(stats_file_path)\n",
    "#                 if accent=='arabic':\n",
    "#                     print([accent, \"all\", clean_cond(function)+get_eta(function, eta), similarity, df_duration, df_samples]+\n",
    "#                                                wers+[df_wer_mean, round(df_wer_var**0.5, 3)])\n",
    "                lines = stats_file.readlines()\n",
    "                df_selections = get_selection_counts(lines[5])\n",
    "                total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "                sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "                sample_total = mean(total_selections)\n",
    "                duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "                duration_total = mean(total_durations)\n",
    "                df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "                df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "                try:\n",
    "                    wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                    df_wer_mean = round(mean(wers), 2)\n",
    "                    df_wer_var = round(variance(wers), 3)\n",
    "                except:\n",
    "                    print(stats_file_path+\" corresponding WER test file not found\")\n",
    "#                     continue\n",
    "                df = df.append(dict(zip(cols, [accent, \"all\", clean_cond(function)+get_eta(function, eta), similarity, df_duration, df_samples]+\n",
    "                                               wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), \n",
    "                               ignore_index=True)\n",
    "                stats_file.close()\n",
    "                \n",
    "#random\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/random/run_1/output\n",
    "for accent in accents:\n",
    "# for accent in ['chinese']:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output_NEW/'.format(accent)).is_dir()):\n",
    "        print(\"no results for accent {}\".format(accent))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output_NEW/'.format(accent)):\n",
    "        print(\"no all results for {}\".format(accent))\n",
    "        continue\n",
    "    if not(os.path.isdir('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/'.format(accent, budget, target))):\n",
    "        continue\n",
    "    if \"random\" in get_dirs('./{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/'.format(accent, budget, target)):\n",
    "            stats_file_path = './{}/manifests/TSS_output_NEW/all/budget_{}/target_{}/random/stats.txt'.format(accent, budget, target)\n",
    "            stats_file = open(stats_file_path, 'r')\n",
    "            lines = stats_file.readlines()\n",
    "            df_selections = get_selection_counts(lines[5])\n",
    "            total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "            sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "            sample_total = mean(total_selections)\n",
    "            duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "            duration_total = mean(total_durations)\n",
    "            df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "            df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "            try:\n",
    "                wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                wers = [x for x in wers if type(x)==float or type(x)==int]\n",
    "#                 print(wers)\n",
    "                df_wer_mean = round(mean(wers), 2)\n",
    "#                 print(df_wer_mean)\n",
    "                df_wer_var = round(variance(wers), 3)\n",
    "            except:\n",
    "                print(\"no WER's in file\", get_test_file_from_stats_path(1, stats_file))\n",
    "#                 continue\n",
    "                wers = [0,0,0]\n",
    "                df_wer_mean = 0\n",
    "                df_wer_var = 999\n",
    "#             if accent=='arabic':\n",
    "#                 print(\"random: \",)\n",
    "            df = df.append(dict(zip(cols, [accent, \"all\", \"random\", \"NA\", df_duration, df_samples]+wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), ignore_index=True)\n",
    "            # print([accent, \"all\", \"random\", \"NA\", df_duration, df_samples]+wers\n",
    "            #                                 +[df_wer_mean, df_wer_var])\n",
    "            stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accent</th>\n",
       "      <th>ground</th>\n",
       "      <th>function</th>\n",
       "      <th>similarity</th>\n",
       "      <th>duration</th>\n",
       "      <th>samples</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-std-dev</th>\n",
       "      <th>accents_run1</th>\n",
       "      <th>accents_run2</th>\n",
       "      <th>accents_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>71.24/358.35</td>\n",
       "      <td>19.39/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>348.93/353.97</td>\n",
       "      <td>85.00/86.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...</td>\n",
       "      <td>{'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...</td>\n",
       "      <td>{'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>304.95/357.89</td>\n",
       "      <td>82.00/94.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...</td>\n",
       "      <td>{'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...</td>\n",
       "      <td>{'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>340.44/356.38</td>\n",
       "      <td>94.00/99.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...</td>\n",
       "      <td>{'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...</td>\n",
       "      <td>{'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>53.65/358.35</td>\n",
       "      <td>14.27/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chinese</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>359.75/359.75</td>\n",
       "      <td>85.00/85.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'LXC': 66, 'BWC': 13, 'TXHC': 6}</td>\n",
       "      <td>{'LXC': 66, 'BWC': 13, 'TXHC': 6}</td>\n",
       "      <td>{'LXC': 66, 'BWC': 13, 'TXHC': 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chinese</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>322.62/354.03</td>\n",
       "      <td>70.00/78.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'LXC': 67, 'NJS': 8, 'TXHC': 3}</td>\n",
       "      <td>{'LXC': 67, 'NJS': 8, 'TXHC': 3}</td>\n",
       "      <td>{'LXC': 67, 'NJS': 8, 'TXHC': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chinese</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>350.81/357.64</td>\n",
       "      <td>90.00/92.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...</td>\n",
       "      <td>{'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...</td>\n",
       "      <td>{'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hindi</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>48.98/358.35</td>\n",
       "      <td>16.01/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hindi</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>353.83/359.20</td>\n",
       "      <td>109.00/110.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...</td>\n",
       "      <td>{'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...</td>\n",
       "      <td>{'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hindi</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>217.95/357.90</td>\n",
       "      <td>73.00/103.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...</td>\n",
       "      <td>{'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...</td>\n",
       "      <td>{'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hindi</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>289.27/359.56</td>\n",
       "      <td>99.00/121.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...</td>\n",
       "      <td>{'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...</td>\n",
       "      <td>{'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>korean</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>59.72/358.35</td>\n",
       "      <td>15.72/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>korean</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>357.93/357.93</td>\n",
       "      <td>87.00/87.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'YKWK': 45, 'YDCK': 32, 'HJK': 10}</td>\n",
       "      <td>{'YKWK': 45, 'YDCK': 32, 'HJK': 10}</td>\n",
       "      <td>{'YKWK': 45, 'YDCK': 32, 'HJK': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>korean</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>359.47/359.47</td>\n",
       "      <td>86.00/86.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'YKWK': 86}</td>\n",
       "      <td>{'YKWK': 86}</td>\n",
       "      <td>{'YKWK': 86}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>korean</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>353.69/359.22</td>\n",
       "      <td>95.00/96.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}</td>\n",
       "      <td>{'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}</td>\n",
       "      <td>{'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spanish</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>61.81/358.35</td>\n",
       "      <td>16.67/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spanish</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>354.22/359.25</td>\n",
       "      <td>81.00/82.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...</td>\n",
       "      <td>{'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...</td>\n",
       "      <td>{'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spanish</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>328.43/355.81</td>\n",
       "      <td>70.00/77.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...</td>\n",
       "      <td>{'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...</td>\n",
       "      <td>{'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spanish</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>323.34/358.85</td>\n",
       "      <td>84.00/94.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...</td>\n",
       "      <td>{'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...</td>\n",
       "      <td>{'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>62.95/358.35</td>\n",
       "      <td>15.60/97.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...</td>\n",
       "      <td>{'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...</td>\n",
       "      <td>{'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>358.65/358.65</td>\n",
       "      <td>84.00/84.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'TLV': 35, 'HQTV': 32, 'THV': 17}</td>\n",
       "      <td>{'TLV': 35, 'HQTV': 32, 'THV': 17}</td>\n",
       "      <td>{'TLV': 35, 'HQTV': 32, 'THV': 17}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>355.97/355.97</td>\n",
       "      <td>91.00/91.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'HQTV': 91}</td>\n",
       "      <td>{'HQTV': 91}</td>\n",
       "      <td>{'HQTV': 91}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>351.58/359.31</td>\n",
       "      <td>96.00/98.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.607</td>\n",
       "      <td>{'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...</td>\n",
       "      <td>{'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...</td>\n",
       "      <td>{'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accent ground      function similarity       duration        samples  \\\n",
       "0       arabic    all        random         NA   71.24/358.35    19.39/97.67   \n",
       "1       arabic    all   FL2MI-n:1.0  euclidean  348.93/353.97    85.00/86.00   \n",
       "2       arabic    all    GCMI-n:1.0  euclidean  304.95/357.89    82.00/94.00   \n",
       "3       arabic    all  LogDMI-n:1.0  euclidean  340.44/356.38    94.00/99.00   \n",
       "4      chinese    all        random         NA   53.65/358.35    14.27/97.67   \n",
       "5      chinese    all   FL2MI-n:1.0  euclidean  359.75/359.75    85.00/85.00   \n",
       "6      chinese    all    GCMI-n:1.0  euclidean  322.62/354.03    70.00/78.00   \n",
       "7      chinese    all  LogDMI-n:1.0  euclidean  350.81/357.64    90.00/92.00   \n",
       "8        hindi    all        random         NA   48.98/358.35    16.01/97.67   \n",
       "9        hindi    all   FL2MI-n:1.0  euclidean  353.83/359.20  109.00/110.00   \n",
       "10       hindi    all    GCMI-n:1.0  euclidean  217.95/357.90   73.00/103.00   \n",
       "11       hindi    all  LogDMI-n:1.0  euclidean  289.27/359.56   99.00/121.00   \n",
       "12      korean    all        random         NA   59.72/358.35    15.72/97.67   \n",
       "13      korean    all   FL2MI-n:1.0  euclidean  357.93/357.93    87.00/87.00   \n",
       "14      korean    all    GCMI-n:1.0  euclidean  359.47/359.47    86.00/86.00   \n",
       "15      korean    all  LogDMI-n:1.0  euclidean  353.69/359.22    95.00/96.00   \n",
       "16     spanish    all        random         NA   61.81/358.35    16.67/97.67   \n",
       "17     spanish    all   FL2MI-n:1.0  euclidean  354.22/359.25    81.00/82.00   \n",
       "18     spanish    all    GCMI-n:1.0  euclidean  328.43/355.81    70.00/77.00   \n",
       "19     spanish    all  LogDMI-n:1.0  euclidean  323.34/358.85    84.00/94.00   \n",
       "20  vietnamese    all        random         NA   62.95/358.35    15.60/97.67   \n",
       "21  vietnamese    all   FL2MI-n:1.0  euclidean  358.65/358.65    84.00/84.00   \n",
       "22  vietnamese    all    GCMI-n:1.0  euclidean  355.97/355.97    91.00/91.00   \n",
       "23  vietnamese    all  LogDMI-n:1.0  euclidean  351.58/359.31    96.00/98.00   \n",
       "\n",
       "   WER-r1 WER-r2 WER-r3 WER-mean  WER-std-dev  \\\n",
       "0       0      0      0        0       31.607   \n",
       "1       0      0      0        0       31.607   \n",
       "2       0      0      0        0       31.607   \n",
       "3       0      0      0        0       31.607   \n",
       "4       0      0      0        0       31.607   \n",
       "5       0      0      0        0       31.607   \n",
       "6       0      0      0        0       31.607   \n",
       "7       0      0      0        0       31.607   \n",
       "8       0      0      0        0       31.607   \n",
       "9       0      0      0        0       31.607   \n",
       "10      0      0      0        0       31.607   \n",
       "11      0      0      0        0       31.607   \n",
       "12      0      0      0        0       31.607   \n",
       "13      0      0      0        0       31.607   \n",
       "14      0      0      0        0       31.607   \n",
       "15      0      0      0        0       31.607   \n",
       "16      0      0      0        0       31.607   \n",
       "17      0      0      0        0       31.607   \n",
       "18      0      0      0        0       31.607   \n",
       "19      0      0      0        0       31.607   \n",
       "20      0      0      0        0       31.607   \n",
       "21      0      0      0        0       31.607   \n",
       "22      0      0      0        0       31.607   \n",
       "23      0      0      0        0       31.607   \n",
       "\n",
       "                                         accents_run1  \\\n",
       "0   {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "1   {'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...   \n",
       "2   {'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...   \n",
       "3   {'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...   \n",
       "4   {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "5                   {'LXC': 66, 'BWC': 13, 'TXHC': 6}   \n",
       "6                    {'LXC': 67, 'NJS': 8, 'TXHC': 3}   \n",
       "7   {'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...   \n",
       "8   {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "9   {'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...   \n",
       "10  {'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...   \n",
       "11  {'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...   \n",
       "12  {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "13                {'YKWK': 45, 'YDCK': 32, 'HJK': 10}   \n",
       "14                                       {'YKWK': 86}   \n",
       "15      {'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}   \n",
       "16  {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "17  {'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...   \n",
       "18  {'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...   \n",
       "19  {'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...   \n",
       "20  {'TLV': 11, 'NJS': 8, 'HQTV': 7, 'ASI': 6, 'NC...   \n",
       "21                 {'TLV': 35, 'HQTV': 32, 'THV': 17}   \n",
       "22                                       {'HQTV': 91}   \n",
       "23  {'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...   \n",
       "\n",
       "                                         accents_run2  \\\n",
       "0   {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "1   {'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...   \n",
       "2   {'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...   \n",
       "3   {'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...   \n",
       "4   {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "5                   {'LXC': 66, 'BWC': 13, 'TXHC': 6}   \n",
       "6                    {'LXC': 67, 'NJS': 8, 'TXHC': 3}   \n",
       "7   {'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...   \n",
       "8   {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "9   {'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...   \n",
       "10  {'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...   \n",
       "11  {'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...   \n",
       "12  {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "13                {'YKWK': 45, 'YDCK': 32, 'HJK': 10}   \n",
       "14                                       {'YKWK': 86}   \n",
       "15      {'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}   \n",
       "16  {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "17  {'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...   \n",
       "18  {'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...   \n",
       "19  {'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...   \n",
       "20  {'YBAA': 7, 'ABA': 7, 'YDCK': 6, 'RRBI': 6, 'Y...   \n",
       "21                 {'TLV': 35, 'HQTV': 32, 'THV': 17}   \n",
       "22                                       {'HQTV': 91}   \n",
       "23  {'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...   \n",
       "\n",
       "                                         accents_run3  \n",
       "0   {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "1   {'ABA': 41, 'ZHAA': 31, 'SKA': 9, 'YBAA': 4, '...  \n",
       "2   {'SKA': 73, 'YDCK': 11, 'ABA': 5, 'YBAA': 4, '...  \n",
       "3   {'SKA': 33, 'ABA': 21, 'ZHAA': 21, 'YBAA': 19,...  \n",
       "4   {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "5                   {'LXC': 66, 'BWC': 13, 'TXHC': 6}  \n",
       "6                    {'LXC': 67, 'NJS': 8, 'TXHC': 3}  \n",
       "7   {'TXHC': 39, 'LXC': 29, 'BWC': 21, 'NCC': 1, '...  \n",
       "8   {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "9   {'SVBI': 71, 'TNI': 23, 'RRBI': 8, 'ASI': 7, '...  \n",
       "10  {'SVBI': 65, 'ABA': 20, 'TNI': 7, 'SKA': 4, 'E...  \n",
       "11  {'SVBI': 32, 'ASI': 31, 'TNI': 18, 'RRBI': 18,...  \n",
       "12  {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "13                {'YKWK': 45, 'YDCK': 32, 'HJK': 10}  \n",
       "14                                       {'YKWK': 86}  \n",
       "15      {'YKWK': 43, 'HJK': 41, 'YDCK': 11, 'ABA': 1}  \n",
       "16  {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "17  {'ERMS': 54, 'EBVS': 13, 'NJS': 12, 'MBMPS': 2...  \n",
       "18  {'ERMS': 66, 'SKA': 5, 'NJS': 2, 'EBVS': 2, 'T...  \n",
       "19  {'EBVS': 30, 'NJS': 26, 'ERMS': 21, 'MBMPS': 7...  \n",
       "20  {'RRBI': 8, 'TXHC': 7, 'NJS': 7, 'ZHAA': 7, 'Y...  \n",
       "21                 {'TLV': 35, 'HQTV': 32, 'THV': 17}  \n",
       "22                                       {'HQTV': 91}  \n",
       "23  {'HQTV': 39, 'THV': 30, 'TLV': 27, 'YDCK': 1, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sort_values(by=['accent', 'similarity', 'ground', 'function'], ascending=True, ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# accented selection: 76 76 76 -> 76.00\n",
    "# accented duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "#Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mayank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./korean/manifests/TSS_output/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./korean/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n",
      "./korean/manifests/TSS_output/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt corresponding WER test file not found\n"
     ]
    }
   ],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/accent/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "\n",
    "\n",
    "cols = ['accent', 'ground', 'function', 'similarity', 'duration', 'samples', \n",
    "        'WER-r1', 'WER-r2', 'WER-r3', 'WER-mean', 'WER-var', 'accents_run1', 'accents_run2', 'accents_run3']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "\n",
    "accents = [f.name for f in os.scandir('./') if f.is_dir() and f.name != '.ipynb_checkpoints' and f.name != 'reserved_TSS_output']\n",
    "\n",
    "for accent in accents:\n",
    "# for accent in ['chinese']:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(accent)).is_dir()):\n",
    "        print(\"no results for accent {}\".format(accent))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output/'.format(accent)):\n",
    "        print(\"no all results for {}\".format(accent))\n",
    "        continue\n",
    "    if not(os.path.isdir('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(accent, budget, target))):\n",
    "        continue\n",
    "    for function in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(accent, budget, target)):\n",
    "#     for budget in [100,200,300,400,600,800]:\n",
    "#         function = \"GCMI\"\n",
    "        if function == \"random\":\n",
    "            continue\n",
    "        for eta in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/{}/'.format(accent, budget, target, function)):\n",
    "            for similarity in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/{}/{}/'.format(accent, budget, target, function, eta)):\n",
    "                stats_file_path = './{}/manifests/TSS_output/all/budget_{}/target_{}/{}/{}/{}/{}/stats.txt'.format(accent, budget, target, function, eta, similarity, features)  \n",
    "                stats_file = open(stats_file_path, 'r')\n",
    "                # print(stats_file_path)\n",
    "                lines = stats_file.readlines()\n",
    "                df_selections = get_selection_counts(lines[5])\n",
    "                total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "                sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "                sample_total = mean(total_selections)\n",
    "                duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "                duration_total = mean(total_durations)\n",
    "                df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "                df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "                try:\n",
    "                    wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                    df_wer_mean = round(mean(wers), 2)\n",
    "                    df_wer_var = round(variance(wers), 3)\n",
    "                except:\n",
    "                    print(stats_file_path+\" corresponding WER test file not found\")\n",
    "                    continue\n",
    "                df = df.append(dict(zip(cols, [accent, \"all\", function+get_eta(function, eta), similarity, df_duration, df_samples]+\n",
    "                                               wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), \n",
    "                               ignore_index=True)\n",
    "                stats_file.close()\n",
    "\n",
    "                \n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/random/run_1/output\n",
    "for accent in accents:\n",
    "# for accent in ['chinese']:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(accent)).is_dir()):\n",
    "        print(\"no results for accent {}\".format(accent))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output/'.format(accent)):\n",
    "        print(\"no all results for {}\".format(accent))\n",
    "        continue\n",
    "    if not(os.path.isdir('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(accent, budget, target))):\n",
    "        continue\n",
    "    if \"random\" in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(accent, budget, target)):\n",
    "            stats_file_path = './{}/manifests/TSS_output/all/budget_{}/target_{}/random/stats.txt'.format(accent, budget, target)\n",
    "            stats_file = open(stats_file_path, 'r')\n",
    "            lines = stats_file.readlines()\n",
    "            df_selections = get_selection_counts(lines[5])\n",
    "            total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "            sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "            sample_total = mean(total_selections)\n",
    "            duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "            duration_total = mean(total_durations)\n",
    "            df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "            df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "            try:\n",
    "                wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                wers = [x for x in wers if type(x)==float or type(x)==int]\n",
    "#                 print(wers)\n",
    "                df_wer_mean = round(mean(wers), 2)\n",
    "#                 print(df_wer_mean)\n",
    "                df_wer_var = round(variance(wers), 3)\n",
    "            except:\n",
    "                print(\"no WER's in file\", get_test_file_from_stats_path(1, stats_file))\n",
    "                continue\n",
    "                wers = [0,0,0]\n",
    "                df_wer_mean = 0\n",
    "                df_wer_var = 999\n",
    "            df = df.append(dict(zip(cols, [accent, \"all\", function, \"NA\", df_duration, df_samples]+wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), ignore_index=True)\n",
    "            # print([accent, \"all\", \"random\", \"NA\", df_duration, df_samples]+wers\n",
    "            #                                 +[df_wer_mean, df_wer_var])\n",
    "            stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accent</th>\n",
       "      <th>ground</th>\n",
       "      <th>function</th>\n",
       "      <th>similarity</th>\n",
       "      <th>duration</th>\n",
       "      <th>samples</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-var</th>\n",
       "      <th>accents_run1</th>\n",
       "      <th>accents_run2</th>\n",
       "      <th>accents_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_FLMI_spanish</td>\n",
       "      <td>NA</td>\n",
       "      <td>71.24/358.35</td>\n",
       "      <td>19.39/97.67</td>\n",
       "      <td>23.83</td>\n",
       "      <td>23.47</td>\n",
       "      <td>24.58</td>\n",
       "      <td>23.96</td>\n",
       "      <td>0.567</td>\n",
       "      <td>{'VTN': 22, 'CHN': 18, 'ESP': 18, 'HIN': 14, '...</td>\n",
       "      <td>{'ARB': 24, 'KOR': 20, 'HIN': 17, 'ESP': 16, '...</td>\n",
       "      <td>{'ARB': 20, 'HIN': 17, 'VTN': 16, 'CHN': 16, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>FL1MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>248.28/359.26</td>\n",
       "      <td>65.00/95.00</td>\n",
       "      <td>22.79</td>\n",
       "      <td>22.88</td>\n",
       "      <td>23.14</td>\n",
       "      <td>22.94</td>\n",
       "      <td>0.182</td>\n",
       "      <td>{'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...</td>\n",
       "      <td>{'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...</td>\n",
       "      <td>{'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>348.93/353.97</td>\n",
       "      <td>85.00/86.00</td>\n",
       "      <td>22.38</td>\n",
       "      <td>22.38</td>\n",
       "      <td>22.39</td>\n",
       "      <td>22.38</td>\n",
       "      <td>0.000</td>\n",
       "      <td>{'ARB': 85, 'KOR': 1}</td>\n",
       "      <td>{'ARB': 85, 'KOR': 1}</td>\n",
       "      <td>{'ARB': 85, 'KOR': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>304.95/357.89</td>\n",
       "      <td>82.00/94.00</td>\n",
       "      <td>22.64</td>\n",
       "      <td>22.60</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.49</td>\n",
       "      <td>0.221</td>\n",
       "      <td>{'ARB': 82, 'KOR': 11, 'CHN': 1}</td>\n",
       "      <td>{'ARB': 82, 'KOR': 11, 'CHN': 1}</td>\n",
       "      <td>{'ARB': 82, 'KOR': 11, 'CHN': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arabic</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>340.44/356.38</td>\n",
       "      <td>94.00/99.00</td>\n",
       "      <td>21.91</td>\n",
       "      <td>22.39</td>\n",
       "      <td>21.93</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.272</td>\n",
       "      <td>{'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...</td>\n",
       "      <td>{'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...</td>\n",
       "      <td>{'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_LogDMI_arabic-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>357.55/357.55</td>\n",
       "      <td>99.00/99.00</td>\n",
       "      <td>32.83</td>\n",
       "      <td>32.64</td>\n",
       "      <td>32.88</td>\n",
       "      <td>32.78</td>\n",
       "      <td>0.126</td>\n",
       "      <td>{'VTN': 99}</td>\n",
       "      <td>{'VTN': 99}</td>\n",
       "      <td>{'VTN': 99}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_LogDMI_chinese-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>350.35/355.29</td>\n",
       "      <td>96.00/97.00</td>\n",
       "      <td>33.02</td>\n",
       "      <td>33.03</td>\n",
       "      <td>33.02</td>\n",
       "      <td>33.02</td>\n",
       "      <td>0.000</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_LogDMI_hindi-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>348.17/355.89</td>\n",
       "      <td>96.00/98.00</td>\n",
       "      <td>32.94</td>\n",
       "      <td>32.94</td>\n",
       "      <td>32.81</td>\n",
       "      <td>32.90</td>\n",
       "      <td>0.077</td>\n",
       "      <td>{'VTN': 96, 'KOR': 1, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'KOR': 1, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'KOR': 1, 'ARB': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_LogDMI_korean-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>353.58/358.52</td>\n",
       "      <td>98.00/99.00</td>\n",
       "      <td>33.02</td>\n",
       "      <td>33.10</td>\n",
       "      <td>32.98</td>\n",
       "      <td>33.03</td>\n",
       "      <td>0.063</td>\n",
       "      <td>{'VTN': 98, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 98, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 98, 'ARB': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>all</td>\n",
       "      <td>conditional_LogDMI_spanish-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>352.84/357.78</td>\n",
       "      <td>96.00/97.00</td>\n",
       "      <td>32.79</td>\n",
       "      <td>32.66</td>\n",
       "      <td>32.67</td>\n",
       "      <td>32.71</td>\n",
       "      <td>0.071</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "      <td>{'VTN': 96, 'ARB': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        accent ground                          function similarity  \\\n",
       "0       arabic    all          conditional_FLMI_spanish         NA   \n",
       "1       arabic    all                       FL1MI-n:1.0  euclidean   \n",
       "2       arabic    all                       FL2MI-n:1.0  euclidean   \n",
       "3       arabic    all                        GCMI-n:1.0  euclidean   \n",
       "4       arabic    all                      LogDMI-n:1.0  euclidean   \n",
       "..         ...    ...                               ...        ...   \n",
       "82  vietnamese    all   conditional_LogDMI_arabic-n:1.0  euclidean   \n",
       "83  vietnamese    all  conditional_LogDMI_chinese-n:1.0  euclidean   \n",
       "84  vietnamese    all    conditional_LogDMI_hindi-n:1.0  euclidean   \n",
       "85  vietnamese    all   conditional_LogDMI_korean-n:1.0  euclidean   \n",
       "86  vietnamese    all  conditional_LogDMI_spanish-n:1.0  euclidean   \n",
       "\n",
       "         duration      samples  WER-r1  WER-r2  WER-r3  WER-mean  WER-var  \\\n",
       "0    71.24/358.35  19.39/97.67   23.83   23.47   24.58     23.96    0.567   \n",
       "1   248.28/359.26  65.00/95.00   22.79   22.88   23.14     22.94    0.182   \n",
       "2   348.93/353.97  85.00/86.00   22.38   22.38   22.39     22.38    0.000   \n",
       "3   304.95/357.89  82.00/94.00   22.64   22.60   22.24     22.49    0.221   \n",
       "4   340.44/356.38  94.00/99.00   21.91   22.39   21.93     22.08    0.272   \n",
       "..            ...          ...     ...     ...     ...       ...      ...   \n",
       "82  357.55/357.55  99.00/99.00   32.83   32.64   32.88     32.78    0.126   \n",
       "83  350.35/355.29  96.00/97.00   33.02   33.03   33.02     33.02    0.000   \n",
       "84  348.17/355.89  96.00/98.00   32.94   32.94   32.81     32.90    0.077   \n",
       "85  353.58/358.52  98.00/99.00   33.02   33.10   32.98     33.03    0.063   \n",
       "86  352.84/357.78  96.00/97.00   32.79   32.66   32.67     32.71    0.071   \n",
       "\n",
       "                                         accents_run1  \\\n",
       "0   {'VTN': 22, 'CHN': 18, 'ESP': 18, 'HIN': 14, '...   \n",
       "1   {'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...   \n",
       "2                               {'ARB': 85, 'KOR': 1}   \n",
       "3                    {'ARB': 82, 'KOR': 11, 'CHN': 1}   \n",
       "4   {'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...   \n",
       "..                                                ...   \n",
       "82                                        {'VTN': 99}   \n",
       "83                              {'VTN': 96, 'ARB': 1}   \n",
       "84                    {'VTN': 96, 'KOR': 1, 'ARB': 1}   \n",
       "85                              {'VTN': 98, 'ARB': 1}   \n",
       "86                              {'VTN': 96, 'ARB': 1}   \n",
       "\n",
       "                                         accents_run2  \\\n",
       "0   {'ARB': 24, 'KOR': 20, 'HIN': 17, 'ESP': 16, '...   \n",
       "1   {'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...   \n",
       "2                               {'ARB': 85, 'KOR': 1}   \n",
       "3                    {'ARB': 82, 'KOR': 11, 'CHN': 1}   \n",
       "4   {'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...   \n",
       "..                                                ...   \n",
       "82                                        {'VTN': 99}   \n",
       "83                              {'VTN': 96, 'ARB': 1}   \n",
       "84                    {'VTN': 96, 'KOR': 1, 'ARB': 1}   \n",
       "85                              {'VTN': 98, 'ARB': 1}   \n",
       "86                              {'VTN': 96, 'ARB': 1}   \n",
       "\n",
       "                                         accents_run3  \n",
       "0   {'ARB': 20, 'HIN': 17, 'VTN': 16, 'CHN': 16, '...  \n",
       "1   {'ARB': 65, 'VTN': 15, 'HIN': 7, 'CHN': 3, 'KO...  \n",
       "2                               {'ARB': 85, 'KOR': 1}  \n",
       "3                    {'ARB': 82, 'KOR': 11, 'CHN': 1}  \n",
       "4   {'ARB': 94, 'HIN': 2, 'KOR': 1, 'ESP': 1, 'VTN...  \n",
       "..                                                ...  \n",
       "82                                        {'VTN': 99}  \n",
       "83                              {'VTN': 96, 'ARB': 1}  \n",
       "84                    {'VTN': 96, 'KOR': 1, 'ARB': 1}  \n",
       "85                              {'VTN': 98, 'ARB': 1}  \n",
       "86                              {'VTN': 96, 'ARB': 1}  \n",
       "\n",
       "[87 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sort_values(by=['accent', 'similarity', 'ground', 'function'], ascending=True, ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
