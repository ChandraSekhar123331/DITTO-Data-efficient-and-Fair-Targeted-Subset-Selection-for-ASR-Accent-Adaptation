{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv, pathlib, ast\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "\n",
    "accent_short_forms = {\"hindi\":\"HIN\", \"korean\":\"KOR\", \"vietnamese\":\"VTN\", \"arabic\":\"ARB\", \"chinese\":\"CHN\", \"spanish\":\"ESP\"}\n",
    "accent_map = {\"ABA\":\"arabic\",\"SKA\":\"arabic\",\"YBAA\":\"arabic\",\"ZHAA\":\"arabic\",\n",
    "              \"BWC\":\"chinese\",\"LXC\":\"chinese\",\"NCC\":\"chinese\",\"TXHC\":\"chinese\",\n",
    "              \"ASI\":\"hindi\",\"RRBI\":\"hindi\",\"SVBI\":\"hindi\",\"TNI\":\"hindi\",\n",
    "              \"HJK\":\"korean\",\"HKK\":\"korean\",\"YDCK\":\"korean\",\"YKWK\":\"korean\",\n",
    "              \"EBVS\":\"spanish\",\"ERMS\":\"spanish\",\"MBMPS\":\"spanish\",\"NJS\":\"spanish\",\n",
    "              \"HQTV\":\"vietnamese\",\"PNV\":\"vietnamese\",\"THV\":\"vietnamese\",\"TLV\":\"vietnamese\"\n",
    "              }\n",
    "raw_string=\"\"\"|ABA|M|Arabic|1129|150|\\n|SKA|F|Arabic|974|150|\\n|YBAA|M|Arabic|1130|149|\\n|ZHAA|F|Arabic|1132|150|\\n|BWC|M|Chinese|1130|150|\\n|LXC|F|Chinese|1131|150|\\n|NCC|F|Chinese|1131|150|\\n|TXHC|M|Chinese|1132|150|\\n|ASI|M|Hindi|1131|150|\\n|RRBI|M|Hindi|1130|150|\\n|SVBI|F|Hindi|1132|150|\\n|TNI|F|Hindi|1131|150|\\n|HJK|F|Korean|1131|150|\\n|HKK|M|Korean|1131|150|\\n|YDCK|F|Korean|1131|150|\\n|YKWK|M|Korean|1131|150|\\n|EBVS|M|Spanish|1007|150|\\n|ERMS|M|Spanish|1132|150|\\n|MBMPS|F|Spanish|1132|150|\\n|NJS|F|Spanish|1131|150|\\n|HQTV|M|Vietnamese|1132|150|\\n|PNV|F|Vietnamese|1132|150|\\n|THV|F|Vietnamese|1132|150|\\n|TLV|M|Vietnamese|1132|150|\"\"\"\n",
    "raw_strings=raw_string.split('\\n')\n",
    "gender_map={}\n",
    "for lne in raw_strings:\n",
    "    attrs=lne.split('|')\n",
    "    gender_map[attrs[1]]=attrs[2]\n",
    "\n",
    "composed_accent_map = {k: accent_short_forms.get(v) for k, v in accent_map.items()}\n",
    "\n",
    "def replace_with_short_forms(s):\n",
    "    for key, value in accent_short_forms.items():\n",
    "        s = s.replace(key, value)\n",
    "    return s\n",
    "\n",
    "def group_speakers(s):\n",
    "    ret = {}\n",
    "    speaker_counts = ast.literal_eval(s)\n",
    "    for speaker, count in speaker_counts.items():\n",
    "        accent = composed_accent_map[speaker]\n",
    "        if accent not in ret:\n",
    "            ret[accent] = {}\n",
    "        ret[accent][speaker] = count\n",
    "    return str(ret)\n",
    "\n",
    "def group_accents(s):\n",
    "    # print(s)\n",
    "    ret = {}\n",
    "    speaker_counts = ast.literal_eval(s)\n",
    "    for speaker, count in speaker_counts.items():\n",
    "        accent = composed_accent_map[speaker]\n",
    "        if accent not in ret:\n",
    "            ret[accent] = {}\n",
    "        ret[accent][speaker] = count\n",
    "    # print(ret)\n",
    "    accent_counts={}\n",
    "    for accent in ret:\n",
    "        cnt=0\n",
    "        for speaker in ret[accent]:\n",
    "            cnt+=ret[accent][speaker]\n",
    "        accent_counts[accent]=cnt\n",
    "    # print(accent_counts)\n",
    "    return str(accent_counts)\n",
    "\n",
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "def get_each_run(lne):\n",
    "    return list(map(float, re.findall(': (.+?) -> ', lne)[0].split(' ')))\n",
    "\n",
    "def get_selection_counts(s):\n",
    "    return list(map(group_accents, re.findall('Counter\\\\((.+?)\\\\)', s)))+list(map(group_speakers, re.findall('Counter\\\\((.+?)\\\\)', s)))\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9]+\"run_{}/output/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    txt_file = open(test_file, 'r')\n",
    "    lines = txt_file.readlines()\n",
    "    matched = \"\"\n",
    "    for line in lines:\n",
    "        if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "            txt_file.close()\n",
    "            return float(line.rstrip().split(\": \")[1])\n",
    "    txt_file.close()\n",
    "    return \"\"\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    return \"-n:\"+str(float(eta[4:]))\n",
    "\n",
    "def clean_cond(fun):\n",
    "    if 'conditional' not in fun:\n",
    "        return fun\n",
    "    _,f,acc=fun.split('_')\n",
    "    acc=accent_short_forms[acc]\n",
    "    f=f.split('MI')[0]\n",
    "    return \"{}_cond_against_{}\".format(f,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 100\n",
    "# target = 50\n",
    "target = 10\n",
    "features = '39'\n",
    "# budget, target, features = 100, 50, 'TRILL'\n",
    "csv_name = \"report_{}_{}_{}.csv\".format(budget, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./YKWK/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./YKWK/manifests/TSS_output/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt\n",
      "./YDCK/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./TNI/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./YBAA/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./YBAA/manifests/TSS_output/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt\n",
      "./YBAA/manifests/TSS_output/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt\n",
      "./TXHC/manifests/TSS_output/all/budget_100/target_10/FL2MI/eta_1.0/euclidean/39/stats.txt\n",
      "./TXHC/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./TXHC/manifests/TSS_output/all/budget_100/target_10/GCMI/eta_1.0/euclidean/39/stats.txt\n",
      "./TXHC/manifests/TSS_output/all/budget_100/target_10/LogDMI/eta_1.0/euclidean/39/stats.txt\n",
      "./RRBI/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./TLV/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./SKA/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n",
      "./PNV/manifests/TSS_output/all/budget_100/target_10/FL1MI/eta_1.0/euclidean/39/stats.txt\n"
     ]
    }
   ],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/accent/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "\n",
    "\n",
    "cols = ['speaker', 'ground', 'function', 'similarity', 'duration', 'samples', \n",
    "        'WER-r1', 'WER-r2', 'WER-r3', 'WER-mean', 'WER-var', 'accents_run1', 'accents_run2', 'accents_run3', 'speakers_run1', 'speakers_run2', 'speakers_run3']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "\n",
    "speakers = [f.name for f in os.scandir('./') if f.is_dir() and f.name != '.ipynb_checkpoints' and f.name != 'reserved_TSS_output' and f.name in accent_map]\n",
    "\n",
    "# within submodular\n",
    "for speaker in speakers:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(speaker)).is_dir()):\n",
    "        print(\"no results for speaker {}\".format(speaker))\n",
    "        continue\n",
    "    if 'within' not in get_dirs('./{}/manifests/TSS_output/'.format(speaker)):\n",
    "        print(\"no all results for {}\".format(speaker))\n",
    "        continue\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/within/budget_{}/target_{}/'.format(speaker, budget, target)).is_dir()):\n",
    "        print(\"no results for budget {} and target {}\".format(budget, target))\n",
    "        continue\n",
    "    for function in get_dirs('./{}/manifests/TSS_output/within/budget_{}/target_{}/'.format(speaker, budget, target)):\n",
    "        if function == \"equal_random\" or function == speaker or function == \"random\" or function == \"other_random\":\n",
    "            continue\n",
    "#         for eta in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/{}/'.format(speaker, budget, target, function)):\n",
    "        for similarity in get_dirs('./{}/manifests/TSS_output/within/budget_{}/target_{}/{}/'.format(speaker, budget, target, function)):\n",
    "            stats_file_path = './{}/manifests/TSS_output/within/budget_{}/target_{}/{}/{}/{}/stats.txt'.format(speaker, budget, target, function, similarity, features)  \n",
    "#                 try:\n",
    "#                     stats_file = open(stats_file_path, 'r')\n",
    "#                 except:\n",
    "#                     continue                                                                                          \n",
    "            stats_file = open(stats_file_path, 'r')\n",
    "            lines = stats_file.readlines()\n",
    "#             df_selections = get_selection_counts(lines[5])\n",
    "#             total_selections, total_durations, speakered_selections, speakered_durations = map(get_each_run, lines[:4])\n",
    "#             sample_frac = mean([x[0]/x[1] for x in zip(speakered_selections, total_selections)])\n",
    "#             sample_total = mean(total_selections)\n",
    "#             duration_frac = mean([x[0]/x[1] for x in zip(speakered_durations, total_durations)])\n",
    "#             duration_total = mean(total_durations)\n",
    "#             df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "#             df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "            try:\n",
    "                wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                df_wer_mean = round(mean(wers), 2)\n",
    "                df_wer_var = round(variance(wers), 3)\n",
    "            except:\n",
    "                print(stats_file_path)\n",
    "                continue\n",
    "            speaker_new=speaker+\"[{}/{}]\".format(composed_accent_map[speaker], gender_map[speaker])\n",
    "            df = df.append(dict(zip(cols, [speaker_new, \"within\", clean_cond(function), similarity, [\"NA\"], [\"NA\"]]+\n",
    "                                           wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + [\"NA\"])), \n",
    "                           ignore_index=True)\n",
    "            stats_file.close()\n",
    "\n",
    "#normal\n",
    "for speaker in speakers:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(speaker)).is_dir()):\n",
    "        print(\"no results for speaker {}\".format(speaker))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output/'.format(speaker)):\n",
    "        print(\"no all results for {}\".format(speaker))\n",
    "        continue\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(speaker, budget, target)).is_dir()):\n",
    "        print(\"no results for budget {} and target {}\".format(budget, target))\n",
    "        continue\n",
    "    for function in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(speaker, budget, target)):\n",
    "        if function == \"random\":\n",
    "            continue\n",
    "        for eta in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/{}/'.format(speaker, budget, target, function)):\n",
    "            for similarity in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/{}/{}/'.format(speaker, budget, target, function, eta)):\n",
    "                stats_file_path = './{}/manifests/TSS_output/all/budget_{}/target_{}/{}/{}/{}/{}/stats.txt'.format(speaker, budget, target, function, \n",
    "                eta, similarity, features)  \n",
    "#                 try:\n",
    "#                     stats_file = open(stats_file_path, 'r')\n",
    "#                 except:\n",
    "#                     continue                                                                                          \n",
    "                stats_file = open(stats_file_path, 'r')\n",
    "                lines = stats_file.readlines()\n",
    "                df_selections = get_selection_counts(lines[5])\n",
    "                total_selections, total_durations, speakered_selections, speakered_durations = map(get_each_run, lines[:4])\n",
    "                sample_frac = mean([x[0]/x[1] for x in zip(speakered_selections, total_selections)])\n",
    "                sample_total = mean(total_selections)\n",
    "                duration_frac = mean([x[0]/x[1] for x in zip(speakered_durations, total_durations)])\n",
    "                duration_total = mean(total_durations)\n",
    "                df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "                df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "                try:\n",
    "                    wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                    df_wer_mean = round(mean(wers), 2)\n",
    "                    df_wer_var = round(variance(wers), 3)\n",
    "                except:\n",
    "                    print(stats_file_path)\n",
    "                    continue\n",
    "                speaker_new=speaker+\"[{}/{}]\".format(composed_accent_map[speaker], gender_map[speaker])\n",
    "                df = df.append(dict(zip(cols, [speaker_new, \"all\", clean_cond(function)+get_eta(function, eta), similarity, df_duration, df_samples]+\n",
    "                                               wers+[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), \n",
    "                               ignore_index=True)\n",
    "                stats_file.close()\n",
    "                \n",
    "                \n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/random/run_1/output\n",
    "for speaker in speakers:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(speaker)).is_dir()):\n",
    "        print(\"no results for speaker {}\".format(speaker))\n",
    "        continue\n",
    "    if 'all' not in get_dirs('./{}/manifests/TSS_output/'.format(speaker)):\n",
    "        print(\"no all results for {}\".format(speaker))\n",
    "        continue\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(speaker, budget, target)).is_dir()):\n",
    "        print(\"no results for budget {} and target {}\".format(budget, target))\n",
    "        continue\n",
    "    if \"random\" in get_dirs('./{}/manifests/TSS_output/all/budget_{}/target_{}/'.format(speaker, budget, target)):\n",
    "        stats_file_path = './{}/manifests/TSS_output/all/budget_{}/target_{}/{}/stats.txt'.format(speaker, budget, target, \"random\")\n",
    "#         try:\n",
    "#             stats_file = open(stats_file_path, 'r')\n",
    "#         except:\n",
    "#             continue\n",
    "        stats_file = open(stats_file_path, 'r')\n",
    "        lines = stats_file.readlines()\n",
    "        df_selections = get_selection_counts(lines[5])\n",
    "        total_selections, total_durations, speakered_selections, speakered_durations = map(get_each_run, lines[:4])\n",
    "        sample_frac = mean([x[0]/x[1] for x in zip(speakered_selections, total_selections)])\n",
    "        sample_total = mean(total_selections)\n",
    "        duration_frac = mean([x[0]/x[1] for x in zip(speakered_durations, total_durations)])\n",
    "        duration_total = mean(total_durations)\n",
    "        df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "        df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "#               print(wers)\n",
    "        try:\n",
    "            wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "            df_wer_mean = round(mean(wers), 2)\n",
    "            df_wer_var = round(variance(wers), 3)\n",
    "        except:\n",
    "            print(\"no WER's in file\", get_test_file_from_stats_path(1, stats_file))\n",
    "            continue\n",
    "            wers = [0,0,0]\n",
    "            df_wer_mean = 0\n",
    "            df_wer_var = 999\n",
    "        speaker_new=speaker+\"[{}/{}]\".format(composed_accent_map[speaker], gender_map[speaker])\n",
    "        df = df.append(dict(zip(cols, [speaker_new, \"all\", \"random\", \"NA\", df_duration, df_samples]+wers\n",
    "                                        +[df_wer_mean, round(df_wer_var**0.5, 3)] + df_selections)), ignore_index=True)\n",
    "        stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>ground</th>\n",
       "      <th>function</th>\n",
       "      <th>similarity</th>\n",
       "      <th>duration</th>\n",
       "      <th>samples</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-var</th>\n",
       "      <th>accents_run1</th>\n",
       "      <th>accents_run2</th>\n",
       "      <th>accents_run3</th>\n",
       "      <th>speakers_run1</th>\n",
       "      <th>speakers_run2</th>\n",
       "      <th>speakers_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABA[ARB/M]</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>14.61/358.25</td>\n",
       "      <td>3.68/100.00</td>\n",
       "      <td>23.76</td>\n",
       "      <td>24.42</td>\n",
       "      <td>23.72</td>\n",
       "      <td>23.97</td>\n",
       "      <td>0.394</td>\n",
       "      <td>{'KOR': 22, 'ESP': 15, 'VTN': 19, 'ARB': 20, '...</td>\n",
       "      <td>{'KOR': 20, 'CHN': 18, 'VTN': 15, 'ARB': 15, '...</td>\n",
       "      <td>{'KOR': 24, 'ESP': 16, 'ARB': 18, 'CHN': 18, '...</td>\n",
       "      <td>{'KOR': {'YDCK': 7, 'HKK': 7, 'YKWK': 4, 'HJK'...</td>\n",
       "      <td>{'KOR': {'HJK': 9, 'HKK': 5, 'YKWK': 3, 'YDCK'...</td>\n",
       "      <td>{'KOR': {'HJK': 9, 'YDCK': 5, 'HKK': 5, 'YKWK'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABA[ARB/M]</td>\n",
       "      <td>all</td>\n",
       "      <td>FL1MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>267.08/357.67</td>\n",
       "      <td>60.00/83.00</td>\n",
       "      <td>22.33</td>\n",
       "      <td>22.26</td>\n",
       "      <td>22.59</td>\n",
       "      <td>22.39</td>\n",
       "      <td>0.173</td>\n",
       "      <td>{'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...</td>\n",
       "      <td>{'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...</td>\n",
       "      <td>{'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...</td>\n",
       "      <td>{'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...</td>\n",
       "      <td>{'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...</td>\n",
       "      <td>{'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABA[ARB/M]</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>357.17/357.17</td>\n",
       "      <td>70.00/70.00</td>\n",
       "      <td>21.45</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.02</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.217</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABA[ARB/M]</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>359.83/359.83</td>\n",
       "      <td>70.00/70.00</td>\n",
       "      <td>21.71</td>\n",
       "      <td>21.53</td>\n",
       "      <td>21.78</td>\n",
       "      <td>21.67</td>\n",
       "      <td>0.130</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': 70}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "      <td>{'ARB': {'ABA': 70}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABA[ARB/M]</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>345.11/358.04</td>\n",
       "      <td>76.00/80.00</td>\n",
       "      <td>21.71</td>\n",
       "      <td>21.89</td>\n",
       "      <td>21.64</td>\n",
       "      <td>21.75</td>\n",
       "      <td>0.130</td>\n",
       "      <td>{'ARB': 76, 'ESP': 2, 'HIN': 2}</td>\n",
       "      <td>{'ARB': 76, 'ESP': 2, 'HIN': 2}</td>\n",
       "      <td>{'ARB': 76, 'ESP': 2, 'HIN': 2}</td>\n",
       "      <td>{'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...</td>\n",
       "      <td>{'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...</td>\n",
       "      <td>{'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ZHAA[ARB/F]</td>\n",
       "      <td>all</td>\n",
       "      <td>random</td>\n",
       "      <td>NA</td>\n",
       "      <td>20.47/358.25</td>\n",
       "      <td>6.36/100.00</td>\n",
       "      <td>17.99</td>\n",
       "      <td>18.73</td>\n",
       "      <td>18.32</td>\n",
       "      <td>18.35</td>\n",
       "      <td>0.370</td>\n",
       "      <td>{'KOR': 22, 'ESP': 15, 'VTN': 19, 'ARB': 20, '...</td>\n",
       "      <td>{'KOR': 20, 'CHN': 18, 'VTN': 15, 'ARB': 15, '...</td>\n",
       "      <td>{'KOR': 24, 'ESP': 16, 'ARB': 18, 'CHN': 18, '...</td>\n",
       "      <td>{'KOR': {'YDCK': 7, 'HKK': 7, 'YKWK': 4, 'HJK'...</td>\n",
       "      <td>{'KOR': {'HJK': 9, 'HKK': 5, 'YKWK': 3, 'YDCK'...</td>\n",
       "      <td>{'KOR': {'HJK': 9, 'YDCK': 5, 'HKK': 5, 'YKWK'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ZHAA[ARB/F]</td>\n",
       "      <td>all</td>\n",
       "      <td>FL1MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>204.59/358.50</td>\n",
       "      <td>56.00/96.00</td>\n",
       "      <td>18.40</td>\n",
       "      <td>18.40</td>\n",
       "      <td>18.40</td>\n",
       "      <td>18.40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>{'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...</td>\n",
       "      <td>{'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...</td>\n",
       "      <td>{'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...</td>\n",
       "      <td>{'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...</td>\n",
       "      <td>{'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...</td>\n",
       "      <td>{'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ZHAA[ARB/F]</td>\n",
       "      <td>all</td>\n",
       "      <td>FL2MI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>357.67/357.67</td>\n",
       "      <td>95.00/95.00</td>\n",
       "      <td>17.99</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>17.90</td>\n",
       "      <td>0.077</td>\n",
       "      <td>{'ARB': 95}</td>\n",
       "      <td>{'ARB': 95}</td>\n",
       "      <td>{'ARB': 95}</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}}</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}}</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>ZHAA[ARB/F]</td>\n",
       "      <td>all</td>\n",
       "      <td>GCMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>357.43/357.43</td>\n",
       "      <td>94.00/94.00</td>\n",
       "      <td>17.84</td>\n",
       "      <td>17.77</td>\n",
       "      <td>17.73</td>\n",
       "      <td>17.78</td>\n",
       "      <td>0.055</td>\n",
       "      <td>{'ARB': 94}</td>\n",
       "      <td>{'ARB': 94}</td>\n",
       "      <td>{'ARB': 94}</td>\n",
       "      <td>{'ARB': {'ZHAA': 94}}</td>\n",
       "      <td>{'ARB': {'ZHAA': 94}}</td>\n",
       "      <td>{'ARB': {'ZHAA': 94}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>ZHAA[ARB/F]</td>\n",
       "      <td>all</td>\n",
       "      <td>LogDMI-n:1.0</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>332.64/355.95</td>\n",
       "      <td>95.00/103.00</td>\n",
       "      <td>18.44</td>\n",
       "      <td>18.44</td>\n",
       "      <td>17.24</td>\n",
       "      <td>18.04</td>\n",
       "      <td>0.693</td>\n",
       "      <td>{'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...</td>\n",
       "      <td>{'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...</td>\n",
       "      <td>{'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...</td>\n",
       "      <td>{'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         speaker ground      function similarity       duration       samples  \\\n",
       "0     ABA[ARB/M]    all        random         NA   14.61/358.25   3.68/100.00   \n",
       "1     ABA[ARB/M]    all   FL1MI-n:1.0  euclidean  267.08/357.67   60.00/83.00   \n",
       "2     ABA[ARB/M]    all   FL2MI-n:1.0  euclidean  357.17/357.17   70.00/70.00   \n",
       "3     ABA[ARB/M]    all    GCMI-n:1.0  euclidean  359.83/359.83   70.00/70.00   \n",
       "4     ABA[ARB/M]    all  LogDMI-n:1.0  euclidean  345.11/358.04   76.00/80.00   \n",
       "..           ...    ...           ...        ...            ...           ...   \n",
       "118  ZHAA[ARB/F]    all        random         NA   20.47/358.25   6.36/100.00   \n",
       "119  ZHAA[ARB/F]    all   FL1MI-n:1.0  euclidean  204.59/358.50   56.00/96.00   \n",
       "120  ZHAA[ARB/F]    all   FL2MI-n:1.0  euclidean  357.67/357.67   95.00/95.00   \n",
       "121  ZHAA[ARB/F]    all    GCMI-n:1.0  euclidean  357.43/357.43   94.00/94.00   \n",
       "122  ZHAA[ARB/F]    all  LogDMI-n:1.0  euclidean  332.64/355.95  95.00/103.00   \n",
       "\n",
       "     WER-r1  WER-r2  WER-r3  WER-mean  WER-var  \\\n",
       "0     23.76   24.42   23.72     23.97    0.394   \n",
       "1     22.33   22.26   22.59     22.39    0.173   \n",
       "2     21.45   21.20   21.02     21.22    0.217   \n",
       "3     21.71   21.53   21.78     21.67    0.130   \n",
       "4     21.71   21.89   21.64     21.75    0.130   \n",
       "..      ...     ...     ...       ...      ...   \n",
       "118   17.99   18.73   18.32     18.35    0.370   \n",
       "119   18.40   18.40   18.40     18.40    0.000   \n",
       "120   17.99   17.88   17.84     17.90    0.077   \n",
       "121   17.84   17.77   17.73     17.78    0.055   \n",
       "122   18.44   18.44   17.24     18.04    0.693   \n",
       "\n",
       "                                          accents_run1  \\\n",
       "0    {'KOR': 22, 'ESP': 15, 'VTN': 19, 'ARB': 20, '...   \n",
       "1    {'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...   \n",
       "2                                          {'ARB': 70}   \n",
       "3                                          {'ARB': 70}   \n",
       "4                      {'ARB': 76, 'ESP': 2, 'HIN': 2}   \n",
       "..                                                 ...   \n",
       "118  {'KOR': 22, 'ESP': 15, 'VTN': 19, 'ARB': 20, '...   \n",
       "119  {'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...   \n",
       "120                                        {'ARB': 95}   \n",
       "121                                        {'ARB': 94}   \n",
       "122  {'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...   \n",
       "\n",
       "                                          accents_run2  \\\n",
       "0    {'KOR': 20, 'CHN': 18, 'VTN': 15, 'ARB': 15, '...   \n",
       "1    {'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...   \n",
       "2                                          {'ARB': 70}   \n",
       "3                                          {'ARB': 70}   \n",
       "4                      {'ARB': 76, 'ESP': 2, 'HIN': 2}   \n",
       "..                                                 ...   \n",
       "118  {'KOR': 20, 'CHN': 18, 'VTN': 15, 'ARB': 15, '...   \n",
       "119  {'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...   \n",
       "120                                        {'ARB': 95}   \n",
       "121                                        {'ARB': 94}   \n",
       "122  {'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...   \n",
       "\n",
       "                                          accents_run3  \\\n",
       "0    {'KOR': 24, 'ESP': 16, 'ARB': 18, 'CHN': 18, '...   \n",
       "1    {'ARB': 60, 'VTN': 8, 'ESP': 6, 'KOR': 5, 'HIN...   \n",
       "2                                          {'ARB': 70}   \n",
       "3                                          {'ARB': 70}   \n",
       "4                      {'ARB': 76, 'ESP': 2, 'HIN': 2}   \n",
       "..                                                 ...   \n",
       "118  {'KOR': 24, 'ESP': 16, 'ARB': 18, 'CHN': 18, '...   \n",
       "119  {'ARB': 56, 'VTN': 30, 'KOR': 7, 'CHN': 2, 'ES...   \n",
       "120                                        {'ARB': 95}   \n",
       "121                                        {'ARB': 94}   \n",
       "122  {'ARB': 95, 'ESP': 2, 'KOR': 4, 'CHN': 1, 'VTN...   \n",
       "\n",
       "                                         speakers_run1  \\\n",
       "0    {'KOR': {'YDCK': 7, 'HKK': 7, 'YKWK': 4, 'HJK'...   \n",
       "1    {'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...   \n",
       "2                                 {'ARB': {'ABA': 70}}   \n",
       "3                                 {'ARB': {'ABA': 70}}   \n",
       "4    {'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...   \n",
       "..                                                 ...   \n",
       "118  {'KOR': {'YDCK': 7, 'HKK': 7, 'YKWK': 4, 'HJK'...   \n",
       "119  {'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...   \n",
       "120                              {'ARB': {'ZHAA': 95}}   \n",
       "121                              {'ARB': {'ZHAA': 94}}   \n",
       "122  {'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...   \n",
       "\n",
       "                                         speakers_run2  \\\n",
       "0    {'KOR': {'HJK': 9, 'HKK': 5, 'YKWK': 3, 'YDCK'...   \n",
       "1    {'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...   \n",
       "2                                 {'ARB': {'ABA': 70}}   \n",
       "3                                 {'ARB': {'ABA': 70}}   \n",
       "4    {'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...   \n",
       "..                                                 ...   \n",
       "118  {'KOR': {'HJK': 9, 'HKK': 5, 'YKWK': 3, 'YDCK'...   \n",
       "119  {'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...   \n",
       "120                              {'ARB': {'ZHAA': 95}}   \n",
       "121                              {'ARB': {'ZHAA': 94}}   \n",
       "122  {'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...   \n",
       "\n",
       "                                         speakers_run3  \n",
       "0    {'KOR': {'HJK': 9, 'YDCK': 5, 'HKK': 5, 'YKWK'...  \n",
       "1    {'ARB': {'ABA': 58, 'SKA': 2}, 'VTN': {'PNV': ...  \n",
       "2                                 {'ARB': {'ABA': 70}}  \n",
       "3                                 {'ARB': {'ABA': 70}}  \n",
       "4    {'ARB': {'ABA': 76}, 'ESP': {'ERMS': 1, 'EBVS'...  \n",
       "..                                                 ...  \n",
       "118  {'KOR': {'HJK': 9, 'YDCK': 5, 'HKK': 5, 'YKWK'...  \n",
       "119  {'ARB': {'ZHAA': 54, 'SKA': 2}, 'VTN': {'PNV':...  \n",
       "120                              {'ARB': {'ZHAA': 95}}  \n",
       "121                              {'ARB': {'ZHAA': 94}}  \n",
       "122  {'ARB': {'ZHAA': 95}, 'ESP': {'NJS': 2}, 'KOR'...  \n",
       "\n",
       "[123 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sort_values(by=['speaker', 'similarity', 'ground', 'function'], ascending=True, ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# accented selection: 76 76 76 -> 76.00\n",
    "# accented duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "#Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers = [f.name for f in os.scandir('../accent/') if f.is_dir() and f.name != '.ipynb_checkpoints' and f.name != 'reserved_TSS_output']\n",
    "# print(speakers)\n",
    "# for speaker in speakers:\n",
    "#     for budget in get_dirs('../accent/{}/manifests/TSS_output/all/'.format(speaker)):\n",
    "#         print(budget)\n",
    "#         for target in get_dirs('../accent/{}/manifests/TSS_output/all/{}/'.format(speaker, budget)):\n",
    "#             print(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
