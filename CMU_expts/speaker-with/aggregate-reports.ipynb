{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import re, os, csv, pathlib, ast\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "\n",
    "accent_short_forms = {\"hindi\":\"HIN\", \"korean\":\"KOR\", \"vietnamese\":\"VTN\", \"arabic\":\"ARB\", \"chinese\":\"CHN\", \"spanish\":\"ESP\"}\n",
    "accent_map = {\"ABA\":\"arabic\",\"SKA\":\"arabic\",\"YBAA\":\"arabic\",\"ZHAA\":\"arabic\",\n",
    "              \"BWC\":\"chinese\",\"LXC\":\"chinese\",\"NCC\":\"chinese\",\"TXHC\":\"chinese\",\n",
    "              \"ASI\":\"hindi\",\"RRBI\":\"hindi\",\"SVBI\":\"hindi\",\"TNI\":\"hindi\",\n",
    "              \"HJK\":\"korean\",\"HKK\":\"korean\",\"YDCK\":\"korean\",\"YKWK\":\"korean\",\n",
    "              \"EBVS\":\"spanish\",\"ERMS\":\"spanish\",\"MBMPS\":\"spanish\",\"NJS\":\"spanish\",\n",
    "              \"HQTV\":\"vietnamese\",\"PNV\":\"vietnamese\",\"THV\":\"vietnamese\",\"TLV\":\"vietnamese\"\n",
    "              }\n",
    "raw_string=\"\"\"|ABA|M|Arabic|1129|150|\\n|SKA|F|Arabic|974|150|\\n|YBAA|M|Arabic|1130|149||ZHAA|F|Arabic|1132|150|\\n|BWC|M|Chinese|1130|150|\\n|LXC|F|Chinese|1131|150|\\n|NCC|F|Chinese|1131|150|\\n|TXHC|M|Chinese|1132|150|\\n|ASI|M|Hindi|1131|150|\\n|RRBI|M|Hindi|1130|150|\\n|SVBI|F|Hindi|1132|150|\\n|TNI|F|Hindi|1131|150|\\n|HJK|F|Korean|1131|150|\\n|HKK|M|Korean|1131|150|\\n|YDCK|F|Korean|1131|150|\\n|YKWK|M|Korean|1131|150|\\n|EBVS|M|Spanish|1007|150|\\n|ERMS|M|Spanish|1132|150|\\n|MBMPS|F|Spanish|1132|150|\\n|NJS|F|Spanish|1131|150|\\n|HQTV|M|Vietnamese|1132|150|\\n|PNV|F|Vietnamese|1132|150|\\n|THV|F|Vietnamese|1132|150|\\n|TLV|M|Vietnamese|1132|150|\"\"\"\n",
    "raw_strings=raw_string.split('\\n')\n",
    "gender_map={}\n",
    "for lne in raw_strings:\n",
    "    attrs=lne.split('|')\n",
    "    gender_map[attrs[1]]=attrs[2]\n",
    "\n",
    "composed_accent_map = {k: accent_short_forms.get(v) for k, v in accent_map.items()}\n",
    "\n",
    "def replace_with_short_forms(s):\n",
    "    for key, value in accent_short_forms.items():\n",
    "        s = s.replace(key, value)\n",
    "    return s\n",
    "\n",
    "def group_speakers(s):\n",
    "    ret = {}\n",
    "    speaker_counts = ast.literal_eval(s)\n",
    "    for speaker, count in speaker_counts.items():\n",
    "        accent = composed_accent_map[speaker]\n",
    "        if accent not in ret:\n",
    "            ret[accent] = {}\n",
    "        ret[accent][speaker] = count\n",
    "    return str(ret)\n",
    "\n",
    "def group_accents(s):\n",
    "    ret = {}\n",
    "    speaker_counts = ast.literal_eval(s)\n",
    "    for speaker, count in speaker_counts.items():\n",
    "        accent = composed_accent_map[speaker]\n",
    "        if accent not in ret:\n",
    "            ret[accent] = {}\n",
    "        ret[accent][speaker] = count\n",
    "    accent_counts={}\n",
    "    for accent in ret:\n",
    "        cnt=0\n",
    "        for speaker in ret[accent]:\n",
    "            cnt+=ret[accent][speaker]\n",
    "        accent_counts[accent]=cnt\n",
    "    return str(accent_counts)\n",
    "\n",
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "def get_each_run(lne):\n",
    "    return list(map(float, re.findall(': (.+?) -> ', lne)[0].split(' ')))\n",
    "\n",
    "def get_selection_counts(s):\n",
    "    return list(map(group_accents, re.findall('Counter\\\\((.+?)\\\\)', s)))+list(map(group_speakers, re.findall('Counter\\\\((.+?)\\\\)', s)))\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9]+\"run_{}/output/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    txt_file = open(test_file, 'r')\n",
    "    lines = txt_file.readlines()\n",
    "    matched = \"\"\n",
    "    for line in lines:\n",
    "        if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "            txt_file.close()\n",
    "            return float(line.rstrip().split(\": \")[1])\n",
    "    txt_file.close()\n",
    "    return \"\"\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    return \"-n:\"+str(float(eta[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 100\n",
    "target = 50\n",
    "features = '39'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/accent/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "cols = ['speaker', 'ground', 'function', 'similarity', 'duration', 'samples', \n",
    "        'WER-r1', 'WER-r2', 'WER-r3', 'WER-mean', 'WER-var', 'accents_run1', 'accents_run2', 'accents_run3', 'speakers_run1', 'speakers_run2', 'speakers_run3']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "\n",
    "# speakers = [f.name for f in os.scandir('./') if f.is_dir() and f.name != '.ipynb_checkpoints']\n",
    "speakers = ['ABA', 'ASI', 'BWC', 'EBVS', 'HJK', 'ERMS', 'HKK', 'HQTV']\n",
    "\n",
    "cnt = 0\n",
    "for speaker in speakers:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(speaker)).is_dir()):\n",
    "        continue\n",
    "    for pick_from in get_dirs('./{}/manifests/TSS_output/'.format(speaker)):\n",
    "        if not(pathlib.Path('./{}/manifests/TSS_output/{}/budget_{}/target_{}/'.format(speaker, pick_from, budget, target)).is_dir()):\n",
    "            continue\n",
    "        for function in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/'.format(speaker, pick_from, budget, target)):\n",
    "            if function == \"random\":\n",
    "                stats_file_path = './{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/stats.txt'.format(speaker, pick_from, budget, \n",
    "                                                                                                                target, function)                                                                                  \n",
    "                if not(os.path.isfile(stats_file_path)):\n",
    "                    continue\n",
    "                stats_file = open(stats_file_path, 'r')\n",
    "                lines = stats_file.readlines()\n",
    "                total_selections, total_durations, speakered_selections, speakered_durations = map(get_each_run, lines[:4])\n",
    "                sample_frac = mean([x[0]/x[1] for x in zip(speakered_selections, total_selections)])\n",
    "                sample_total = mean(total_selections)\n",
    "                duration_frac = mean([x[0]/x[1] for x in zip(speakered_durations, total_durations)])\n",
    "                duration_total = mean(total_durations)\n",
    "                df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "                df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "                df_selections = get_selection_counts(lines[5])\n",
    "                wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                df_wer_mean = round(mean(wers), 2)\n",
    "                df_wer_var = round(variance(wers), 4)\n",
    "                speaker_new=speaker+\"[{}/{}]\".format(composed_accent_map[speaker], gender_map[speaker])\n",
    "                df = df.append(dict(zip(cols, [speaker_new, pick_from, function, \"NA\", df_duration, df_samples]+wers\n",
    "                                                +[df_wer_mean, df_wer_var] + df_selections)), ignore_index=True)\n",
    "                stats_file.close()\n",
    "                continue\n",
    "            for eta in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/'.format(speaker, pick_from, budget, target, function)):\n",
    "                for similarity in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/{}/'.format(speaker, pick_from, \n",
    "                                                                                                        budget, target, function, eta)):\n",
    "                    # print(cnt)\n",
    "                    cnt += 1\n",
    "                    stats_file_path = './{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/{}/{}/{}/stats.txt'.format(speaker, pick_from, budget, \n",
    "                                                                                                                target, function, eta, similarity, \n",
    "                                                                                                                features)                                                                                  \n",
    "                    if not(os.path.isfile(stats_file_path)):\n",
    "                        continue\n",
    "                    stats_file = open(stats_file_path, 'r')\n",
    "                    lines = stats_file.readlines()\n",
    "                    total_selections, total_durations, speakered_selections, speakered_durations = map(get_each_run, lines[:4])\n",
    "                    sample_frac = mean([x[0]/x[1] for x in zip(speakered_selections, total_selections)])\n",
    "                    sample_total = mean(total_selections)\n",
    "                    duration_frac = mean([x[0]/x[1] for x in zip(speakered_durations, total_durations)])\n",
    "                    duration_total = mean(total_durations)\n",
    "                    df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "                    df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "                    df_selections = get_selection_counts(lines[5])\n",
    "                    wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                    df_wer_mean = round(mean(wers), 2)\n",
    "                    df_wer_var = round(variance(wers), 4)\n",
    "                    speaker_new=speaker+\"[{}/{}]\".format(composed_accent_map[speaker], gender_map[speaker])\n",
    "                    df = df.append(dict(zip(cols, [speaker_new, pick_from, function+get_eta(function, eta), similarity, df_duration, df_samples]+\n",
    "                                                   wers+[df_wer_mean, df_wer_var] + df_selections)), \n",
    "                                   ignore_index=True)\n",
    "                    stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>ground</th>\n",
       "      <th>function</th>\n",
       "      <th>similarity</th>\n",
       "      <th>duration</th>\n",
       "      <th>samples</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-var</th>\n",
       "      <th>accents_run1</th>\n",
       "      <th>accents_run2</th>\n",
       "      <th>accents_run3</th>\n",
       "      <th>speakers_run1</th>\n",
       "      <th>speakers_run2</th>\n",
       "      <th>speakers_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [speaker, ground, function, similarity, duration, samples, WER-r1, WER-r2, WER-r3, WER-mean, WER-var, accents_run1, accents_run2, accents_run3, speakers_run1, speakers_run2, speakers_run3]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sort_values(by=['speaker', 'similarity', 'ground', 'function'], ascending=True, ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# accented selection: 76 76 76 -> 76.00\n",
    "# accented duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "#Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(\"report_{}_{}_{}.csv\".format(budget, target, features), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
