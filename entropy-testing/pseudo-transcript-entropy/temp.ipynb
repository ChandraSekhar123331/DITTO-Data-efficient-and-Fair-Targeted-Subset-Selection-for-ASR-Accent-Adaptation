{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a50bad",
   "metadata": {},
   "source": [
    "# This is the driver file for the full pseudo transcript, TF-IDF based subset selection experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae402db",
   "metadata": {},
   "source": [
    "### Creating pseudo transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0e2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a1dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mayank/.conda/envs/error/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85700348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8870a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/mayank/MTP/begin_again/Error-Driven-ASR-Personalization'\n",
    "CURR_DIR = BASE_PATH + '/entropy-testing/pseudo-transcript-entropy'\n",
    "PARENT_DIR = BASE_PATH + '/entropy-testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8191f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCENTS = list(map(lambda x : x + '_english', [\n",
    "    'assamese_female', \n",
    "    'gujarati_female',\n",
    "    'hindi_male',\n",
    "    'kannada_male',\n",
    "    'malayalam_male',\n",
    "    'manipuri_female',\n",
    "    'rajasthani_male',\n",
    "    'tamil_male'\n",
    "]))\n",
    "\n",
    "BUDGETS= [\n",
    "    '100',\n",
    "    '200',\n",
    "    '400',\n",
    "    '800'\n",
    "]\n",
    "\n",
    "TARGET = [10]\n",
    "\n",
    "METHODS = ['FL2MI', 'GCMI', 'LogDMI']\n",
    "\n",
    "ETA = ['1.0']\n",
    "\n",
    "SIM = ['euclidean']\n",
    "\n",
    "FEATURES = ['39']\n",
    "\n",
    "RUNS = ['1', '2', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a394a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do the SMI generation\n",
    "\n",
    "def doSMI(feature, similarity, eta, target, budget, method, accent):\n",
    "    \n",
    "    python_file = CURR_DIR + \"/TSS.py\"\n",
    "    \n",
    "    print(\"----------------------- TSS -----------------------\")\n",
    "    print(f\"accent_{accent}, budget_{budget}, method_{method}\")\n",
    "    !$sys.executable $python_file --target $target --budget $budget --similarity $similarity --eta $eta --accent $accent --fxn $method --feature_type $feature\n",
    "    \n",
    "\n",
    "def generate_SMI_selections():\n",
    "    for feature in FEATURES:\n",
    "        for sim in SIM:\n",
    "            for eta in ETA:\n",
    "                for target in TARGET:\n",
    "                    for budget in BUDGETS:\n",
    "                        for method in METHODS:\n",
    "                            for accent in ACCENTS:\n",
    "                                doSMI(\n",
    "                                    feature = feature,\n",
    "                                    similarity = sim,\n",
    "                                    eta = eta,\n",
    "                                    target = target,\n",
    "                                    budget = budget,\n",
    "                                    method = method,\n",
    "                                    accent = accent\n",
    "                                )\n",
    "\n",
    "generate_SMI_selections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56ec94",
   "metadata": {},
   "source": [
    "### Generate the transcripts and do the grapheme to phoneme.\n",
    "-  We'll be directly using the code from error-model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188f46a",
   "metadata": {},
   "source": [
    "### Generate the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_transcripts(feature, similarity, eta, target, budget, method, accent, run):\n",
    "    python_env = sys.executable\n",
    "    python_file = CURR_DIR + \"/models/quartznet_asr/inference.py\"\n",
    "    data_base_dir = CURR_DIR + f\"/data/{accent}/manifests/TSS_output/all/budget_{budget}/target_{target}/{method}/eta_{eta}/{similarity}/{feature}/run_{run}/\"\n",
    "    wav_dir=BASE_PATH + \"/data/indicTTS_audio/indicTTS/{accent}/english/wav/\"\n",
    "    ckpt_base_dir=CURR_DIR + \"/models/pretrained_checkpoints/\"\n",
    "    batch_size=32\n",
    "    bash_file = CURR_DIR + \"/models/quartznet_asr/scripts/infer_transcriptions_on_seed_set.sh\"\n",
    "\n",
    "    \n",
    "    print(\"------ Generating Pseudo Transcripts -------\")\n",
    "    print(f\"accent_{accent}, budget_{budget}, method_{method}, run_{run}\")\n",
    "    !bash $bash_file $python_env $python_file $data_base_dir $wav_dir $ckpt_base_dir $batch_size\n",
    "    \n",
    "def infer_transcripts_all():\n",
    "    ACCENTS = [\"kannada_male_english\"]\n",
    "    METHODS = [\"FL2MI\"]\n",
    "    BUDGETS = [800]\n",
    "    for feature in FEATURES:\n",
    "        for sim in SIM:\n",
    "            for eta in ETA:\n",
    "                for target in TARGET:\n",
    "                    for budget in BUDGETS:\n",
    "                        for method in METHODS:\n",
    "                            for run in RUNS:\n",
    "                                for accent in ACCENTS:\n",
    "                                    infer_transcripts(\n",
    "                                        feature = feature,\n",
    "                                        similarity = sim,\n",
    "                                        eta = eta,\n",
    "                                        target = target,\n",
    "                                        budget = budget,\n",
    "                                        method = method,\n",
    "                                        accent = accent,\n",
    "                                        run = run\n",
    "                                    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_transcripts_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0ab8e",
   "metadata": {},
   "source": [
    "#### Get the phoneme versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "a = \" \".join(list(map(str, [1,1,2,4,5,6,125,54,456])))\n",
    "b = \" \".join(list(map(str, [4])))\n",
    "print(a)\n",
    "print(b)\n",
    "corpus = [b,a,a]\n",
    "vectorizer = TfidfVectorizer(lowercase=False, token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(len(vectorizer.get_feature_names_out()))\n",
    "data = X.toarray()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9a288",
   "metadata": {},
   "source": [
    "#### Do the TF-IDF vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1077f3",
   "metadata": {},
   "source": [
    "#### Make selections using some submodlib utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from submodlib import FacilityLocationFunction\n",
    "objFL = FacilityLocationFunction(n = data.shape[0], data=data, mode=\"dense\", metric = \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "objFL.maximize(budget=2, optimizer=\"NaiveGreedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "181b6a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- TF- IDF based TSS -----------------------\n",
      "Arguments:\n",
      "\tjson_path : /home/mayank/MTP/begin_again/Error-Driven-ASR-Personalization/entropy-testing/pseudo-transcript-entropy/quartznet_outputs/infer_out.txt\n",
      "\t     seed : 42\n",
      "loading data....\n",
      "100%|████████████████████████████████████████| 721/721 [00:01<00:00, 401.11it/s]\n",
      "<class 'list'>\n",
      "i could not agree with ernest\n",
      "100%|████████████████████████████████████████| 721/721 [00:03<00:00, 211.73it/s]\n",
      "<class 'list'>\n",
      "['<s>', 'AY', ' ', 'K', 'UH', 'D', ' ', 'N', 'AA', 'T', ' ', 'AH', 'G', 'R', 'IY', ' ', 'W', 'IH', 'DH', ' ', 'ER', 'N', 'AH', 'S', 'T', '</s>']\n",
      "**********Sample phoneme data******\n",
      "['<s>', 'AY', ' ', 'K', 'UH', 'D', ' ', 'N', 'AA', 'T', ' ', 'AH', 'G', 'R', 'IY', ' ', 'W', 'IH', 'DH', ' ', 'ER', 'N', 'AH', 'S', 'T', '</s>']\n",
      "data_size: 721\n",
      "**** Converting phonemes to ids*****\n",
      "Sample phonemes converted to ids\n",
      "2 9 44 23 36 12 44 26 4 34 44 6 18 31 21 44 39 20 13 44 15 26 6 32 34 3\n",
      "TF-IDF vectorised data\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "shape of TF-IDF data is = (721, 5234)\n",
      "[||||||||||||||||||||]100% [Iteration 100 of 100][(131, 720.8352241516113),\n",
      " (538, 0.001545250415802002),\n",
      " (551, 0.0010895729064941406),\n",
      " (168, 0.0006974935531616211),\n",
      " (320, 0.0006539225578308105),\n",
      " (264, 0.0005894303321838379),\n",
      " (360, 0.0005593299865722656),\n",
      " (128, 0.0005223751068115234),\n",
      " (203, 0.0005143880844116211),\n",
      " (599, 0.0005016922950744629),\n",
      " (140, 0.00048094987869262695),\n",
      " (524, 0.0004615187644958496),\n",
      " (147, 0.0004494190216064453),\n",
      " (454, 0.0004057884216308594),\n",
      " (433, 0.00038945674896240234),\n",
      " (718, 0.00037801265716552734),\n",
      " (427, 0.0003757476806640625),\n",
      " (1, 0.00035065412521362305),\n",
      " (330, 0.0003479123115539551),\n",
      " (407, 0.000337064266204834),\n",
      " (457, 0.0003370046615600586),\n",
      " (246, 0.0003229379653930664),\n",
      " (580, 0.00031894445419311523),\n",
      " (356, 0.00031197071075439453),\n",
      " (269, 0.0003069639205932617),\n",
      " (310, 0.00030475854873657227),\n",
      " (274, 0.00030475854873657227),\n",
      " (166, 0.0003039836883544922),\n",
      " (287, 0.00030344724655151367),\n",
      " (355, 0.00030028820037841797),\n",
      " (52, 0.0002983212471008301),\n",
      " (111, 0.00029790401458740234),\n",
      " (92, 0.0002976059913635254),\n",
      " (75, 0.0002962350845336914),\n",
      " (118, 0.0002944469451904297),\n",
      " (539, 0.00028568506240844727),\n",
      " (548, 0.0002855062484741211),\n",
      " (508, 0.00028330087661743164),\n",
      " (173, 0.00027865171432495117),\n",
      " (115, 0.0002785921096801758),\n",
      " (177, 0.0002779364585876465),\n",
      " (342, 0.00027751922607421875),\n",
      " (344, 0.0002760887145996094),\n",
      " (428, 0.0002753734588623047),\n",
      " (379, 0.0002733469009399414),\n",
      " (123, 0.00027233362197875977),\n",
      " (76, 0.0002689957618713379),\n",
      " (225, 0.0002676844596862793),\n",
      " (222, 0.00026702880859375),\n",
      " (240, 0.00026661157608032227),\n",
      " (601, 0.0002664327621459961),\n",
      " (271, 0.0002644062042236328),\n",
      " (290, 0.000263214111328125),\n",
      " (324, 0.00026303529739379883),\n",
      " (266, 0.00026297569274902344),\n",
      " (3, 0.00025957822799682617),\n",
      " (159, 0.0002589225769042969),\n",
      " (622, 0.0002586841583251953),\n",
      " (367, 0.00025856494903564453),\n",
      " (83, 0.0002582073211669922),\n",
      " (699, 0.00025731325149536133),\n",
      " (255, 0.0002555251121520996),\n",
      " (242, 0.0002555251121520996),\n",
      " (231, 0.00025522708892822266),\n",
      " (73, 0.00025522708892822266),\n",
      " (482, 0.0002550482749938965),\n",
      " (665, 0.0002541542053222656),\n",
      " (592, 0.0002541542053222656),\n",
      " (716, 0.0002536177635192871),\n",
      " (618, 0.00025326013565063477),\n",
      " (541, 0.0002530813217163086),\n",
      " (646, 0.00025266408920288086),\n",
      " (251, 0.0002518296241760254),\n",
      " (171, 0.0002509951591491699),\n",
      " (633, 0.0002506375312805176),\n",
      " (381, 0.0002492070198059082),\n",
      " (198, 0.00024884939193725586),\n",
      " (249, 0.0002486109733581543),\n",
      " (48, 0.00024837255477905273),\n",
      " (526, 0.00024777650833129883),\n",
      " (517, 0.0002471804618835449),\n",
      " (277, 0.0002471804618835449),\n",
      " (598, 0.00024688243865966797),\n",
      " (666, 0.00024586915969848633),\n",
      " (611, 0.000245511531829834),\n",
      " (591, 0.0002446174621582031),\n",
      " (687, 0.0002442598342895508),\n",
      " (58, 0.0002429485321044922),\n",
      " (47, 0.00024276971817016602),\n",
      " (686, 0.0002421736717224121),\n",
      " (282, 0.00024175643920898438),\n",
      " (375, 0.00024169683456420898),\n",
      " (29, 0.0002416372299194336),\n",
      " (200, 0.00024121999740600586),\n",
      " (410, 0.0002409815788269043),\n",
      " (522, 0.00024014711380004883),\n",
      " (644, 0.00023955106735229492),\n",
      " (404, 0.0002391338348388672),\n",
      " (18, 0.0002390146255493164),\n",
      " (34, 0.00023883581161499023)]\n"
     ]
    }
   ],
   "source": [
    "python_file = CURR_DIR + \"/models/error_model/select_tf_idf.py\"\n",
    "json_path = CURR_DIR + \"/quartznet_outputs/infer_out.txt\"\n",
    "print(\"----------------------- TF- IDF based TSS -----------------------\")\n",
    "!$sys.executable $python_file --json_path $json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6b8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "error_env",
   "language": "python",
   "name": "error_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
