{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a027e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from collections import Counter\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import submodlib\n",
    "import re\n",
    "from submodlib.helper import create_kernel\n",
    "from submodlib.functions.facilityLocationMutualInformation import (\n",
    "    FacilityLocationMutualInformationFunction,\n",
    ")\n",
    "from submodlib.functions.facilityLocationVariantMutualInformation import (\n",
    "    FacilityLocationVariantMutualInformationFunction,\n",
    ")\n",
    "from submodlib.functions.graphCutMutualInformation import (\n",
    "    GraphCutMutualInformationFunction,\n",
    ")\n",
    "from submodlib.functions.logDeterminantMutualInformation import (\n",
    "    LogDeterminantMutualInformationFunction,\n",
    ")\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def budget(budget_size):\n",
    "    return int(4.92 * budget_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(lst):\n",
    "    return [(val - lst[-1]) / (lst[0] - lst[-1]) for val in lst]\n",
    "\n",
    "# def normalize_derivatives(derv):\n",
    "#     arr = np.array(derv)\n",
    "#     return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def second_derivative(lst):\n",
    "#     temp_lst = [lst[0]] + [_ for _ in lst]\n",
    "#     temp_lst.extend([temp_lst[-1]])\n",
    "#     length = len(lst)\n",
    "#     res = [temp_lst[ind + 1] - 2 * temp_lst[ind] + temp_lst[ind - 1] for ind in range(1, len(lst) + 1)]\n",
    "#     return res\n",
    "\n",
    "# print(second_derivative([-4,-2,-1,0,1,2,4,8,6,45,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a86191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subset(\n",
    "    dirs,\n",
    "    base_dir,\n",
    "    query_dir,\n",
    "    ground_list,\n",
    "    ground_features,\n",
    "    ground_features_Y,\n",
    "    query_list,\n",
    "    query_features,\n",
    "    test_features,\n",
    "    greedyList,\n",
    "    budget_size,\n",
    "    target_size,\n",
    "    accent,\n",
    "    fxn,\n",
    "    similarity,\n",
    "    etaValue,\n",
    "    feature_type,\n",
    "    right_end=-1,\n",
    "    left_end=50,\n",
    "):\n",
    "    #     print(greedyList)\n",
    "    list_total_selection, list_total_count, list_total_duration = [], [], []\n",
    "    list_accent_sample_count, list_accent_sample_duration = [], []\n",
    "    #     output_dir = os.path.join(\n",
    "    #         base_dir,\n",
    "    #         query_dir,\n",
    "    #         f'TSS_output/all/budget_{budget_size}/target_{target_size}/{fxn}/eta_{etaValue}/{similarity}/{feature_type}'\n",
    "    #     )   ------>\n",
    "    #     os.makedirs(output_dir, exist_ok=True)\n",
    "    #     for i in [1, 2, 3]:\n",
    "    for i in [1]:\n",
    "        run = f\"run_{i}\"\n",
    "        #         run_dir = os.path.join(output_dir, run) ------>\n",
    "        #         for folder in ['train', 'output', 'plots']:\n",
    "        #             os.makedirs(os.path.join(run_dir, folder), exist_ok=True)\n",
    "\n",
    "        all_indices = [j[0] for j in greedyList]\n",
    "        all_gains = [j[1] for j in greedyList]\n",
    "\n",
    "        def get_accent(ind):\n",
    "            return re.search(\n",
    "                \"/indicTTS/([A-Za-z_\\-]+)/\", ground_list[ind][\"audio_filepath\"]\n",
    "            ).group(1)\n",
    "\n",
    "        all_accents = [get_accent(ind) for ind in all_indices]\n",
    "        #         print(all_accents)\n",
    "        total_duration, index = 0, 0\n",
    "        while total_duration + ground_list[all_indices[index]][\"duration\"] <= budget(\n",
    "            budget_size\n",
    "        ):\n",
    "            total_duration += ground_list[all_indices[index]][\"duration\"]\n",
    "            index += 1\n",
    "\n",
    "        list_total_count.append(index)\n",
    "        list_total_duration.append(total_duration)\n",
    "        selected_indices = all_indices[:index]\n",
    "        selected_gains = all_gains[:index]\n",
    "        # derivatives = second_derivative(selected_gains)[:index]\n",
    "        selected_accents = all_accents[:index]\n",
    "\n",
    "        print(Counter(selected_accents))\n",
    "        #         print(\"Number of repeated values = \", len(selected_indices) - len(set(selected_indices)))\n",
    "        #         with open(f'{run_dir}/gains.txt' , 'w') as file:\n",
    "        #             for ind, (entry, accent) in enumerate(zip(selected_gains, selected_accents)):\n",
    "        #                 file.write(f\"{ind}, {entry}, {accent}\\n\")\n",
    "        excluded = left_end\n",
    "        color_array = [\n",
    "            \"r\" if get_accent(ind) == accent else \"b\" for ind in selected_indices\n",
    "        ]\n",
    "        accent_count_array = [\n",
    "            np.sum(\n",
    "                [False if acc == accent else True for acc in selected_accents[: i + 1]]\n",
    "            )\n",
    "            / (i)\n",
    "            for i in range(1, 1 + len(selected_gains))\n",
    "        ]\n",
    "\n",
    "        plt.scatter(\n",
    "            left_end + np.arange(len(selected_gains[left_end:right_end])),\n",
    "            normalize(selected_gains[left_end:right_end]),\n",
    "            c=color_array[left_end:right_end],\n",
    "            s=5,\n",
    "            marker=\".\",\n",
    "            label=\"scatter\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            left_end + np.arange(len(selected_gains[left_end:right_end])),\n",
    "            accent_count_array[left_end:right_end],\n",
    "            \"orange\",\n",
    "            label=\"outside_accent_percent\"\n",
    "        )\n",
    "#         plt.plot(left_end + np.arange(len(derivatives[left_end:right_end])),normalize_derivatives(derivatives[left_end:right_end]), \"green\", label=\"curvature\")\n",
    "        plt.xlabel(\"sample number\")\n",
    "        plt.ylabel(\"selected gains\")\n",
    "        # plt.title(f\"gains graph excluding left = {left_end}, right = {right_end}: Linear scale\")\n",
    "        plt.title(\n",
    "            f\"gains graph  left: {left_end}, right: {right_end}, scale: Linear, run: {i}, accent={accent}, fn = {fxn}\"\n",
    "        )\n",
    "        # plt.yscale(\"log\")\n",
    "        #         plt.xscale(\"log\")\n",
    "        plt.show()\n",
    "\n",
    "        #         excluded = 0\n",
    "        #         color_array = ['r' if get_accent(ind) == accent else 'b' for ind in selected_indices[excluded:]]\n",
    "        #         plt.scatter(excluded + np.arange(len(selected_gains[excluded:])), selected_gains[excluded:], c=color_array)\n",
    "        #         plt.xlabel(\"sample number\")\n",
    "        #         plt.ylabel(\"selected gains\")\n",
    "        #         plt.title(f\"gains graph excluded = {excluded} scale = Linear run = {i} accent={accent} fn = {fxn}\")\n",
    "        #         plt.show()\n",
    "\n",
    "        selected_list = [ground_list[j] for j in selected_indices]\n",
    "\n",
    "        #        train_list = selected_list + query_list\n",
    "        train_list = selected_list\n",
    "\n",
    "        accent_sample_count, accent_sample_duration = 0, 0\n",
    "        for item in selected_list:\n",
    "            if item[\"audio_filepath\"].split(\"/\")[-4] == accent:\n",
    "                accent_sample_count += 1\n",
    "                accent_sample_duration += item[\"duration\"]\n",
    "        list_accent_sample_count.append(accent_sample_count)\n",
    "        list_accent_sample_duration.append(accent_sample_duration)\n",
    "        list_total_selection.append(\n",
    "            Counter([item[\"audio_filepath\"].split(\"/\")[-4] for item in selected_list])\n",
    "        )\n",
    "\n",
    "    #        with open(base_dir + query_dir + f'train/error_model/{budget_size}/seed_{i}/train.json', 'w') as f:\n",
    "    #            for line in train_list:\n",
    "    #                f.write('{}\\n'.format(json.dumps(line)))\n",
    "    #         with open(f'{run_dir}/train/train.json', 'w') as f:\n",
    "    #             for line in train_list:\n",
    "    #                 f.write('{}\\n'.format(json.dumps(line)))\n",
    "\n",
    "    #        plots(dirs, run_dir, ground_features, ground_features_Y, query_features, test_features, selected_indices, selected_gains, fxn)\n",
    "\n",
    "    print(\"\\n subset computed .... \\n\")\n",
    "    stats = (\n",
    "        \"total selection : \"\n",
    "        + \" \".join(map(str, list_total_count))\n",
    "        + \" -> {0:.2f}\\n\".format(statistics.mean(list_total_count))\n",
    "    )\n",
    "    stats += (\n",
    "        \"total selection duration: \"\n",
    "        + \" \".join(map(str, list_total_duration))\n",
    "        + \" -> {0:.2f}\\n\".format(statistics.mean(list_total_duration))\n",
    "    )\n",
    "    stats += (\n",
    "        \"accented selection: \"\n",
    "        + \" \".join(map(str, list_accent_sample_count))\n",
    "        + \" -> {0:.2f}\\n\".format(statistics.mean(list_accent_sample_count))\n",
    "    )\n",
    "    stats += (\n",
    "        \"accented duration: \"\n",
    "        + \" \".join(map(str, list_accent_sample_duration))\n",
    "        + \" -> {0:.2f}\\n\".format(statistics.mean(list_accent_sample_duration))\n",
    "    )\n",
    "    stats += \"\\nall selections: \" + str(list_total_selection)\n",
    "\n",
    "\n",
    "#     with open(output_dir + '/stats.txt', 'w') as f:\n",
    "#         f.write(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedyList(\n",
    "    ground_kernel, query_kernel, query_query_kernel, fxn, budget_size, etaValue\n",
    "):\n",
    "    print(f\"\\ncreating {fxn} object\\n\")\n",
    "    if fxn == \"FL1MI\":\n",
    "        obj1 = FacilityLocationMutualInformationFunction(\n",
    "            n=len(ground_kernel),\n",
    "            num_queries=query_kernel.shape[1],\n",
    "            query_sijs=query_kernel,\n",
    "            data_sijs=ground_kernel,\n",
    "            magnificationEta=etaValue,\n",
    "        )\n",
    "    elif fxn == \"FL2MI\":\n",
    "        obj1 = FacilityLocationVariantMutualInformationFunction(\n",
    "            n=len(ground_kernel),\n",
    "            num_queries=query_kernel.shape[1],\n",
    "            query_sijs=query_kernel,\n",
    "            queryDiversityEta=etaValue,\n",
    "        )\n",
    "    elif fxn == \"GCMI\":\n",
    "        obj1 = GraphCutMutualInformationFunction(\n",
    "            n=len(ground_kernel),\n",
    "            num_queries=query_kernel.shape[1],\n",
    "            query_sijs=query_kernel,\n",
    "        )\n",
    "    elif fxn == \"LogDMI\":\n",
    "        obj1 = LogDeterminantMutualInformationFunction(\n",
    "            n=len(ground_kernel),\n",
    "            num_queries=query_kernel.shape[1],\n",
    "            lambdaVal=1,\n",
    "            query_sijs=query_kernel,\n",
    "            data_sijs=ground_kernel,\n",
    "            query_query_sijs=query_query_kernel,\n",
    "            magnificationEta=etaValue,\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n\\n\\n............... ERROR not a valid FUNCTION ............\\n\\n\\n\")\n",
    "        exit()\n",
    "    # print(f\"\\n{fxn} object created\\n\")\n",
    "    # print(\"\\ngenerating greedyList...\\n\")\n",
    "    greedyList = obj1.maximize(\n",
    "        budget=3 * budget_size,\n",
    "        optimizer=\"LazyGreedy\",\n",
    "        stopIfZeroGain=False,\n",
    "        stopIfNegativeGain=False,\n",
    "        epsilon=0.1,\n",
    "        verbose=False,\n",
    "        show_progress=True\n",
    "    )\n",
    "    # print(\"\\n.... greedyList generated ... \\n\")\n",
    "    return greedyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(file_dir, feature_type):\n",
    "    features = []\n",
    "    par_dir = os.path.dirname(file_dir)\n",
    "    file_name = os.path.basename(file_dir)\n",
    "    new_par_dir = os.path.join(par_dir, \"39\")\n",
    "    new_file_name = file_name.replace(\".json\", f\"_{feature_type}.file\")\n",
    "    new_path = os.path.join(new_par_dir, new_file_name)\n",
    "    with open(new_path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                features.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    # print(features.shape)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(base_dir, target_size, budget_size, accent, similarity, feature_type):\n",
    "    dirs = [\n",
    "        \"kannada_male_english\",\n",
    "        \"malayalam_male_english\",\n",
    "        \"rajasthani_male_english\",\n",
    "        \"hindi_male_english\",\n",
    "        \"tamil_male_english\",\n",
    "        \"gujarati_female_english\",\n",
    "        \"manipuri_female_english\",\n",
    "        \"assamese_female_english\",\n",
    "    ]\n",
    "\n",
    "    query_dir = f\"{accent}/\"\n",
    "    query_file_path = base_dir + query_dir + \"seed.json\"\n",
    "    query_list = [json.loads(line.strip()) for line in open(query_file_path)]\n",
    "    query_features = load_features(query_file_path, feature_type)\n",
    "    query_list, query_features = query_list[:target_size], query_features[:target_size]\n",
    "\n",
    "    ground_list, ground_list_Y, ground_features = [], [], []\n",
    "    for i, _dir in enumerate(dirs):\n",
    "        selection_file_path = base_dir + _dir + \"/\" + \"selection.json\"\n",
    "        selection_file_list = [\n",
    "            json.loads(line.strip()) for line in open(selection_file_path)\n",
    "        ]\n",
    "        ground_list.extend(selection_file_list)\n",
    "        ground_features.append(load_features(selection_file_path, feature_type))\n",
    "        ground_list_Y.extend([i] * len(selection_file_list))\n",
    "    ground_features = np.concatenate(ground_features, axis=0)\n",
    "    ground_features_Y = np.asarray(ground_list_Y).reshape(-1, 1)\n",
    "\n",
    "    ### test file\n",
    "    test_file_path = base_dir + query_dir + \"test.json\"\n",
    "    test_list = [json.loads(line.strip()) for line in open(test_file_path)]\n",
    "    test_features = load_features(test_file_path, feature_type)\n",
    "\n",
    "    # print(len(ground_list), ground_features.shape)\n",
    "    # print(len(query_list), query_features.shape)\n",
    "    # print(len(test_list), test_features.shape)\n",
    "\n",
    "    # print(\"creating kernels ....\")\n",
    "    t1 = time.time()\n",
    "    ground_kernel = create_kernel(ground_features, metric=similarity, mode=\"dense\")\n",
    "    query_kernel = create_kernel(\n",
    "        query_features, metric=similarity, mode=\"dense\", X_rep=ground_features\n",
    "    )\n",
    "    query_query_kernel = create_kernel(\n",
    "        query_features, metric=similarity, mode=\"dense\", X_rep=query_features\n",
    "    )\n",
    "    t2 = time.time()\n",
    "    # print(\"kernel creation done ....\", t2 - t1)\n",
    "\n",
    "    # print(\"ground_kernel: \", ground_kernel.shape)\n",
    "    # print(\"query_kernel: \", query_kernel.shape)\n",
    "    # print(\"query_query_kernel: \", query_query_kernel.shape)\n",
    "\n",
    "    return (\n",
    "        dirs,\n",
    "        query_dir,\n",
    "        ground_list,\n",
    "        ground_features,\n",
    "        ground_features_Y,\n",
    "        ground_kernel,\n",
    "        query_list,\n",
    "        query_features,\n",
    "        query_kernel,\n",
    "        query_query_kernel,\n",
    "        test_features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(\n",
    "    budget_size,\n",
    "    target_size,\n",
    "    etaValue,\n",
    "    similarity,\n",
    "    fxn,\n",
    "    accent,\n",
    "    feature_type,\n",
    "    left_end=50,\n",
    "    right_end=-1,\n",
    "):\n",
    "    base_dir = \"../../data/\"\n",
    "\n",
    "    (\n",
    "        dirs,\n",
    "        query_dir,\n",
    "        ground_list,\n",
    "        ground_features,\n",
    "        ground_features_Y,\n",
    "        ground_kernel,\n",
    "        query_list,\n",
    "        query_features,\n",
    "        query_kernel,\n",
    "        query_query_kernel,\n",
    "        test_features,\n",
    "    ) = preprocess(base_dir, target_size, budget_size, accent, similarity, feature_type)\n",
    "\n",
    "    greedyList = generate_greedyList(\n",
    "        ground_kernel, query_kernel, query_query_kernel, fxn, budget_size, etaValue\n",
    "    )\n",
    "    compute_subset(\n",
    "        dirs,\n",
    "        base_dir,\n",
    "        query_dir,\n",
    "        ground_list,\n",
    "        ground_features,\n",
    "        ground_features_Y,\n",
    "        query_list,\n",
    "        query_features,\n",
    "        test_features,\n",
    "        greedyList,\n",
    "        budget_size,\n",
    "        target_size,\n",
    "        accent,\n",
    "        fxn,\n",
    "        similarity,\n",
    "        etaValue,\n",
    "        feature_type,\n",
    "        left_end=left_end,\n",
    "        right_end=right_end,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0456d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"assamese_female_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"assamese_female_english\", \"39\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"manipuri_female_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"manipuri_female_english\", \"39\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"kannada_male_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"kannada_male_english\", \"39\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55867c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"hindi_male_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"hindi_male_english\", \"39\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"tamil_male_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"tamil_male_english\", \"39\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"rajasthani_male_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"rajasthani_male_english\", \"39\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6db23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"malayalam_male_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"malayalam_male_english\", \"39\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"FL2MI\", \"gujarati_female_english\", \"39\"\n",
    ")\n",
    "get_plots(\n",
    "    11000, 20, 1.0, \"cosine\", \"GCMI\", \"gujarati_female_english\", \"39\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16929df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "error",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 19 2021, 18:05:58) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc33b118a8b882057d92ab3e840283c71bfc0408e638fa49ffb4a6668b810896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
