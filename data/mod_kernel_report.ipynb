{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "from pathlib import Path\n",
    "from cmath import inf\n",
    "import re, os, csv, pathlib, ast, os.path\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accent_from_path(json_path):\n",
    "    pieces = json_path.split('/')\n",
    "    piece = [piece for piece in pieces if 'english' in piece]\n",
    "    piece = piece[0]\n",
    "    return replace_with_short_forms(piece)\n",
    "\n",
    "def replace_with_short_forms(s):\n",
    "    s = s.replace(\"english\", \"\")\n",
    "    s = s.replace(\"_female_\", \"[F]\")\n",
    "    s = s.replace(\"_male_\", \"[M]\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# def shortened_speaker(s):\n",
    "#     speaker, gender, _ = s.split(\"_\")\n",
    "#     return speaker[0].upper() + speaker[1:] + \"[\" + gender[0].upper() + \"]\"\n",
    "\n",
    "\n",
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "\n",
    "def get_each_run(lne):\n",
    "    # print(lne.strip())\n",
    "    # print(re.findall(\": (.+)\", lne))\n",
    "    # print(re.findall(\": (.+)\", lne)[0])\n",
    "    # print(list(map(float, re.findall(\": (.+)\", lne)[0].split(\" \"))))\n",
    "\n",
    "    return list(map(float, re.findall(\": (.+)\", lne)[0].split(\" \")))\n",
    "\n",
    "\n",
    "def get_selection_counts(s):\n",
    "    return list(map(replace_with_short_forms, re.findall(\"Counter\\\\((.+?)\\\\)\", s)))\n",
    "\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9] + \"run_{}/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "\n",
    "# def get_test(stats_file_path):\n",
    "#     return stats_file_path[:-9] + \"run_1/output/test_out.txt\"\n",
    "\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    try:\n",
    "        txt_file = open(test_file, \"r\")\n",
    "        lines = txt_file.readlines()\n",
    "        matched = \"\"\n",
    "        for line in lines:\n",
    "            if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "                txt_file.close()\n",
    "                return float(line.rstrip().split(\": \")[1])\n",
    "    except:\n",
    "        txt_file.close()\n",
    "        print(\"weiowdnio\")\n",
    "        return inf\n",
    "\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    return \"-n:\" + str(float(eta[4:]))\n",
    "\n",
    "\n",
    "def accent_distribution(json_path):\n",
    "    file = open(json_path, 'r')\n",
    "    paths = [accent_from_path(json.loads(sample)['audio_filepath'])[:3] for sample in file.readlines()]\n",
    "    counts = Counter(paths)\n",
    "    return dict(counts.most_common())\n",
    "\n",
    "def time_fraction(json_parent, speaker):\n",
    "    total_duration, domain_duration = 0, 0\n",
    "    for i in range(1,4):\n",
    "        json_path = \"{}/run_{}/train.json\".format(json_parent, i)\n",
    "        file = open(json_path, 'r')\n",
    "        samples = [sample for sample in file.readlines()]\n",
    "        total_duration += sum([json.loads(sample)['duration'] for sample in samples])\n",
    "        domain_duration += sum([json.loads(sample)['duration'] for sample in samples if speaker in json.loads(sample)['audio_filepath'].split('/')])\n",
    "#         print(speaker, json.loads(samples[0])['audio_filepath'])\n",
    "    total_duration/=3\n",
    "    domain_duration/=3\n",
    "    return \"{:.1f}/{:.1f}\".format(domain_duration, total_duration)\n",
    "\n",
    "def sample_fraction(json_parent, speaker):\n",
    "    # print(speaker)\n",
    "    total, domain_counts = 0, 0\n",
    "    for i in range(1,4):\n",
    "        json_path = \"{}/run_{}/train.json\".format(json_parent, i)\n",
    "        file = open(json_path, 'r')\n",
    "        lines = [line for line in file.readlines()]\n",
    "        total += len(lines)\n",
    "        domain_counts += len([json.loads(sample) for sample in lines if speaker in json.loads(\n",
    "            sample)['audio_filepath'].split('/')])\n",
    "    total/=3\n",
    "    domain_counts/=3\n",
    "    return \"{:.1f}/{:.1f}\".format(domain_counts, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(path):\n",
    "    try:\n",
    "        search_pattern = r\"/target_\\d*/\"\n",
    "        matched_string = re.search(search_pattern, path).group()\n",
    "        new_path = re.sub(search_pattern, os.path.sep, path)\n",
    "        target = matched_string.split('_')[1][:-1]\n",
    "        return new_path, target\n",
    "    except:\n",
    "        return path, \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "\n",
    "# csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "# df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "expt_results = []\n",
    "os_sep = os.path.sep\n",
    "submod_fxns = (\"FL1MI\", \"FL2MI\", \"GCMI\", \"LogDMI\")\n",
    "\n",
    "for speaker in speakers:\n",
    "    shortened_speaker = replace_with_short_forms(speaker)\n",
    "    print(\"Extracting experiments from {}\".format(shortened_speaker))\n",
    "    base_dir = os.path.join(\".\", f\"{speaker}\", \"all\")\n",
    "    all_paths = list(set([f\"{os.path.sep}\".join(str(p).split(os.path.sep)[:-2]) for p in Path(base_dir).rglob(\"*/test_infer_log.txt\")]))\n",
    "    for path in all_paths:\n",
    "        dct = {}\n",
    "        orig_path = str(path)\n",
    "        path = str(path).replace(f\"{speaker}/all/\", \"\")\n",
    "        dct[\"speaker\"] = shortened_speaker\n",
    "        \n",
    "        if path.startswith(\"budget_\"):\n",
    "            dct[\"method\"] = \"-\"\n",
    "        else:\n",
    "            if path.startswith(\"dim_phoneme_gains\"):\n",
    "                dct[\"method\"] = \"phone_decay-\"\n",
    "                path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "                tau_str = path.split(os.path.sep)[0]\n",
    "                tau_val = tau_str.split('_')[-1]\n",
    "                dct[\"method\"] += tau_val\n",
    "                path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "#             print(path)\n",
    "            else:\n",
    "                dct[\"method\"] = path.split(os.path.sep)[0]\n",
    "                path = path.replace(f\"{dct['method']}/\", \"\")\n",
    "            dct[\"other_accents\"] = replace_with_short_forms(path.split(os.path.sep)[0])\n",
    "            path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "        assert(path.startswith(\"budget_\"))\n",
    "        dct[\"budget_b1\"] = path.split(os.path.sep)[0].replace(\"budget_\", \"\")\n",
    "        path = path.replace(f\"budget_{dct['budget_b1']}/\", \"\")\n",
    "#         print(path, dct)\n",
    "        if path.startswith(\"random\"):\n",
    "            continue\n",
    "        dct[\"target\"] = path.split(os.path.sep)[0].replace(\"target_\", \"\")\n",
    "        path = path.replace(f\"target_{dct['target']}/\", \"\")\n",
    "        if path.split(os.path.sep)[0] == \"SM_select\":\n",
    "            dct[\"method\"] += \"SM_select\"\n",
    "            path = path.replace(\"SM_select/\", \"\")\n",
    "            ls = path.split(os.path.sep)\n",
    "            assert(len(ls) == 3)\n",
    "            dct[\"fxn\"] = \"-\".join([ls[0], ls[2]])\n",
    "            dct[\"accent_features\"] = ls[1]\n",
    "            path = \"\"\n",
    "        fxn = path.split(os.path.sep)[0]\n",
    "        if fxn in submod_fxns:\n",
    "            dct[\"fxn\"] = fxn\n",
    "#             print(fxn)\n",
    "            dct[\"etaScale\"] = \"default(1.0)\"\n",
    "        elif not path: \n",
    "            pass\n",
    "        else:\n",
    "            ls = fxn.split(\"_\")\n",
    "            print(fxn)\n",
    "            assert(len(ls) == 3)\n",
    "            assert(ls[1] == \"etaScale\")\n",
    "            assert(ls[0] in submod_fxns)\n",
    "            dct[\"fxn\"] = ls[0]\n",
    "            dct[\"etaScale\"] = ls[2]\n",
    "            # print(ls[2])\n",
    "        path = path.replace(f\"{fxn}/\", \"\")\n",
    "        \n",
    "        if path.startswith(\"39/\"):\n",
    "            dct[\"accent_features\"] = \"39\"\n",
    "            path = path.replace(f\"{dct['accent_features']}/\", \"\")\n",
    "        elif not path: \n",
    "            pass\n",
    "        else:\n",
    "            assert(path.startswith(\"accent_\"))\n",
    "            dct[\"accent_features\"] = path.split(os.path.sep)[0].replace(\"accent_\", \"\")\n",
    "            path = path.replace(f\"accent_{dct['accent_features']}/\", \"\")\n",
    "        \n",
    "        \n",
    "        if path.startswith(\"budget_\"):\n",
    "            # print(path)\n",
    "            dct[\"budget_b2\"] = path.split(os.path.sep)[0].replace(\"budget_\", \"\")\n",
    "            path = path.replace(f\"budget_{dct['budget_b2']}/\", \"\")\n",
    "            # print(path)\n",
    "            dct[\"method\"] = path.split(os.path.sep)[0] + \"(stage2)\"\n",
    "            # print(dct[\"method\"])\n",
    "        elif not path: \n",
    "            pass\n",
    "        else:\n",
    "            assert(path.startswith(\"content_\"))\n",
    "            dct[\"content_features\"] = path.split(os.path.sep)[0].replace(\"content_\", \"\")\n",
    "            path = path.replace(f\"content_{dct['content_features']}/\", \"\")\n",
    "    #         print(path, dct)\n",
    "            if path.startswith(\"phoneme_\"):\n",
    "                path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "            \n",
    "            kernels = path.split(os.path.sep)[0].replace(\"kernel_\", \"\")\n",
    "            kernels_ls = \"; \".join(kernels.split(\"_\"))\n",
    "            dct[\"kernels\"] = kernels_ls\n",
    "            path = path.replace(f\"kernel_{kernels}/\", \"\")\n",
    "            assert(path.startswith(\"accent_\"))\n",
    "            dct[\"accent_sim\"] = path.split(os.path.sep)[0].replace(\"accent_\", \"\")\n",
    "            path = path.replace(f\"accent_{dct['accent_sim']}/\", \"\")\n",
    "            \n",
    "            assert(path.startswith(\"content_\"))\n",
    "            dct[\"content_sim\"] = path.split(os.path.sep)[0].replace(\"content_\", \"\")\n",
    "            path = path.replace(f\"content_{dct['content_sim']}/\", \"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            dct[\"time_fraction\"] = time_fraction(orig_path, speaker)\n",
    "            dct[\"sample_fraction\"] = sample_fraction(orig_path, speaker)\n",
    "            \n",
    "            dct[\"speakers\"] = accent_distribution(os.path.join(orig_path, \"run_1/train.json\"))\n",
    "            \n",
    "            wer_list = []\n",
    "            for run in range(1,4):\n",
    "                test_file_path = os.path.join(orig_path, f\"run_{run}\", \"test_infer_log.txt\")\n",
    "                wer_list.append(WER_test_file(test_file_path))\n",
    "            \n",
    "    #         print(wer_list, len(wer_list))\n",
    "                \n",
    "            mean = np.nanmean(wer_list)\n",
    "            var = np.nanvar(wer_list)\n",
    "            dct[\"WER-mean\"] = round(mean, 2)\n",
    "            dct[\"WER-stdev\"] = round(var**0.5, 2)\n",
    "            for run in range(1, 4):\n",
    "                dct[f\"WER-r{run}\"] = wer_list[run - 1]\n",
    "            \n",
    "            expt_results.append(dct)\n",
    "            \n",
    "        except:\n",
    "            print(orig_path, dct[\"method\"], dct[\"budget_b1\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"End of {}\".format(shortened_speaker))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"other_accents\",\n",
    "    \"budget_b1\",\n",
    "    \"target\",\n",
    "    \"fxn\",\n",
    "    \"method\",\n",
    "    \"budget_b2\",\n",
    "    \"etaScale\",\n",
    "    \"accent_features\",\n",
    "    \"content_features\",\n",
    "    \"accent_sim\",\n",
    "    \"content_sim\",\n",
    "    \"kernels\",\n",
    "    \"time_fraction\",\n",
    "    \"sample_fraction\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(expt_results, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"budget_b1\"] == \"3500\") | (df[\"budget_b1\"] == \"250\")\n",
    "filtered_df = df.loc[mask]\n",
    "filtered_df = filtered_df.sort_values([\"speaker\", \"method\"])\n",
    "filtered_df.to_csv(\"INDIC-results.csv\", index=False)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"budget_b1\"] == \"3500\")\n",
    "filtered_df = df.loc[mask]\n",
    "filtered_df = filtered_df.sort_values([\"speaker\", \"method\"])\n",
    "filtered_df.to_csv(\"INDIC-results.csv\", index=False)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"method\"] == \"mixed_query_set\") & (df[\"budget_b1\"] == \"750\")\n",
    "filtered_df = df.loc[mask]\n",
    "filtered_df = filtered_df.sort_values([\"other_accents\", \"fxn\", \"speaker\"])\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"budget_750_mixed_query_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"method\"].str.startswith(\"phone_decay\"))\n",
    "# mask = (((df[\"method\"].str.endswith(\"random(stage2)\")) | (df[\"method\"].str.startswith(\"uniform\"))) & (df[\"budget_b1\"] == \"3500\") & (df[\"budget_b2\"] == \"150\")) \n",
    "filtered_df = df.loc[mask]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.sort_values([\"other_accents\", \"fxn\", \"speaker\"])\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"report-21st-oct2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = list(range(1, len(df[\"etaScale\"]) + 1))\n",
    "plt.errorbar(x, df[\"WER-mean\"], df[\"WER-stdev\"], marker=\"o\", ecolor=\"red\")\n",
    "plt.xticks(x, df[\"etaScale\"])\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = list(range(1, len(df[\"etaScale\"]) + 1))\n",
    "plt.plot(x, df[\"WER-mean\"])\n",
    "plt.xticks(x, df[\"etaScale\"])\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# speakered selection: 76 76 76 -> 76.00\n",
    "# speakered duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "# Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"base_eta\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    # \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    # \"rajasthani_male_english\",\n",
    "    # \"hindi_male_english\",\n",
    "    # \"malayalam_male_english\",\n",
    "    # \"tamil_male_english\",\n",
    "    # \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        (func, base_eta, etaScale) = split_function(function)\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for accent_similarity in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for content_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}\"\n",
    "                    ):\n",
    "                        stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                        if not (os.path.isfile(stats_file_path)):\n",
    "                            continue\n",
    "                        stats_file = open(stats_file_path, \"r\")\n",
    "                        lines = stats_file.readlines()\n",
    "                        # print(\"lines length \", len(lines), lines)\n",
    "                        (\n",
    "                            total_selections,\n",
    "                            total_durations,\n",
    "                            speakered_selections,\n",
    "                            speakered_durations,\n",
    "                        ) = map(get_each_run, lines[:4])\n",
    "                        # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                        sample_frac = mean(\n",
    "                            [\n",
    "                                x[0] / x[1]\n",
    "                                for x in zip(speakered_selections, total_selections)\n",
    "                            ]\n",
    "                        )\n",
    "                        sample_total = mean(total_selections)\n",
    "                        duration_frac = mean(\n",
    "                            [\n",
    "                                x[0] / x[1]\n",
    "                                for x in zip(speakered_durations, total_durations)\n",
    "                            ]\n",
    "                        )\n",
    "                        duration_total = mean(total_durations)\n",
    "                        df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                            duration_total * duration_frac, duration_total\n",
    "                        )\n",
    "                        df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                            sample_total * sample_frac, sample_total\n",
    "                        )\n",
    "                        df_selections = get_selection_counts(lines[4])\n",
    "                        try:\n",
    "                            wers = [\n",
    "                                WER_test_file(\n",
    "                                    get_test_file_from_stats_path(i, stats_file)\n",
    "                                )\n",
    "                                for i in range(1, 4)\n",
    "                            ]\n",
    "                            df_wer_mean = round(mean(wers), 2)\n",
    "                            df_wer_stdev = round(variance(wers), 3) ** 0.5\n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            base_eta,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections,\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                        except:\n",
    "                            #                     continue\n",
    "                            print(\n",
    "                                \"no WER's in file\",\n",
    "                                get_test_file_from_stats_path(1, stats_file),\n",
    "                            )\n",
    "                            wers = [0, 0, 0]\n",
    "                            df_wer_mean = 0\n",
    "                            df_wer_stdev = 0\n",
    "                        # df = df.append(\n",
    "                        #     dict(\n",
    "                        #         zip(\n",
    "                        #             cols,\n",
    "                        #             [\n",
    "                        #                 speaker,\n",
    "                        #                 func,\n",
    "                        #                 base_eta,\n",
    "                        #                 etaScale,\n",
    "                        #                 target,\n",
    "                        #                 # accent_features,\n",
    "                        #                 # content_features,\n",
    "                        #                 # accent_similarity,\n",
    "                        #                 # content_similarity,\n",
    "                        #                 df_duration,\n",
    "                        #                 df_samples,\n",
    "                        #             ]\n",
    "                        #             + wers\n",
    "                        #             + [df_wer_mean, df_wer_stdev]\n",
    "                        #             + df_selections,\n",
    "                        #         )\n",
    "                        #     ),\n",
    "                        #     ignore_index=True,\n",
    "                        # )\n",
    "                        stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"base_eta\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            if not accent_features.endswith(\"_3rep\"):\n",
    "                continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mod_rep_csv_150_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            # if not accent_features.endswith(\"_3rep\"):\n",
    "            #     continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mix_query_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        if etaScale != \"1.0\":\n",
    "            continue\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            # if not accent_features.endswith(\"_3rep\"):\n",
    "            #     continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "#         continue\n",
    "#     pick_from = \"all\"\n",
    "#     if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "#         continue\n",
    "#     for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "#         (func, base_eta, etaScale) = split_function(function)\n",
    "#         for accent_features in get_dirs(\n",
    "#             f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "#         ):\n",
    "#             for content_features in get_dirs(\n",
    "#                 f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "#             ):\n",
    "#                 for accent_similarity in get_dirs(\n",
    "#                     f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "#                 ):\n",
    "#                     for content_similarity in get_dirs(\n",
    "#                         f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}\"\n",
    "#                     ):\n",
    "#                         stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "#                         if not (os.path.isfile(stats_file_path)):\n",
    "#                             continue\n",
    "#                         stats_file = open(stats_file_path, \"r\")\n",
    "#                         lines = stats_file.readlines()\n",
    "#                         # print(\"lines length \", len(lines), lines)\n",
    "#                         (\n",
    "#                             total_selections,\n",
    "#                             total_durations,\n",
    "#                             speakered_selections,\n",
    "#                             speakered_durations,\n",
    "#                         ) = map(get_each_run, lines[:4])\n",
    "#                         # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "#                         sample_frac = mean(\n",
    "#                             [\n",
    "#                                 x[0] / x[1]\n",
    "#                                 for x in zip(speakered_selections, total_selections)\n",
    "#                             ]\n",
    "#                         )\n",
    "#                         sample_total = mean(total_selections)\n",
    "#                         duration_frac = mean(\n",
    "#                             [\n",
    "#                                 x[0] / x[1]\n",
    "#                                 for x in zip(speakered_durations, total_durations)\n",
    "#                             ]\n",
    "#                         )\n",
    "#                         duration_total = mean(total_durations)\n",
    "#                         df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "#                             duration_total * duration_frac, duration_total\n",
    "#                         )\n",
    "#                         df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "#                             sample_total * sample_frac, sample_total\n",
    "#                         )\n",
    "#                         df_selections = get_selection_counts(lines[4])\n",
    "#                         try:\n",
    "#                             wers = [\n",
    "#                                 WER_test_file(\n",
    "#                                     get_test_file_from_stats_path(i, stats_file)\n",
    "#                                 )\n",
    "#                                 for i in range(1, 4)\n",
    "#                             ]\n",
    "#                             df_wer_mean = round(mean(wers), 2)\n",
    "#                             df_wer_stdev = round(variance(wers), 3) ** 0.5\n",
    "#                             df = df.append(\n",
    "#                                 dict(\n",
    "#                                     zip(\n",
    "#                                         cols,\n",
    "#                                         [\n",
    "#                                             speaker,\n",
    "#                                             func,\n",
    "#                                             base_eta,\n",
    "#                                             etaScale,\n",
    "#                                             target,\n",
    "#                                             # accent_features,\n",
    "#                                             # content_features,\n",
    "#                                             # accent_similarity,\n",
    "#                                             # content_similarity,\n",
    "#                                             df_duration,\n",
    "#                                             df_samples,\n",
    "#                                         ]\n",
    "#                                         + wers\n",
    "#                                         + [df_wer_mean, df_wer_stdev]\n",
    "#                                         + df_selections,\n",
    "#                                     )\n",
    "#                                 ),\n",
    "#                                 ignore_index=True,\n",
    "#                             )\n",
    "#                         except:\n",
    "#                             #                     continue\n",
    "#                             print(\n",
    "#                                 \"no WER's in file\",\n",
    "#                                 get_test_file_from_stats_path(1, stats_file),\n",
    "#                             )\n",
    "#                             wers = [0, 0, 0]\n",
    "#                             df_wer_mean = 0\n",
    "#                             df_wer_stdev = 0\n",
    "#                         # df = df.append(\n",
    "#                         #     dict(\n",
    "#                         #         zip(\n",
    "#                         #             cols,\n",
    "#                         #             [\n",
    "#                         #                 speaker,\n",
    "#                         #                 func,\n",
    "#                         #                 base_eta,\n",
    "#                         #                 etaScale,\n",
    "#                         #                 target,\n",
    "#                         #                 # accent_features,\n",
    "#                         #                 # content_features,\n",
    "#                         #                 # accent_similarity,\n",
    "#                         #                 # content_similarity,\n",
    "#                         #                 df_duration,\n",
    "#                         #                 df_samples,\n",
    "#                         #             ]\n",
    "#                         #             + wers\n",
    "#                         #             + [df_wer_mean, df_wer_stdev]\n",
    "#                         #             + df_selections,\n",
    "#                         #         )\n",
    "#                         #     ),\n",
    "#                         #     ignore_index=True,\n",
    "#                         # )\n",
    "#                         stats_file.close()\n",
    "# df = df.sort_values(\n",
    "#     by=[\n",
    "#         \"speaker\",\n",
    "#         # \"accent_features\",\n",
    "#         # \"content_features\",\n",
    "#         \"function\",\n",
    "#         \"base_eta\",\n",
    "#         \"etaScale\",\n",
    "#     ],\n",
    "#     ascending=True,\n",
    "#     ignore_index=True,\n",
    "# )\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    selection_json = f\"./{speaker}/selection.json\"\n",
    "    print(speaker, end=\" \")\n",
    "    with open(selection_json) as file:\n",
    "        print(len(file.readlines()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('error')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 19 2021, 18:05:58) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc33b118a8b882057d92ab3e840283c71bfc0408e638fa49ffb4a6668b810896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
