{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, os, sys, json, librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import islice\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=22050,\n",
    "    n_mfcc=39,\n",
    "    melkwargs={\n",
    "      'n_fft': 2048,\n",
    "      'n_mels': 256,\n",
    "      'hop_length': 512,\n",
    "      'mel_scale': 'htk',\n",
    "    }\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'philippines': 0, 'wales': 1, 'scotland': 2, 'hongkong': 3, 'malaysia': 4, 'indian': 5, 'australia': 6, 'us': 7, 'southatlandtic': 8, 'england': 9, 'canada': 10, 'ireland': 11, 'newzealand': 12, 'bermuda': 13, 'singapore': 14, 'african': 15}\n",
      "[3217, 452, 12580, 1613, 1057, 59797, 31943, 224790, 33, 64328, 34529, 6262, 4080, 248, 2556, 4197]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "json_folder = '/home/mayank/MTP/begin_again/Error-Driven-ASR-Personalization/MCV_accent/jsons/'\n",
    "jsons = [f.name for f in os.scandir(json_folder) if 'json' in f.name and f.name.split('.')[0] not in ['unlabelled', 'other']]\n",
    "counts = [sum(1 for line in open('jsons/'+f)) for f in jsons]\n",
    "labels = [f.split('.json')[0] for f in jsons]\n",
    "labels2id = dict([(y,x) for x, y in enumerate(labels)])\n",
    "id2labels = dict([(x,y) for x, y in enumerate(labels)])\n",
    "num_classes = len(labels)\n",
    "print(labels2id)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        jsons = [f.name for f in os.scandir(json_folder) if 'json' in f.name and f.name.split('.')[0] not in ['unlabelled', 'other']]\n",
    "        features, labels = [], []\n",
    "        \n",
    "        for accent in jsons:\n",
    "            print(\"loading\", accent)\n",
    "            json_path = json_folder + accent\n",
    "            json_file = open(json_path)\n",
    "#             json_item_list = [line for line in json_file]\n",
    "            json_item_list = list(islice(json_file, 1000))\n",
    "            print(len(json_item_list), \"samples\")\n",
    "            json_item_list = [json.loads(line.strip()) for line in json_item_list]\n",
    "            \n",
    "            for sample in tqdm(json_item_list):\n",
    "                if librosa.get_duration(filename=sample['audio_filepath']) > 30:\n",
    "                    continue\n",
    "                try:\n",
    "                    waveform, sample_rate = torchaudio.load(sample['audio_filepath'])\n",
    "                    features.append(mfcc_transform(waveform).mean(2).detach().numpy())\n",
    "                    labels.append(sample['accent'])\n",
    "                except Exception as e:\n",
    "                    print(str(sample['audio_filepath']), e)\n",
    "                    pass\n",
    "            print(\"finished\")\n",
    "        self.X = np.concatenate(features, axis=0)\n",
    "#         self.Y = np.eye(num_classes, dtype='uint8')[labels]\n",
    "        self.Y = np.array([labels2id[sample] for sample in labels])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.Y[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10b89ZpLDqx9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading philippines.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:48<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading wales.json\n",
      "452 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 452/452 [04:26<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading scotland.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:47<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading hongkong.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:53<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading malaysia.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:51<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading indian.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:54<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading australia.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:58<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading us.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:59<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading southatlandtic.json\n",
      "33 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:19<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "loading england.json\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 655/1000 [06:29<03:19,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "dataset = accentDataset(json_folder)\n",
    "np.save('dataset.pkl', dataset, allow_pickle =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10b89ZpLDqx9"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "dataset = np.load('dataset.pkl')\n",
    "# generate indices: instead of the actual data we pass in integers instead\n",
    "train_indices, test_indices, _, _ = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    dataset.Y,\n",
    "    stratify=dataset.Y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# generate subset based on indices\n",
    "train_split = Subset(dataset, train_indices)\n",
    "test_split = Subset(dataset, test_indices)\n",
    "\n",
    "# create batches\n",
    "train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_batches = DataLoader(test_split, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset type:{}, size:{}\".format(type(dataset), len(dataset)))\n",
    "print(\"train_split type:{}, size:{}\".format(type(train_split), len(train_split)))\n",
    "print(\"test_split type:{}, size:{}\".format(type(test_split), len(test_split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "feature_size = 39\n",
    "\n",
    "class classifierHead(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(feature_size, feature_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.out_proj = nn.Linear(feature_size, num_classes)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEwVoZVPNaha"
   },
   "outputs": [],
   "source": [
    "learning_rate, epochs = 0.001, 50\n",
    "model = classifierHead()\n",
    "print(model)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# keeping-track-of-losses \n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # keep-track-of-training-and-validation-loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # training-the-model\n",
    "    model.train()\n",
    "    for data, target in train_batches:\n",
    "        # move-tensors-to-GPU \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # clear-the-gradients-of-all-optimized-variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
    "        output = model(data)\n",
    "        # calculate-the-batch-loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n",
    "        loss.backward()\n",
    "        # perform-a-ingle-optimization-step (parameter-update)\n",
    "        optimizer.step()\n",
    "        # update-training-loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    # validate-the-model\n",
    "    model.eval()\n",
    "    for data, target in test_batches:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # update-average-validation-loss \n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    # calculate-average-losses\n",
    "    train_loss = train_loss/len(train_batches.sampler)\n",
    "    valid_loss = valid_loss/len(test_batches.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "        \n",
    "    # print-training/validation-statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-the-model\n",
    "model.eval()  # it-disables-dropout\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_batches:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "          \n",
    "    print('Test Accuracy of the model: {:.2f} %'.format(100 * correct / total))\n",
    "\n",
    "# Save \n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(frameon=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion recognition in Greek speech using Wav2Vec2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "w2v2",
   "language": "python",
   "name": "w2v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
