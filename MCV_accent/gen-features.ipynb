{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45f8289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af3700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torchaudio, os, sys, json, pickle, librosa\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb70663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    hidden_rep: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x0 = self.dropout(x)\n",
    "#         print('----------------------------')\n",
    "#         print(x0[:,-10:])\n",
    "#         print(x0.shape)\n",
    "#         print('----------------------------')\n",
    "        x1 = self.out_proj(x0)\n",
    "        return x0, x1\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(self, hidden_states, mode=\"mean\"):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        hidden_rep, logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (hidden_rep + logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            hidden_rep=hidden_rep\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07c65b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bdbea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"/mnt/data/aman/mayank/MTP/mount_points/jan_19/Error-Driven-ASR-Personalization/MCV_accent/data/dristi_accent-recognition/checkpoint-6400/\"\n",
    "model_name_or_path = \"/home/mayank/MTP/begin_again/Error-Driven-ASR-Personalization/MCV_accent/data/dristi_accent-recognition/checkpoint-6400/\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d246806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def predict(path, sampling_rate):\n",
    "#     print(path)\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model(input_values, attention_mask=attention_mask)\n",
    "        logits = op.logits\n",
    "        hidden_rep = op.hidden_rep\n",
    "        \n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Accent\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    return outputs, hidden_rep\n",
    "\n",
    "def prediction(df_row):\n",
    "    if 'path' in df_row: path = df_row[\"path\"]\n",
    "    else: path = df_row[\"audio_filepath\"]\n",
    "    speech, sr = torchaudio.load(path)\n",
    "    speech = speech[0].numpy().squeeze()\n",
    "    speech = librosa.resample(np.asarray(speech), sr, sampling_rate)\n",
    "    outputs, hidden_rep = predict(path, sampling_rate)\n",
    "#     print(hidden_rep[:,-10:])\n",
    "    return hidden_rep\n",
    "\n",
    "def extract_features(file_list, file_dir):\n",
    "    with open(file_dir.replace('.json', '_w2v2.file'), 'wb') as f:\n",
    "        for file in file_list:\n",
    "            w2v2_features = prediction(file).cpu().detach().numpy()\n",
    "            pickle.dump(w2v2_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c92018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['philippines.json', 'indian.json', 'australia.json', 'us.json', 'england.json', 'canada.json', 'african.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "../MCV_accent/invalidated/philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 1/7 [07:12<43:15, 432.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "____________________ \n",
      "\n",
      "\n",
      "____________________\n",
      "../MCV_accent/invalidated/indian.json\n"
     ]
    }
   ],
   "source": [
    "jsons_path = '.json'\n",
    "jsons = [f.name for f in os.scandir('../MCV_accent/invalidated/') if '.json' in f.name and f.name.split('.')[0] not in ['unlabelled',\n",
    "                                                                                                              'other']]\n",
    "print(jsons)\n",
    "\n",
    "for file in tqdm(jsons):\n",
    "    print('_'*20)\n",
    "    \n",
    "    \n",
    "    json_file_path = '../MCV_accent/invalidated/' + file \n",
    "    json_file = open(json_file_path)\n",
    "    json_list = [json.loads(line.strip()) for line in json_file]\n",
    "    print(json_file_path)\n",
    "    \n",
    "    extract_features(json_list, json_file_path)\n",
    "    print(len(json_list))\n",
    "    print('_'*20, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63971c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "for fxn in ['FL2MI', 'GCMI', 'LogDMI']:\n",
    "    for run in [1, 2, 3]:\n",
    "        json_file_path = f'../mz-expts/african/manifests/TSS_output/all/budget_100/target_10/{fxn}/eta_1.0/euclidean/w2v2/run_{run}/train/train.json'\n",
    "        json_file = open(json_file_path)\n",
    "        json_list = [json.loads(line.strip()) for line in json_file]\n",
    "        count = 0\n",
    "        alarm = 0\n",
    "        for f in json_list:\n",
    "            if not os.path.exists(f['audio_filepath']):\n",
    "                mp3path = f['audio_filepath']\n",
    "                mp3path = path.replace('/wav/', '/clips/')\n",
    "                mp3path = path.replace('wav', 'mp3')\n",
    "                count += 1\n",
    "                if not os.path.exists(mp3path):\n",
    "                    alarm += 1\n",
    "                sound = AudioSegment.from_mp3(mp3path)\n",
    "                sound.export(f['audio_filepath'], format=\"wav\")\n",
    "        print(len(json_list))\n",
    "        print(count)\n",
    "        print(alarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in json_list:\n",
    "    if not os.path.exists(f['audio_filepath']):\n",
    "        count += 1\n",
    "print(len(json_list))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719382b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f931774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4bb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v2",
   "language": "python",
   "name": "w2v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
