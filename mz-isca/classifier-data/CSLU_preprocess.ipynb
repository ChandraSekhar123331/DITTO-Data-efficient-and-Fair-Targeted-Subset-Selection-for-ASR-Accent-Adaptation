{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e4abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbed5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR  CA\tFA  GE\tHU  IT\tKO  MY\tPP  SD\tSW  VI\n",
      "BP  CZ\tFR  HI\tIN  JA\tMA  PO\tRU  SP\tTA\n",
      "AR  BP\tCA  FA\tFR  GE\tHI  HU\tIN  IT\tJA  KO\tMY  PO\tPP  RU\tSD  SP\tSW  VI\n"
     ]
    }
   ],
   "source": [
    "!ls ../../chandra/CSLU/data/agg-0/\n",
    "!ls ../../chandra/CSLU/data/mod-agg-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc6c8b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['IN',\n",
       "  'PO',\n",
       "  'AR',\n",
       "  'PP',\n",
       "  'KO',\n",
       "  'HU',\n",
       "  'SW',\n",
       "  'SP',\n",
       "  'FR',\n",
       "  'SD',\n",
       "  'MY',\n",
       "  'JA',\n",
       "  'VI',\n",
       "  'IT',\n",
       "  'BP',\n",
       "  'FA',\n",
       "  'HI',\n",
       "  'CA',\n",
       "  'RU',\n",
       "  'GE'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "#dataroot = '/exp/data/CSLU-AccentedEnglish/cslu_fae/speech'\n",
    "dataroot = '../../chandra/CSLU/data/mod-agg-3'\n",
    "\n",
    "dirs = [f for f in listdir(dataroot)]\n",
    "len(dirs), dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01714b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR  Arabic BP  Brazillian Portuguese CA  Cantonese CZ  Czech FA  Farsi FR  French GE  German HI  Hindi HU  Hungarian IN  Indonesian IT  Italian JA  Japanese KO  Korean MA  Mandarin MY  Malay PO  Polish PP  Iberian Portuguese RU  Russian SD  Swedish SP  Spanish SW  Swahili TA  Tamil VI  Vietnamese\n"
     ]
    }
   ],
   "source": [
    "# s = input()\n",
    "## AR  Arabic BP  Brazillian Portuguese CA  Cantonese CZ  Czech FA  Farsi FR  French GE  German HI  Hindi HU  Hungarian IN  Indonesian IT  Italian JA  Japanese KO  Korean MA  Mandarin MY  Malay PO  Polish PP  Iberian Portuguese RU  Russian SD  Swedish SP  Spanish SW  Swahili TA  Tamil VI  Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8136794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AR': 'Arabic ', 'BP': 'Brazillian Portuguese ', 'CA': 'Cantonese ', 'CZ': 'Czech ', 'FA': 'Farsi ', 'FR': 'French ', 'GE': 'German ', 'HI': 'Hindi ', 'HU': 'Hungarian ', 'IN': 'Indonesian ', 'IT': 'Italian ', 'JA': 'Japanese ', 'KO': 'Korean ', 'MA': 'Mandarin ', 'MY': 'Malay ', 'PO': 'Polish ', 'PP': 'Iberian Portuguese ', 'RU': 'Russian ', 'SD': 'Swedish ', 'SP': 'Spanish ', 'SW': 'Swahili ', 'TA': 'Tamil ', 'VI': 'Vietnamese '}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "st = s.split()\n",
    "accent_mp = {}\n",
    "for i in range(len(st)):\n",
    "    if len(st[i]) == 2:\n",
    "        accent_mp[st[i]] = ''\n",
    "        key = st[i]\n",
    "    else:\n",
    "        accent_mp[key] = accent_mp[key] + f'{st[i]} '\n",
    "print(accent_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91acc355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian  \t 277\n",
      "Polish  \t 443\n",
      "Arabic  \t 327\n",
      "Iberian Portuguese  \t 203\n",
      "Korean  \t 479\n",
      "Hungarian  \t 800\n",
      "Swahili  \t 180\n",
      "Spanish  \t 663\n",
      "French  \t 841\n",
      "Swedish  \t 637\n",
      "Malay  \t 156\n",
      "Japanese  \t 496\n",
      "Vietnamese  \t 255\n",
      "Italian  \t 635\n",
      "Brazillian Portuguese  \t 1558\n",
      "Farsi  \t 747\n",
      "Hindi  \t 1123\n",
      "Cantonese  \t 677\n",
      "Russian  \t 667\n",
      "German  \t 891\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "for _dir in dirs:\n",
    "    print(f'{accent_mp[_dir]} \\t {len(listdir(os.path.join(dataroot, _dir)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7fc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, sys, json, random\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f692498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadingloadingloadingloadingloadingloadingloadingloadingloadingloadingloadingloading          loading  Russian Brazillian Portuguese German Hindi loadingloadingMalay Swedish Polish Indonesian Spanish Farsi Arabic  Iberian Portuguese \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Hungarian \n",
      "Korean \n",
      "Cantonese \n",
      "\n",
      "loading Japanese \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:00<00:00, 346.29it/s]]\n",
      "100%|██████████| 203/203 [00:00<00:00, 358.18it/s]]\n",
      "100%|██████████| 277/277 [00:00<00:00, 404.81it/s]]\n",
      "100%|██████████| 327/327 [00:00<00:00, 422.84it/s]\n",
      "100%|██████████| 443/443 [00:00<00:00, 467.52it/s]]\n",
      "100%|██████████| 479/479 [00:00<00:00, 533.41it/s]]\n",
      "100%|██████████| 496/496 [00:00<00:00, 509.26it/s]]\n",
      "100%|██████████| 637/637 [00:01<00:00, 548.14it/s]]\n",
      "100%|██████████| 663/663 [00:01<00:00, 553.99it/s]\n",
      "100%|██████████| 677/677 [00:01<00:00, 595.75it/s]\n",
      "100%|██████████| 667/667 [00:01<00:00, 544.48it/s]\n",
      "100%|██████████| 747/747 [00:01<00:00, 592.02it/s]]\n",
      "100%|██████████| 800/800 [00:01<00:00, 618.03it/s]]\n",
      "100%|██████████| 891/891 [00:01<00:00, 621.36it/s]\n",
      "100%|██████████| 1123/1123 [00:01<00:00, 686.78it/s]\n",
      "100%|██████████| 1558/1558 [00:02<00:00, 744.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10144\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "def process_samples(_dir):\n",
    "    print(\"loading\", accent_mp[_dir])\n",
    "    samples_lst = [item for item in os.listdir(os.path.join(dataroot, _dir))]\n",
    "    for sample in tqdm(samples_lst):\n",
    "        try:\n",
    "            name = sample.replace('.wav', '')\n",
    "            path = os.path.join(dataroot, _dir, sample)\n",
    "            duration = librosa.get_duration(filename=path)\n",
    "            if duration > 30:\n",
    "                continue\n",
    "            L.append({\n",
    "                \"name\": name,\n",
    "                \"path\": path,\n",
    "                \"accent\": accent_mp[_dir]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(str(path), e)\n",
    "            pass\n",
    "\n",
    "all_accents = list(accent_mp.keys())\n",
    "not_accents = ['CZ', 'MA', 'TA', 'FR', 'IT', 'SW', 'VI']\n",
    "train_accents = list(set(all_accents) - set(not_accents))\n",
    "\n",
    "manager = Manager()\n",
    "L = manager.list()\n",
    "pool = Pool(16)\n",
    "pool.map(process_samples, train_accents)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(len(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcd6e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10144\n",
      "[{'name': 'FAR00523-chunk-00', 'path': '../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-chunk-00.wav', 'accent': 'Arabic '}, {'name': 'FIN00145-chunk-02', 'path': '../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-chunk-02.wav', 'accent': 'Indonesian '}, {'name': 'FAR00576-chunk-00', 'path': '../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-chunk-00.wav', 'accent': 'Arabic '}]\n"
     ]
    }
   ],
   "source": [
    "data = list(L)\n",
    "print(len(data))\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38eb6295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAR00523-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIN00145-chunk-02</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAR00576-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FMY00139-chunk-01</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...</td>\n",
       "      <td>Malay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FPP00035-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...</td>\n",
       "      <td>Iberian Portuguese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                               path  \\\n",
       "0  FAR00523-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...   \n",
       "1  FIN00145-chunk-02  ../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...   \n",
       "2  FAR00576-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...   \n",
       "3  FMY00139-chunk-01  ../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...   \n",
       "4  FPP00035-chunk-00  ../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...   \n",
       "\n",
       "                accent  \n",
       "0              Arabic   \n",
       "1          Indonesian   \n",
       "2              Arabic   \n",
       "3               Malay   \n",
       "4  Iberian Portuguese   "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4943f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['mp3path'] = df['path'].str.replace('.wav', '.mp3').replace('wav', 'clips')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccd3f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: 10144\n",
      "Step 1: 10144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAR00523-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIN00145-chunk-02</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...</td>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAR00576-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FMY00139-chunk-01</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...</td>\n",
       "      <td>Malay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FPP00035-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...</td>\n",
       "      <td>Iberian Portuguese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                               path  \\\n",
       "0  FAR00523-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...   \n",
       "1  FIN00145-chunk-02  ../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...   \n",
       "2  FAR00576-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...   \n",
       "3  FMY00139-chunk-01  ../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...   \n",
       "4  FPP00035-chunk-00  ../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...   \n",
       "\n",
       "                accent  \n",
       "0              Arabic   \n",
       "1          Indonesian   \n",
       "2              Arabic   \n",
       "3               Malay   \n",
       "4  Iberian Portuguese   "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filter broken and non-existed paths\n",
    "\n",
    "print(f\"Step 0: {len(df)}\")\n",
    "\n",
    "df[\"status\"] = df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
    "df = df.dropna(subset=[\"path\"])\n",
    "df = df.drop(\"status\", 1)\n",
    "print(f\"Step 1: {len(df)}\")\n",
    "\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39c49bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['Arabic ' 'Indonesian ' 'Malay ' 'Iberian Portuguese ' 'Polish '\n",
      " 'Russian ' 'German ' 'Spanish ' 'Farsi ' 'Brazillian Portuguese '\n",
      " 'Hindi ' 'Swedish ' 'Korean ' 'Cantonese ' 'Japanese ' 'Hungarian ']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accent</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazillian Portuguese</th>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cantonese</th>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Farsi</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hungarian</th>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iberian Portuguese</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malay</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        path\n",
       "accent                      \n",
       "Arabic                   327\n",
       "Brazillian Portuguese   1558\n",
       "Cantonese                677\n",
       "Farsi                    747\n",
       "German                   891\n",
       "Hindi                   1123\n",
       "Hungarian                800\n",
       "Iberian Portuguese       203\n",
       "Indonesian               277\n",
       "Japanese                 496\n",
       "Korean                   479\n",
       "Malay                    156\n",
       "Polish                   443\n",
       "Russian                  667\n",
       "Spanish                  663\n",
       "Swedish                  637"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels: \", df[\"accent\"].unique())\n",
    "print()\n",
    "df.groupby(\"accent\").count()[[\"path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d23ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232f9af3",
   "metadata": {},
   "source": [
    "## grouping 4:\n",
    "brazillian portuguese, iberian portuguese, spanish <br>\n",
    "korean, japanese, mandarin, cantonese <br>\n",
    "polish, czech, russian <br>\n",
    "tamil, malay, indonesian <br>\n",
    "arabic, farsi, hindi <br>\n",
    "swedish, german, Hungarian <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7b52fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazillian portuguese, iberian portuguese, spanish <br> korean, japanese, mandarin, cantonese <br> polish, czech, russian <br> tamil, malay, indonesian <br> arabic, farsi, hindi <br> swedish, german, Hungarian <br>\n"
     ]
    }
   ],
   "source": [
    "groups = input()\n",
    "groups = groups.split('<br>')\n",
    "accent2group = {}\n",
    "for i, group in enumerate(groups):\n",
    "    if len(group):\n",
    "        for accent in group.split(','):\n",
    "            accent2group[accent.strip().lower()] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5b36e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazillian portuguese': 0,\n",
       " 'iberian portuguese': 0,\n",
       " 'spanish': 0,\n",
       " 'korean': 1,\n",
       " 'japanese': 1,\n",
       " 'mandarin': 1,\n",
       " 'cantonese': 1,\n",
       " 'polish': 2,\n",
       " 'czech': 2,\n",
       " 'russian': 2,\n",
       " 'tamil': 3,\n",
       " 'malay': 3,\n",
       " 'indonesian': 3,\n",
       " 'arabic': 4,\n",
       " 'farsi': 4,\n",
       " 'hindi': 4,\n",
       " 'swedish': 5,\n",
       " 'german': 5,\n",
       " 'hungarian': 5}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accent2group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f4852c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAR00523-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIN00145-chunk-02</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAR00576-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FMY00139-chunk-01</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FPP00035-chunk-00</td>\n",
       "      <td>../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                               path  \\\n",
       "0  FAR00523-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00523-...   \n",
       "1  FIN00145-chunk-02  ../../chandra/CSLU/data/mod-agg-3/IN/FIN00145-...   \n",
       "2  FAR00576-chunk-00  ../../chandra/CSLU/data/mod-agg-3/AR/FAR00576-...   \n",
       "3  FMY00139-chunk-01  ../../chandra/CSLU/data/mod-agg-3/MY/FMY00139-...   \n",
       "4  FPP00035-chunk-00  ../../chandra/CSLU/data/mod-agg-3/PP/FPP00035-...   \n",
       "\n",
       "   accent  \n",
       "0       4  \n",
       "1       3  \n",
       "2       4  \n",
       "3       3  \n",
       "4       0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"accent\"] = df[\"accent\"].str.lower().str.strip()\n",
    "df[\"accent\"].replace(accent2group, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2aff1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [4 3 0 2 5 1]\n",
      "\n",
      "        path\n",
      "accent      \n",
      "0       2424\n",
      "1       1652\n",
      "2       1110\n",
      "3        433\n",
      "4       2197\n",
      "5       2328\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels: \", df[\"accent\"].unique())\n",
    "print()\n",
    "print(df.groupby(\"accent\").count()[[\"path\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c58942c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "407ea3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p CSLU_training_data/grouping_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "034d59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8115, 3)\n",
      "(1014, 3)\n",
      "(1015, 3)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"CSLU_training_data/grouping_4\"\n",
    "\n",
    "train_df, rem_df = train_test_split(df, train_size=0.8, random_state=101, stratify=df[\"accent\"])\n",
    "dev_df, test_df = train_test_split(rem_df, test_size=0.5, random_state=101, stratify=rem_df['accent'])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "dev_df = dev_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "dev_df.to_csv(f\"{save_path}/dev.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(dev_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26985e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb74ce72",
   "metadata": {},
   "source": [
    "##\n",
    "1. try on smaller audio based on silence --> ping Ma'am\n",
    "2. try on simpler architecture\n",
    "3.a use wav2vec2 feature architecute and then use those features for classification directly\n",
    "3.b user 3 with RNN or LSTM architecture \n",
    "3.c maybe see TDNN\n",
    "***4. maybe check whisper\n",
    "5. check papers shares on chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba239e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe7570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb80e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b377bcd7",
   "metadata": {},
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c96818ff",
   "metadata": {
    "id": "4tGNY7hRXO44"
   },
   "outputs": [],
   "source": [
    "# need to save the model first\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoConfig, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84b9bdfe",
   "metadata": {
    "id": "IYxg1Tfo2VUw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7b74c9463294d8e2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/mayur/.cache/huggingface/datasets/csv/default-7b74c9463294d8e2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db58deb8ab04190bc88c23bf2b18bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc57e48e4c7a4f0f8ae3bf3bec4abdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/mayur/.cache/huggingface/datasets/csv/default-7b74c9463294d8e2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea42b2025ee484180bcc46ed2d836bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'path', 'accent'],\n",
       "    num_rows: 474\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files={\"test\": \"CSLU_training_data/init/test.csv\"}, delimiter=\"\\t\")[\"test\"]\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef12150c",
   "metadata": {
    "id": "QgZFkMDHW_Um"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32cd1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    h1: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    h2: Optional[Tuple[torch.FloatTensor]] = None\n",
    "        \n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(config.hidden_size, 300)\n",
    "        self.dense2 = nn.Linear(300, 100)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(100, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense1(x)\n",
    "        x1 = torch.tanh(x)\n",
    "        x2 = self.dropout(x1)\n",
    "        x2 = self.dense2(x2)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x3 = self.dropout(x2)        \n",
    "        x3 = self.out_proj(x3)\n",
    "        return x1, x2, x3\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "        for module in self.wav2vec2.encoder.layers[:10]:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "#        pdb.set_trace()\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        #hidden_states = outputs[2][7]  ## taking output of 7th layer\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        h1, h2, logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (h1 + h2 + logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            h1=h1,\n",
    "            h2=h2\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83c74d0a",
   "metadata": {
    "id": "-ESFEXeaWgua"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"CSLU_training_data/checkpoints/checkpoint-6500/\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c43985c",
   "metadata": {
    "id": "BkEd4w8IV7kZ"
   },
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    speech_array = speech_array.squeeze().numpy()\n",
    "    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch\n",
    "\n",
    "def predict(batch):\n",
    "    features = processor(batch[\"speech\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits \n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41fe8d9f",
   "metadata": {
    "id": "S4P6P6XwW85p"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512328dd6db64185adaa49a8843aaf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/474 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayur/exp/anaconda3/envs/w2v2_new/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Pass orig_sr=8000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  after removing the cwd from sys.path.\n",
      "/home/mayur/exp/anaconda3/envs/w2v2_new/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Pass orig_sr=8000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "443d1db2",
   "metadata": {
    "id": "K_oZJzHsXKHv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function predict at 0x7ff17a1d42f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aff001e747a4ad78904e127e80d9d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = test_dataset.map(predict, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16230062",
   "metadata": {
    "id": "BnfJLZvAaxTo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabic ',\n",
       " 'Brazillian Portuguese ',\n",
       " 'Cantonese ',\n",
       " 'Czech ',\n",
       " 'Farsi ',\n",
       " 'French ',\n",
       " 'German ',\n",
       " 'Hindi ',\n",
       " 'Hungarian ',\n",
       " 'Indonesian ',\n",
       " 'Italian ',\n",
       " 'Japanese ',\n",
       " 'Korean ',\n",
       " 'Mandarin ',\n",
       " 'Polish ',\n",
       " 'Russian ',\n",
       " 'Spanish ',\n",
       " 'Swedish ',\n",
       " 'Tamil ',\n",
       " 'Vietnamese ']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [config.id2label[i] for i in range(config.num_labels)]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "640dbf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'path', 'accent', 'speech', 'predicted'],\n",
       "    num_rows: 474\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98a04543",
   "metadata": {
    "id": "vRtajzvTabeH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 12, 0, 1]\n",
      "[8, 7, 9, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "y_true = [config.label2id[name] for name in result[\"accent\"]]\n",
    "y_pred = result[\"predicted\"]\n",
    "\n",
    "print(y_true[:5])\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f5e5f60",
   "metadata": {
    "id": "tUt5rIppXrzl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               Arabic        0.17      0.36      0.23        11\n",
      "Brazillian Portuguese        0.59      0.59      0.59        46\n",
      "            Cantonese        0.67      0.23      0.34        26\n",
      "                Czech        0.14      0.10      0.12        10\n",
      "                Farsi        0.48      0.38      0.43        26\n",
      "               French        0.26      0.28      0.27        29\n",
      "               German        0.69      0.27      0.39        33\n",
      "                Hindi        0.84      0.60      0.70        35\n",
      "            Hungarian        0.34      0.64      0.44        28\n",
      "           Indonesian        0.40      0.22      0.29         9\n",
      "              Italian        0.19      0.27      0.23        22\n",
      "             Japanese        0.37      0.37      0.37        19\n",
      "               Korean        0.24      0.47      0.31        17\n",
      "             Mandarin        0.42      0.57      0.48        28\n",
      "               Polish        0.15      0.14      0.15        14\n",
      "              Russian        0.32      0.42      0.36        24\n",
      "              Spanish        0.43      0.29      0.35        31\n",
      "              Swedish        0.35      0.35      0.35        20\n",
      "                Tamil        0.62      0.39      0.48        33\n",
      "           Vietnamese        0.25      0.23      0.24        13\n",
      "\n",
      "              accuracy                           0.39       474\n",
      "             macro avg       0.40      0.36      0.36       474\n",
      "          weighted avg       0.45      0.39      0.40       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286e6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f390f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e9137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68b99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c3bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f08e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python w2v2_new",
   "language": "python",
   "name": "w2v2_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
