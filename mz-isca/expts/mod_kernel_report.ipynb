{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-4c7d4e88c7a8>:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "from pathlib import Path\n",
    "from cmath import inf\n",
    "import re, os, csv, pathlib, ast, os.path\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "\n",
    "def get_each_run(lne):\n",
    "    # print(lne.strip())\n",
    "    # print(re.findall(\": (.+)\", lne))\n",
    "    # print(re.findall(\": (.+)\", lne)[0])\n",
    "    # print(list(map(float, re.findall(\": (.+)\", lne)[0].split(\" \"))))\n",
    "\n",
    "    return list(map(float, re.findall(\": (.+)\", lne)[0].split(\" \")))\n",
    "\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9] + \"run_{}/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "\n",
    "# def get_test(stats_file_path):\n",
    "#     return stats_file_path[:-9] + \"run_1/output/test_out.txt\"\n",
    "\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    try:\n",
    "        txt_file = open(test_file, \"r\")\n",
    "        lines = txt_file.readlines()\n",
    "        matched = \"\"\n",
    "        for line in lines:\n",
    "            if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "                txt_file.close()\n",
    "                return float(line.rstrip().split(\": \")[1])\n",
    "    except:\n",
    "        txt_file.close()\n",
    "        print(\"weiowdnio\")\n",
    "        return inf\n",
    "\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    return \"-n:\" + str(float(eta[4:]))\n",
    "\n",
    "\n",
    "def accent_distribution(json_path):\n",
    "    file = open(json_path, 'r')\n",
    "    paths = [json.loads(sample)[\"accent\"] for sample in file.readlines()]\n",
    "    counts = Counter(paths)\n",
    "    return dict(counts.most_common())\n",
    "\n",
    "def time_fraction(json_parent, speaker):\n",
    "    total_duration, domain_duration = 0, 0\n",
    "    for i in range(1,4):\n",
    "        json_path = \"{}/run_{}/train.json\".format(json_parent, i)\n",
    "        file = open(json_path, 'r')\n",
    "        samples = [sample for sample in file.readlines()]\n",
    "        total_duration += sum([json.loads(sample)['duration'] for sample in samples])\n",
    "        domain_duration += sum([json.loads(sample)['duration'] for sample in samples if speaker == json.loads(sample)['accent']])\n",
    "#         print(speaker, json.loads(samples[0])['audio_filepath'])\n",
    "    total_duration/=3\n",
    "    domain_duration/=3\n",
    "    return \"{:.1f}/{:.1f}\".format(domain_duration, total_duration)\n",
    "\n",
    "def sample_fraction(json_parent, speaker):\n",
    "    # print(speaker)\n",
    "    total, domain_counts = 0, 0\n",
    "    for i in range(1,4):\n",
    "        json_path = \"{}/run_{}/train.json\".format(json_parent, i)\n",
    "        file = open(json_path, 'r')\n",
    "        lines = [line for line in file.readlines()]\n",
    "        total += len(lines)\n",
    "        domain_counts += len([json.loads(sample) for sample in lines if speaker == json.loads(sample)['accent']])\n",
    "    total/=3\n",
    "    domain_counts/=3\n",
    "    return \"{:.1f}/{:.1f}\".format(domain_counts, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(path):\n",
    "    try:\n",
    "        search_pattern = r\"/target_\\d*/\"\n",
    "        matched_string = re.search(search_pattern, path).group()\n",
    "        new_path = re.sub(search_pattern, os.path.sep, path)\n",
    "        target = matched_string.split('_')[1][:-1]\n",
    "        return new_path, target\n",
    "    except:\n",
    "        return path, \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_two_stage(path, dct):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_within(path, dct):\n",
    "    assert(path.split(os.path.sep)[0] == \"within\")\n",
    "    path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "\n",
    "    dct[\"budget_b1\"] = path.split(os.path.sep)[0].split('_')[-1]\n",
    "    path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "    assert(path.split(os.path.sep)[0] in [\"error_model\", \"random\", \"error_model_orig_transc\", \"true_wer\"])\n",
    "    dct[\"method\"] += path.split(os.path.sep)[0]\n",
    "    # print(path)\n",
    "    path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "\n",
    "\n",
    "    # print(path)\n",
    "    assert(path == \"\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_results(orig_path, dct, speaker):\n",
    "    try:\n",
    "        dct[\"time_fraction\"] = time_fraction(orig_path, speaker)\n",
    "        dct[\"sample_fraction\"] = sample_fraction(orig_path, speaker)\n",
    "        \n",
    "        dct[\"speakers\"] = accent_distribution(os.path.join(orig_path, \"run_1/train.json\"))\n",
    "        \n",
    "        wer_list = []\n",
    "        for run in range(1,4):\n",
    "            test_file_path = os.path.join(orig_path, f\"run_{run}\", \"test_infer_log.txt\")\n",
    "            wer_list.append(WER_test_file(test_file_path))\n",
    "        \n",
    "#         print(wer_list, len(wer_list))\n",
    "            \n",
    "        mean = np.nanmean(wer_list)\n",
    "        var = np.nanvar(wer_list)\n",
    "        dct[\"WER-mean\"] = round(mean, 2)\n",
    "        dct[\"WER-stdev\"] = round(var**0.5, 2)\n",
    "        for run in range(1, 4):\n",
    "            dct[f\"WER-r{run}\"] = wer_list[run - 1]\n",
    "        \n",
    "    except:\n",
    "        print(\"Unable to obtain wers from: \", orig_path, dct[\"method\"], dct[\"budget_b1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting experiments from african\n",
      "End of african\n",
      "Extracting experiments from australia\n",
      "End of australia\n",
      "Extracting experiments from bermuda\n",
      "End of bermuda\n",
      "Extracting experiments from canada\n",
      "End of canada\n",
      "Extracting experiments from england\n",
      "End of england\n",
      "Extracting experiments from hongkong\n",
      "End of hongkong\n",
      "Extracting experiments from indian\n",
      "End of indian\n",
      "Extracting experiments from ireland\n",
      "End of ireland\n",
      "Extracting experiments from malaysia\n",
      "End of malaysia\n",
      "Extracting experiments from philippines\n",
      "End of philippines\n",
      "Extracting experiments from scotland\n",
      "End of scotland\n",
      "Extracting experiments from southatlandtic\n",
      "End of southatlandtic\n",
      "Extracting experiments from us\n",
      "End of us\n",
      "Extracting experiments from wales\n",
      "End of wales\n"
     ]
    }
   ],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "\n",
    "# csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "# df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"african\",\n",
    "    \"australia\",\n",
    "    \"bermuda\",\n",
    "    \"canada\",\n",
    "    \"england\",\n",
    "    \"hongkong\",\n",
    "    \"indian\",\n",
    "    \"ireland\",\n",
    "    \"malaysia\",\n",
    "    \"philippines\",\n",
    "    \"scotland\",\n",
    "    \"southatlandtic\",\n",
    "    \"us\",\n",
    "    \"wales\",\n",
    "]\n",
    "\n",
    "expt_results = []\n",
    "os_sep = os.path.sep\n",
    "submod_fxns = (\"FL1MI\", \"FL2MI\", \"GCMI\", \"LogDMI\")\n",
    "\n",
    "for speaker in speakers:\n",
    "    print(\"Extracting experiments from {}\".format(speaker))\n",
    "    base_dir = os.path.join(\".\", f\"{speaker}\", \"all\")\n",
    "    all_paths = list(set([f\"{os.path.sep}\".join(str(p).split(os.path.sep)[:-2]) for p in Path(base_dir).rglob(\"*/test_infer_log.txt\")]))\n",
    "    for path in all_paths:\n",
    "        dct = {}\n",
    "        orig_path = str(path) + \"/\"\n",
    "        path = (str(path) + \"/\").replace(f\"{speaker}/all/\", \"\")\n",
    "        dct[\"speaker\"] = speaker\n",
    "        \n",
    "        if path.startswith(\"budget_\"):\n",
    "            dct[\"method\"] = \"Two_stage-\"\n",
    "            parse_two_stage(path, dct) \n",
    "            continue\n",
    "        elif path.startswith(\"within\"):\n",
    "            dct[\"method\"] = \"Acc-within-\"\n",
    "            parse_within(path, dct)\n",
    "\n",
    "        \n",
    "\n",
    "        fill_results(orig_path, dct, speaker)\n",
    "        expt_results.append(dct)\n",
    "\n",
    "    print(\"End of {}\".format(speaker))\n",
    "#         else:\n",
    "#             if path.startswith(\"dim_phoneme_gains\"):\n",
    "#                 dct[\"method\"] = \"phone_decay-\"\n",
    "#                 path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "#                 tau_str = path.split(os.path.sep)[0]\n",
    "#                 tau_val = tau_str.split('_')[-1]\n",
    "#                 dct[\"method\"] += tau_val\n",
    "#                 path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "# #             print(path)\n",
    "#             else:\n",
    "#                 dct[\"method\"] = path.split(os.path.sep)[0]\n",
    "#                 path = path.replace(f\"{dct['method']}/\", \"\")\n",
    "#             dct[\"other_accents\"] = replace_with_short_forms(path.split(os.path.sep)[0])\n",
    "#             path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "#         assert(path.startswith(\"budget_\"))\n",
    "#         dct[\"budget_b1\"] = path.split(os.path.sep)[0].replace(\"budget_\", \"\")\n",
    "#         path = path.replace(f\"budget_{dct['budget_b1']}/\", \"\")\n",
    "# #         print(path, dct)\n",
    "        \n",
    "        \n",
    "#         if path.startswith(\"target\"):\n",
    "#             dct[\"target\"] = path.split(os.path.sep)[0].replace(\"target_\", \"\")\n",
    "#             path = path.replace(f\"target_{dct['target']}/\", \"\")\n",
    "#         fxn = path.split(os.path.sep)[0]\n",
    "#         if fxn in submod_fxns:\n",
    "#             dct[\"fxn\"] = fxn\n",
    "# #             print(fxn)\n",
    "#             dct[\"etaScale\"] = \"default(1.0)\"\n",
    "            \n",
    "#         else:\n",
    "#             ls = fxn.split(\"_\")\n",
    "#             assert(len(ls) == 3)\n",
    "#             assert(ls[1] == \"etaScale\")\n",
    "#             assert(ls[0] in submod_fxns)\n",
    "#             dct[\"fxn\"] = ls[0]\n",
    "#             dct[\"etaScale\"] = ls[2]\n",
    "# #             print(ls[2])\n",
    "#         path = path.replace(f\"{fxn}/\", \"\")\n",
    "        \n",
    "#         if path.startswith(\"39/\"):\n",
    "#             dct[\"accent_features\"] = \"39\"\n",
    "#             path = path.replace(f\"{dct['accent_features']}/\", \"\")\n",
    "#         else:\n",
    "#             assert(path.startswith(\"accent_\"))\n",
    "#             dct[\"accent_features\"] = path.split(os.path.sep)[0].replace(\"accent_\", \"\")\n",
    "#             path = path.replace(f\"accent_{dct['accent_features']}/\", \"\")\n",
    "        \n",
    "#         if path.startswith(\"budget_\"):\n",
    "#             # print(path)\n",
    "#             dct[\"budget_b2\"] = path.split(os.path.sep)[0].replace(\"budget_\", \"\")\n",
    "#             path = path.replace(f\"budget_{dct['budget_b2']}/\", \"\")\n",
    "#             # print(path)\n",
    "#             dct[\"method\"] = path.split(os.path.sep)[0] + \"(stage2)\"\n",
    "#             # print(dct[\"method\"])\n",
    "#         else:\n",
    "#             assert(path.startswith(\"content_\"))\n",
    "#             dct[\"content_features\"] = path.split(os.path.sep)[0].replace(\"content_\", \"\")\n",
    "#             path = path.replace(f\"content_{dct['content_features']}/\", \"\")\n",
    "#     #         print(path, dct)\n",
    "#             if path.startswith(\"phoneme_\"):\n",
    "#                 path = path.replace(f\"{path.split(os.path.sep)[0]}/\", \"\")\n",
    "            \n",
    "#             kernels = path.split(os.path.sep)[0].replace(\"kernel_\", \"\")\n",
    "#             kernels_ls = \"; \".join(kernels.split(\"_\"))\n",
    "#             dct[\"kernels\"] = kernels_ls\n",
    "#             path = path.replace(f\"kernel_{kernels}/\", \"\")\n",
    "#             assert(path.startswith(\"accent_\"))\n",
    "#             dct[\"accent_sim\"] = path.split(os.path.sep)[0].replace(\"accent_\", \"\")\n",
    "#             path = path.replace(f\"accent_{dct['accent_sim']}/\", \"\")\n",
    "            \n",
    "#             assert(path.startswith(\"content_\"))\n",
    "#             dct[\"content_sim\"] = path.split(os.path.sep)[0].replace(\"content_\", \"\")\n",
    "#             path = path.replace(f\"content_{dct['content_sim']}/\", \"\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"other_accents\",\n",
    "    \"budget_b1\",\n",
    "    \"target\",\n",
    "    \"fxn\",\n",
    "    \"method\",\n",
    "    \"budget_b2\",\n",
    "    \"etaScale\",\n",
    "    \"accent_features\",\n",
    "    \"content_features\",\n",
    "    \"accent_sim\",\n",
    "    \"content_sim\",\n",
    "    \"kernels\",\n",
    "    \"time_fraction\",\n",
    "    \"sample_fraction\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(expt_results, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"speaker\", \"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>other_accents</th>\n",
       "      <th>budget_b1</th>\n",
       "      <th>target</th>\n",
       "      <th>fxn</th>\n",
       "      <th>method</th>\n",
       "      <th>budget_b2</th>\n",
       "      <th>etaScale</th>\n",
       "      <th>accent_features</th>\n",
       "      <th>content_features</th>\n",
       "      <th>...</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>kernels</th>\n",
       "      <th>time_fraction</th>\n",
       "      <th>sample_fraction</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-stdev</th>\n",
       "      <th>speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>african</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1235.0/1235.0</td>\n",
       "      <td>223.7/223.7</td>\n",
       "      <td>22.06</td>\n",
       "      <td>22.66</td>\n",
       "      <td>22.33</td>\n",
       "      <td>22.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{'african': 222}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>african</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1235.3/1235.3</td>\n",
       "      <td>224.7/224.7</td>\n",
       "      <td>23.34</td>\n",
       "      <td>22.02</td>\n",
       "      <td>22.06</td>\n",
       "      <td>22.47</td>\n",
       "      <td>0.61</td>\n",
       "      <td>{'african': 219}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1223.8/1223.8</td>\n",
       "      <td>247.7/247.7</td>\n",
       "      <td>21.80</td>\n",
       "      <td>22.66</td>\n",
       "      <td>23.11</td>\n",
       "      <td>22.52</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'african': 250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>african</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.0/1232.0</td>\n",
       "      <td>235.0/235.0</td>\n",
       "      <td>22.04</td>\n",
       "      <td>22.19</td>\n",
       "      <td>22.29</td>\n",
       "      <td>22.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>{'african': 235}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.6/1232.6</td>\n",
       "      <td>226.0/226.0</td>\n",
       "      <td>26.06</td>\n",
       "      <td>26.13</td>\n",
       "      <td>25.87</td>\n",
       "      <td>26.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>{'australia': 220}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.5/1232.5</td>\n",
       "      <td>225.0/225.0</td>\n",
       "      <td>25.85</td>\n",
       "      <td>26.10</td>\n",
       "      <td>26.00</td>\n",
       "      <td>25.98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>{'australia': 213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1224.0/1224.0</td>\n",
       "      <td>249.0/249.0</td>\n",
       "      <td>26.08</td>\n",
       "      <td>26.36</td>\n",
       "      <td>25.39</td>\n",
       "      <td>25.94</td>\n",
       "      <td>0.41</td>\n",
       "      <td>{'australia': 246}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1230.1/1230.1</td>\n",
       "      <td>244.0/244.0</td>\n",
       "      <td>25.28</td>\n",
       "      <td>25.14</td>\n",
       "      <td>25.46</td>\n",
       "      <td>25.29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>{'australia': 244}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1231.2/1231.2</td>\n",
       "      <td>215.7/215.7</td>\n",
       "      <td>17.21</td>\n",
       "      <td>16.68</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0.28</td>\n",
       "      <td>{'canada': 217}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1233.5/1233.5</td>\n",
       "      <td>217.0/217.0</td>\n",
       "      <td>16.45</td>\n",
       "      <td>16.75</td>\n",
       "      <td>16.12</td>\n",
       "      <td>16.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>{'canada': 217}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1228.7/1228.7</td>\n",
       "      <td>240.3/240.3</td>\n",
       "      <td>16.52</td>\n",
       "      <td>16.85</td>\n",
       "      <td>16.82</td>\n",
       "      <td>16.73</td>\n",
       "      <td>0.15</td>\n",
       "      <td>{'canada': 242}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.8/1232.8</td>\n",
       "      <td>241.0/241.0</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.20</td>\n",
       "      <td>16.33</td>\n",
       "      <td>16.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>{'canada': 241}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1230.9/1230.9</td>\n",
       "      <td>246.3/246.3</td>\n",
       "      <td>19.44</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.38</td>\n",
       "      <td>19.46</td>\n",
       "      <td>0.08</td>\n",
       "      <td>{'england': 258}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1235.0/1235.0</td>\n",
       "      <td>241.3/241.3</td>\n",
       "      <td>19.44</td>\n",
       "      <td>20.05</td>\n",
       "      <td>19.47</td>\n",
       "      <td>19.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>{'england': 260}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1227.7/1227.7</td>\n",
       "      <td>256.0/256.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>19.65</td>\n",
       "      <td>19.34</td>\n",
       "      <td>19.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>{'england': 250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.9/1232.9</td>\n",
       "      <td>277.0/277.0</td>\n",
       "      <td>19.51</td>\n",
       "      <td>19.14</td>\n",
       "      <td>19.28</td>\n",
       "      <td>19.31</td>\n",
       "      <td>0.15</td>\n",
       "      <td>{'england': 277}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hongkong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1231.5/1231.5</td>\n",
       "      <td>241.3/241.3</td>\n",
       "      <td>29.84</td>\n",
       "      <td>28.59</td>\n",
       "      <td>28.95</td>\n",
       "      <td>29.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>{'hongkong': 238}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hongkong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.0/1234.0</td>\n",
       "      <td>246.3/246.3</td>\n",
       "      <td>28.50</td>\n",
       "      <td>29.01</td>\n",
       "      <td>28.56</td>\n",
       "      <td>28.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>{'hongkong': 248}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hongkong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1227.8/1227.8</td>\n",
       "      <td>270.3/270.3</td>\n",
       "      <td>29.19</td>\n",
       "      <td>29.33</td>\n",
       "      <td>29.30</td>\n",
       "      <td>29.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>{'hongkong': 274}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>indian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.3/1234.3</td>\n",
       "      <td>230.7/230.7</td>\n",
       "      <td>38.12</td>\n",
       "      <td>38.39</td>\n",
       "      <td>37.94</td>\n",
       "      <td>38.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>{'indian': 216}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>indian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.3/1234.3</td>\n",
       "      <td>228.3/228.3</td>\n",
       "      <td>38.80</td>\n",
       "      <td>38.45</td>\n",
       "      <td>38.38</td>\n",
       "      <td>38.54</td>\n",
       "      <td>0.18</td>\n",
       "      <td>{'indian': 218}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>indian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1227.6/1227.6</td>\n",
       "      <td>241.3/241.3</td>\n",
       "      <td>38.04</td>\n",
       "      <td>37.83</td>\n",
       "      <td>37.95</td>\n",
       "      <td>37.94</td>\n",
       "      <td>0.09</td>\n",
       "      <td>{'indian': 243}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>indian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1230.3/1230.3</td>\n",
       "      <td>285.0/285.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>38.23</td>\n",
       "      <td>38.12</td>\n",
       "      <td>38.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>{'indian': 285}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.9/1232.9</td>\n",
       "      <td>232.7/232.7</td>\n",
       "      <td>22.48</td>\n",
       "      <td>20.97</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.55</td>\n",
       "      <td>0.66</td>\n",
       "      <td>{'ireland': 225}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.8/1234.8</td>\n",
       "      <td>226.7/226.7</td>\n",
       "      <td>21.25</td>\n",
       "      <td>21.37</td>\n",
       "      <td>20.84</td>\n",
       "      <td>21.15</td>\n",
       "      <td>0.23</td>\n",
       "      <td>{'ireland': 222}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1229.1/1229.1</td>\n",
       "      <td>260.3/260.3</td>\n",
       "      <td>20.72</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.02</td>\n",
       "      <td>20.98</td>\n",
       "      <td>0.20</td>\n",
       "      <td>{'ireland': 263}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>philippines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1230.8/1230.8</td>\n",
       "      <td>238.3/238.3</td>\n",
       "      <td>29.10</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.12</td>\n",
       "      <td>28.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>{'philippines': 238}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>philippines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.4/1234.4</td>\n",
       "      <td>235.0/235.0</td>\n",
       "      <td>29.16</td>\n",
       "      <td>29.37</td>\n",
       "      <td>29.10</td>\n",
       "      <td>29.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>{'philippines': 239}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>philippines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1225.9/1225.9</td>\n",
       "      <td>239.0/239.0</td>\n",
       "      <td>28.91</td>\n",
       "      <td>29.90</td>\n",
       "      <td>29.48</td>\n",
       "      <td>29.43</td>\n",
       "      <td>0.41</td>\n",
       "      <td>{'philippines': 240}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1233.9/1233.9</td>\n",
       "      <td>207.3/207.3</td>\n",
       "      <td>45.39</td>\n",
       "      <td>46.09</td>\n",
       "      <td>45.68</td>\n",
       "      <td>45.72</td>\n",
       "      <td>0.29</td>\n",
       "      <td>{'scotland': 210}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1233.3/1233.3</td>\n",
       "      <td>208.3/208.3</td>\n",
       "      <td>45.52</td>\n",
       "      <td>44.92</td>\n",
       "      <td>44.98</td>\n",
       "      <td>45.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>{'scotland': 208}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1227.1/1227.1</td>\n",
       "      <td>225.3/225.3</td>\n",
       "      <td>44.94</td>\n",
       "      <td>45.37</td>\n",
       "      <td>45.27</td>\n",
       "      <td>45.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>{'scotland': 232}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1233.9/1233.9</td>\n",
       "      <td>233.7/233.7</td>\n",
       "      <td>17.61</td>\n",
       "      <td>17.99</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>{'us': 223}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-error_model_orig_transc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1234.8/1234.8</td>\n",
       "      <td>229.3/229.3</td>\n",
       "      <td>18.09</td>\n",
       "      <td>18.04</td>\n",
       "      <td>18.03</td>\n",
       "      <td>18.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'us': 218}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-random</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1228.2/1228.2</td>\n",
       "      <td>262.0/262.0</td>\n",
       "      <td>17.77</td>\n",
       "      <td>18.10</td>\n",
       "      <td>17.90</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.14</td>\n",
       "      <td>{'us': 264}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acc-within-true_wer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1232.2/1232.2</td>\n",
       "      <td>293.0/293.0</td>\n",
       "      <td>17.67</td>\n",
       "      <td>17.82</td>\n",
       "      <td>17.71</td>\n",
       "      <td>17.73</td>\n",
       "      <td>0.06</td>\n",
       "      <td>{'us': 293}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker  other_accents budget_b1  target  fxn  \\\n",
       "2       african            NaN       250     NaN  NaN   \n",
       "1       african            NaN       250     NaN  NaN   \n",
       "0       african            NaN       250     NaN  NaN   \n",
       "3       african            NaN       250     NaN  NaN   \n",
       "4     australia            NaN       250     NaN  NaN   \n",
       "5     australia            NaN       250     NaN  NaN   \n",
       "7     australia            NaN       250     NaN  NaN   \n",
       "6     australia            NaN       250     NaN  NaN   \n",
       "9        canada            NaN       250     NaN  NaN   \n",
       "8        canada            NaN       250     NaN  NaN   \n",
       "10       canada            NaN       250     NaN  NaN   \n",
       "11       canada            NaN       250     NaN  NaN   \n",
       "12      england            NaN       250     NaN  NaN   \n",
       "14      england            NaN       250     NaN  NaN   \n",
       "15      england            NaN       250     NaN  NaN   \n",
       "13      england            NaN       250     NaN  NaN   \n",
       "18     hongkong            NaN       250     NaN  NaN   \n",
       "16     hongkong            NaN       250     NaN  NaN   \n",
       "17     hongkong            NaN       250     NaN  NaN   \n",
       "19       indian            NaN       250     NaN  NaN   \n",
       "22       indian            NaN       250     NaN  NaN   \n",
       "20       indian            NaN       250     NaN  NaN   \n",
       "21       indian            NaN       250     NaN  NaN   \n",
       "24      ireland            NaN       250     NaN  NaN   \n",
       "23      ireland            NaN       250     NaN  NaN   \n",
       "25      ireland            NaN       250     NaN  NaN   \n",
       "27  philippines            NaN       250     NaN  NaN   \n",
       "28  philippines            NaN       250     NaN  NaN   \n",
       "26  philippines            NaN       250     NaN  NaN   \n",
       "29     scotland            NaN       250     NaN  NaN   \n",
       "31     scotland            NaN       250     NaN  NaN   \n",
       "30     scotland            NaN       250     NaN  NaN   \n",
       "35           us            NaN       250     NaN  NaN   \n",
       "33           us            NaN       250     NaN  NaN   \n",
       "34           us            NaN       250     NaN  NaN   \n",
       "32           us            NaN       250     NaN  NaN   \n",
       "\n",
       "                                method  budget_b2  etaScale  accent_features  \\\n",
       "2               Acc-within-error_model        NaN       NaN              NaN   \n",
       "1   Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "0                    Acc-within-random        NaN       NaN              NaN   \n",
       "3                  Acc-within-true_wer        NaN       NaN              NaN   \n",
       "4               Acc-within-error_model        NaN       NaN              NaN   \n",
       "5   Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "7                    Acc-within-random        NaN       NaN              NaN   \n",
       "6                  Acc-within-true_wer        NaN       NaN              NaN   \n",
       "9               Acc-within-error_model        NaN       NaN              NaN   \n",
       "8   Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "10                   Acc-within-random        NaN       NaN              NaN   \n",
       "11                 Acc-within-true_wer        NaN       NaN              NaN   \n",
       "12              Acc-within-error_model        NaN       NaN              NaN   \n",
       "14  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "15                   Acc-within-random        NaN       NaN              NaN   \n",
       "13                 Acc-within-true_wer        NaN       NaN              NaN   \n",
       "18              Acc-within-error_model        NaN       NaN              NaN   \n",
       "16  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "17                   Acc-within-random        NaN       NaN              NaN   \n",
       "19              Acc-within-error_model        NaN       NaN              NaN   \n",
       "22  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "20                   Acc-within-random        NaN       NaN              NaN   \n",
       "21                 Acc-within-true_wer        NaN       NaN              NaN   \n",
       "24              Acc-within-error_model        NaN       NaN              NaN   \n",
       "23  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "25                   Acc-within-random        NaN       NaN              NaN   \n",
       "27              Acc-within-error_model        NaN       NaN              NaN   \n",
       "28  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "26                   Acc-within-random        NaN       NaN              NaN   \n",
       "29              Acc-within-error_model        NaN       NaN              NaN   \n",
       "31  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "30                   Acc-within-random        NaN       NaN              NaN   \n",
       "35              Acc-within-error_model        NaN       NaN              NaN   \n",
       "33  Acc-within-error_model_orig_transc        NaN       NaN              NaN   \n",
       "34                   Acc-within-random        NaN       NaN              NaN   \n",
       "32                 Acc-within-true_wer        NaN       NaN              NaN   \n",
       "\n",
       "    content_features  ...  content_sim  kernels  time_fraction  \\\n",
       "2                NaN  ...          NaN      NaN  1235.0/1235.0   \n",
       "1                NaN  ...          NaN      NaN  1235.3/1235.3   \n",
       "0                NaN  ...          NaN      NaN  1223.8/1223.8   \n",
       "3                NaN  ...          NaN      NaN  1232.0/1232.0   \n",
       "4                NaN  ...          NaN      NaN  1232.6/1232.6   \n",
       "5                NaN  ...          NaN      NaN  1232.5/1232.5   \n",
       "7                NaN  ...          NaN      NaN  1224.0/1224.0   \n",
       "6                NaN  ...          NaN      NaN  1230.1/1230.1   \n",
       "9                NaN  ...          NaN      NaN  1231.2/1231.2   \n",
       "8                NaN  ...          NaN      NaN  1233.5/1233.5   \n",
       "10               NaN  ...          NaN      NaN  1228.7/1228.7   \n",
       "11               NaN  ...          NaN      NaN  1232.8/1232.8   \n",
       "12               NaN  ...          NaN      NaN  1230.9/1230.9   \n",
       "14               NaN  ...          NaN      NaN  1235.0/1235.0   \n",
       "15               NaN  ...          NaN      NaN  1227.7/1227.7   \n",
       "13               NaN  ...          NaN      NaN  1232.9/1232.9   \n",
       "18               NaN  ...          NaN      NaN  1231.5/1231.5   \n",
       "16               NaN  ...          NaN      NaN  1234.0/1234.0   \n",
       "17               NaN  ...          NaN      NaN  1227.8/1227.8   \n",
       "19               NaN  ...          NaN      NaN  1234.3/1234.3   \n",
       "22               NaN  ...          NaN      NaN  1234.3/1234.3   \n",
       "20               NaN  ...          NaN      NaN  1227.6/1227.6   \n",
       "21               NaN  ...          NaN      NaN  1230.3/1230.3   \n",
       "24               NaN  ...          NaN      NaN  1232.9/1232.9   \n",
       "23               NaN  ...          NaN      NaN  1234.8/1234.8   \n",
       "25               NaN  ...          NaN      NaN  1229.1/1229.1   \n",
       "27               NaN  ...          NaN      NaN  1230.8/1230.8   \n",
       "28               NaN  ...          NaN      NaN  1234.4/1234.4   \n",
       "26               NaN  ...          NaN      NaN  1225.9/1225.9   \n",
       "29               NaN  ...          NaN      NaN  1233.9/1233.9   \n",
       "31               NaN  ...          NaN      NaN  1233.3/1233.3   \n",
       "30               NaN  ...          NaN      NaN  1227.1/1227.1   \n",
       "35               NaN  ...          NaN      NaN  1233.9/1233.9   \n",
       "33               NaN  ...          NaN      NaN  1234.8/1234.8   \n",
       "34               NaN  ...          NaN      NaN  1228.2/1228.2   \n",
       "32               NaN  ...          NaN      NaN  1232.2/1232.2   \n",
       "\n",
       "   sample_fraction WER-r1  WER-r2  WER-r3  WER-mean  WER-stdev  \\\n",
       "2      223.7/223.7  22.06   22.66   22.33     22.35       0.25   \n",
       "1      224.7/224.7  23.34   22.02   22.06     22.47       0.61   \n",
       "0      247.7/247.7  21.80   22.66   23.11     22.52       0.54   \n",
       "3      235.0/235.0  22.04   22.19   22.29     22.17       0.10   \n",
       "4      226.0/226.0  26.06   26.13   25.87     26.02       0.11   \n",
       "5      225.0/225.0  25.85   26.10   26.00     25.98       0.10   \n",
       "7      249.0/249.0  26.08   26.36   25.39     25.94       0.41   \n",
       "6      244.0/244.0  25.28   25.14   25.46     25.29       0.13   \n",
       "9      215.7/215.7  17.21   16.68   16.57     16.82       0.28   \n",
       "8      217.0/217.0  16.45   16.75   16.12     16.44       0.26   \n",
       "10     240.3/240.3  16.52   16.85   16.82     16.73       0.15   \n",
       "11     241.0/241.0  16.19   16.20   16.33     16.24       0.06   \n",
       "12     246.3/246.3  19.44   19.57   19.38     19.46       0.08   \n",
       "14     241.3/241.3  19.44   20.05   19.47     19.65       0.28   \n",
       "15     256.0/256.0  19.58   19.65   19.34     19.52       0.13   \n",
       "13     277.0/277.0  19.51   19.14   19.28     19.31       0.15   \n",
       "18     241.3/241.3  29.84   28.59   28.95     29.13       0.53   \n",
       "16     246.3/246.3  28.50   29.01   28.56     28.69       0.23   \n",
       "17     270.3/270.3  29.19   29.33   29.30     29.27       0.06   \n",
       "19     230.7/230.7  38.12   38.39   37.94     38.15       0.18   \n",
       "22     228.3/228.3  38.80   38.45   38.38     38.54       0.18   \n",
       "20     241.3/241.3  38.04   37.83   37.95     37.94       0.09   \n",
       "21     285.0/285.0  38.49   38.23   38.12     38.28       0.16   \n",
       "24     232.7/232.7  22.48   20.97   21.20     21.55       0.66   \n",
       "23     226.7/226.7  21.25   21.37   20.84     21.15       0.23   \n",
       "25     260.3/260.3  20.72   21.20   21.02     20.98       0.20   \n",
       "27     238.3/238.3  29.10   28.75   28.12     28.66       0.41   \n",
       "28     235.0/235.0  29.16   29.37   29.10     29.21       0.12   \n",
       "26     239.0/239.0  28.91   29.90   29.48     29.43       0.41   \n",
       "29     207.3/207.3  45.39   46.09   45.68     45.72       0.29   \n",
       "31     208.3/208.3  45.52   44.92   44.98     45.14       0.27   \n",
       "30     225.3/225.3  44.94   45.37   45.27     45.19       0.18   \n",
       "35     233.7/233.7  17.61   17.99   17.88     17.83       0.16   \n",
       "33     229.3/229.3  18.09   18.04   18.03     18.05       0.03   \n",
       "34     262.0/262.0  17.77   18.10   17.90     17.92       0.14   \n",
       "32     293.0/293.0  17.67   17.82   17.71     17.73       0.06   \n",
       "\n",
       "                speakers  \n",
       "2       {'african': 222}  \n",
       "1       {'african': 219}  \n",
       "0       {'african': 250}  \n",
       "3       {'african': 235}  \n",
       "4     {'australia': 220}  \n",
       "5     {'australia': 213}  \n",
       "7     {'australia': 246}  \n",
       "6     {'australia': 244}  \n",
       "9        {'canada': 217}  \n",
       "8        {'canada': 217}  \n",
       "10       {'canada': 242}  \n",
       "11       {'canada': 241}  \n",
       "12      {'england': 258}  \n",
       "14      {'england': 260}  \n",
       "15      {'england': 250}  \n",
       "13      {'england': 277}  \n",
       "18     {'hongkong': 238}  \n",
       "16     {'hongkong': 248}  \n",
       "17     {'hongkong': 274}  \n",
       "19       {'indian': 216}  \n",
       "22       {'indian': 218}  \n",
       "20       {'indian': 243}  \n",
       "21       {'indian': 285}  \n",
       "24      {'ireland': 225}  \n",
       "23      {'ireland': 222}  \n",
       "25      {'ireland': 263}  \n",
       "27  {'philippines': 238}  \n",
       "28  {'philippines': 239}  \n",
       "26  {'philippines': 240}  \n",
       "29     {'scotland': 210}  \n",
       "31     {'scotland': 208}  \n",
       "30     {'scotland': 232}  \n",
       "35           {'us': 223}  \n",
       "33           {'us': 218}  \n",
       "34           {'us': 264}  \n",
       "32           {'us': 293}  \n",
       "\n",
       "[36 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values([\"speaker\", \"fxn\", \"method\"])\n",
    "sorted_df.to_csv(\"mcv_250_within.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"method\"].str.startswith(\"phone_decay\"))\n",
    "# mask = (((df[\"method\"].str.endswith(\"random(stage2)\")) | (df[\"method\"].str.startswith(\"uniform\"))) & (df[\"budget_b1\"] == \"3500\") & (df[\"budget_b2\"] == \"150\")) \n",
    "filtered_df = df.loc[mask]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>other_accents</th>\n",
       "      <th>budget_b1</th>\n",
       "      <th>target</th>\n",
       "      <th>fxn</th>\n",
       "      <th>method</th>\n",
       "      <th>budget_b2</th>\n",
       "      <th>etaScale</th>\n",
       "      <th>accent_features</th>\n",
       "      <th>content_features</th>\n",
       "      <th>...</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>kernels</th>\n",
       "      <th>time_fraction</th>\n",
       "      <th>sample_fraction</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-stdev</th>\n",
       "      <th>speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>679.5/734.2</td>\n",
       "      <td>111.0/116.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>20.33</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.09</td>\n",
       "      <td>0.19</td>\n",
       "      <td>{'ass': 111, 'tam': 4, 'guj': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>465.3/733.3</td>\n",
       "      <td>76.0/103.0</td>\n",
       "      <td>20.52</td>\n",
       "      <td>20.61</td>\n",
       "      <td>20.12</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.21</td>\n",
       "      <td>{'ass': 76, 'tam': 20, 'guj': 5, 'raj': 1, 'ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>199.3/737.5</td>\n",
       "      <td>31.0/87.0</td>\n",
       "      <td>22.86</td>\n",
       "      <td>22.74</td>\n",
       "      <td>22.81</td>\n",
       "      <td>22.80</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'tam': 33, 'ass': 31, 'guj': 14, 'kan': 3, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>553.9/733.9</td>\n",
       "      <td>88.0/106.0</td>\n",
       "      <td>19.66</td>\n",
       "      <td>19.83</td>\n",
       "      <td>20.28</td>\n",
       "      <td>19.92</td>\n",
       "      <td>0.26</td>\n",
       "      <td>{'ass': 88, 'tam': 14, 'guj': 3, 'kan': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>610.7/732.0</td>\n",
       "      <td>100.0/112.0</td>\n",
       "      <td>19.51</td>\n",
       "      <td>19.55</td>\n",
       "      <td>19.42</td>\n",
       "      <td>19.49</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'ass': 100, 'tam': 11, 'raj': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>640.0/734.3</td>\n",
       "      <td>103.0/113.0</td>\n",
       "      <td>20.18</td>\n",
       "      <td>19.66</td>\n",
       "      <td>19.56</td>\n",
       "      <td>19.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>{'ass': 103, 'tam': 9, 'kan': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>693.2/734.3</td>\n",
       "      <td>114.0/118.0</td>\n",
       "      <td>19.86</td>\n",
       "      <td>20.24</td>\n",
       "      <td>19.84</td>\n",
       "      <td>19.98</td>\n",
       "      <td>0.18</td>\n",
       "      <td>{'ass': 114, 'tam': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>assamese[F]</td>\n",
       "      <td>assamese[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>478.2/733.6</td>\n",
       "      <td>79.0/105.0</td>\n",
       "      <td>21.01</td>\n",
       "      <td>21.92</td>\n",
       "      <td>21.14</td>\n",
       "      <td>21.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>{'ass': 79, 'tam': 18, 'guj': 6, 'kan': 1, 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>gujarati[F]</td>\n",
       "      <td>gujarati[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>562.7/728.5</td>\n",
       "      <td>54.0/71.0</td>\n",
       "      <td>10.46</td>\n",
       "      <td>10.50</td>\n",
       "      <td>10.51</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'guj': 54, 'tam': 12, 'kan': 2, 'ass': 2, 'ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>gujarati[F]</td>\n",
       "      <td>gujarati[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>706.7/736.4</td>\n",
       "      <td>80.0/83.0</td>\n",
       "      <td>9.89</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.93</td>\n",
       "      <td>9.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>{'guj': 80, 'tam': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>gujarati[F]</td>\n",
       "      <td>gujarati[F]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>706.3/727.2</td>\n",
       "      <td>76.0/78.0</td>\n",
       "      <td>9.72</td>\n",
       "      <td>9.68</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'guj': 76, 'tam': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>586.3/735.3</td>\n",
       "      <td>112.0/126.0</td>\n",
       "      <td>9.16</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.05</td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'hin': 112, 'tam': 6, 'raj': 3, 'kan': 3, 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>685.4/736.8</td>\n",
       "      <td>134.0/139.0</td>\n",
       "      <td>8.58</td>\n",
       "      <td>8.66</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'hin': 134, 'tam': 2, 'kan': 1, 'mal': 1, 'ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>616.8/737.1</td>\n",
       "      <td>115.0/127.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.74</td>\n",
       "      <td>8.66</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{'hin': 115, 'tam': 4, 'mal': 3, 'raj': 3, 'ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>508.3/733.8</td>\n",
       "      <td>95.0/117.0</td>\n",
       "      <td>9.13</td>\n",
       "      <td>9.14</td>\n",
       "      <td>9.14</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>{'hin': 95, 'tam': 8, 'raj': 5, 'mal': 4, 'kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>343.2/737.4</td>\n",
       "      <td>49.0/90.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>9.16</td>\n",
       "      <td>9.20</td>\n",
       "      <td>9.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'hin': 49, 'tam': 15, 'mal': 10, 'raj': 7, 'k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>493.5/735.0</td>\n",
       "      <td>80.0/106.0</td>\n",
       "      <td>8.46</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.45</td>\n",
       "      <td>8.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'hin': 80, 'raj': 15, 'tam': 6, 'kan': 3, 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>306.7/736.5</td>\n",
       "      <td>44.0/89.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>9.24</td>\n",
       "      <td>9.26</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'hin': 44, 'raj': 23, 'tam': 12, 'kan': 6, 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hindi[M]</td>\n",
       "      <td>hindi[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>683.7/736.5</td>\n",
       "      <td>119.0/125.0</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.38</td>\n",
       "      <td>8.32</td>\n",
       "      <td>8.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'hin': 119, 'raj': 3, 'mal': 2, 'kan': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>kannada[M]</td>\n",
       "      <td>kannada[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>491.0/734.3</td>\n",
       "      <td>74.0/97.0</td>\n",
       "      <td>13.99</td>\n",
       "      <td>14.06</td>\n",
       "      <td>14.19</td>\n",
       "      <td>14.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>{'kan': 74, 'tam': 14, 'guj': 3, 'mal': 3, 'ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>kannada[M]</td>\n",
       "      <td>kannada[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>695.4/735.6</td>\n",
       "      <td>113.0/117.0</td>\n",
       "      <td>13.24</td>\n",
       "      <td>13.41</td>\n",
       "      <td>13.34</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.07</td>\n",
       "      <td>{'kan': 113, 'tam': 3, 'raj': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>kannada[M]</td>\n",
       "      <td>kannada[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>713.2/734.2</td>\n",
       "      <td>115.0/117.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>13.91</td>\n",
       "      <td>13.73</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.10</td>\n",
       "      <td>{'kan': 115, 'tam': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>malayalam[M]</td>\n",
       "      <td>malayalam[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>438.6/733.5</td>\n",
       "      <td>61.0/93.0</td>\n",
       "      <td>15.65</td>\n",
       "      <td>15.59</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>{'mal': 61, 'kan': 15, 'tam': 10, 'guj': 3, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>malayalam[M]</td>\n",
       "      <td>malayalam[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>684.6/737.3</td>\n",
       "      <td>105.0/111.0</td>\n",
       "      <td>14.55</td>\n",
       "      <td>14.59</td>\n",
       "      <td>14.65</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{'mal': 105, 'tam': 3, 'kan': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>malayalam[M]</td>\n",
       "      <td>malayalam[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>674.3/732.3</td>\n",
       "      <td>96.0/102.0</td>\n",
       "      <td>15.03</td>\n",
       "      <td>15.15</td>\n",
       "      <td>14.93</td>\n",
       "      <td>15.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>{'mal': 96, 'kan': 3, 'tam': 2, 'raj': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>rajasthani[M]</td>\n",
       "      <td>rajasthani[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>502.4/732.2</td>\n",
       "      <td>71.0/96.0</td>\n",
       "      <td>16.26</td>\n",
       "      <td>16.50</td>\n",
       "      <td>16.58</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>{'raj': 71, 'tam': 11, 'hin': 10, 'kan': 2, 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>rajasthani[M]</td>\n",
       "      <td>rajasthani[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>567.6/737.4</td>\n",
       "      <td>74.0/92.0</td>\n",
       "      <td>15.53</td>\n",
       "      <td>15.51</td>\n",
       "      <td>15.28</td>\n",
       "      <td>15.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>{'raj': 74, 'hin': 11, 'tam': 4, 'mal': 2, 'ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tamil[M]</td>\n",
       "      <td>tamil[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>FL2MI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>654.0/734.4</td>\n",
       "      <td>92.0/100.0</td>\n",
       "      <td>11.78</td>\n",
       "      <td>11.81</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'tam': 92, 'raj': 2, 'kan': 2, 'mal': 2, 'hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>tamil[M]</td>\n",
       "      <td>tamil[M]=1</td>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>GCMI</td>\n",
       "      <td>phone_decay-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>tf_idf_2gram</td>\n",
       "      <td>...</td>\n",
       "      <td>cosine</td>\n",
       "      <td>g=accent; gq=accent; qq=accent</td>\n",
       "      <td>722.3/736.9</td>\n",
       "      <td>102.0/104.0</td>\n",
       "      <td>11.58</td>\n",
       "      <td>11.52</td>\n",
       "      <td>11.64</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'tam': 102, 'kan': 1, 'hin': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker    other_accents budget_b1 target    fxn           method  \\\n",
       "5      assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-1.0   \n",
       "8      assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "16     assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-0.1   \n",
       "31     assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-0.3   \n",
       "38     assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-0.4   \n",
       "45     assamese[F]    assamese[F]=1       150     20  FL2MI  phone_decay-0.5   \n",
       "22     assamese[F]    assamese[F]=1       150     20   GCMI  phone_decay-1.0   \n",
       "24     assamese[F]    assamese[F]=1       150     20   GCMI  phone_decay-0.2   \n",
       "289    gujarati[F]    gujarati[F]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "318    gujarati[F]    gujarati[F]=1       150     20  FL2MI  phone_decay-1.0   \n",
       "284    gujarati[F]    gujarati[F]=1       150     20   GCMI  phone_decay-1.0   \n",
       "181       hindi[M]       hindi[M]=1       150     20  FL2MI  phone_decay-0.3   \n",
       "185       hindi[M]       hindi[M]=1       150     20  FL2MI  phone_decay-1.0   \n",
       "200       hindi[M]       hindi[M]=1       150     20  FL2MI  phone_decay-0.4   \n",
       "204       hindi[M]       hindi[M]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "206       hindi[M]       hindi[M]=1       150     20  FL2MI  phone_decay-0.1   \n",
       "172       hindi[M]       hindi[M]=1       150     20   GCMI  phone_decay-0.2   \n",
       "176       hindi[M]       hindi[M]=1       150     20   GCMI  phone_decay-0.1   \n",
       "186       hindi[M]       hindi[M]=1       150     20   GCMI  phone_decay-1.0   \n",
       "111     kannada[M]     kannada[M]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "124     kannada[M]     kannada[M]=1       150     20  FL2MI  phone_decay-1.0   \n",
       "119     kannada[M]     kannada[M]=1       150     20   GCMI  phone_decay-1.0   \n",
       "233   malayalam[M]   malayalam[M]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "242   malayalam[M]   malayalam[M]=1       150     20  FL2MI  phone_decay-1.0   \n",
       "228   malayalam[M]   malayalam[M]=1       150     20   GCMI  phone_decay-1.0   \n",
       "150  rajasthani[M]  rajasthani[M]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "149  rajasthani[M]  rajasthani[M]=1       150     20   GCMI  phone_decay-0.2   \n",
       "269       tamil[M]       tamil[M]=1       150     20  FL2MI  phone_decay-0.2   \n",
       "249       tamil[M]       tamil[M]=1       150     20   GCMI  phone_decay-0.2   \n",
       "\n",
       "    budget_b2 etaScale accent_features content_features  ... content_sim  \\\n",
       "5         NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "8         NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "16        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "31        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "38        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "45        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "22        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "24        NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "289       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "318       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "284       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "181       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "185       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "200       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "204       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "206       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "172       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "176       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "186       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "111       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "124       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "119       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "233       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "242       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "228       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "150       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "149       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "269       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "249       NaN      1.0              39     tf_idf_2gram  ...      cosine   \n",
       "\n",
       "                            kernels time_fraction sample_fraction WER-r1  \\\n",
       "5    g=accent; gq=accent; qq=accent   679.5/734.2     111.0/116.0  20.05   \n",
       "8    g=accent; gq=accent; qq=accent   465.3/733.3      76.0/103.0  20.52   \n",
       "16   g=accent; gq=accent; qq=accent   199.3/737.5       31.0/87.0  22.86   \n",
       "31   g=accent; gq=accent; qq=accent   553.9/733.9      88.0/106.0  19.66   \n",
       "38   g=accent; gq=accent; qq=accent   610.7/732.0     100.0/112.0  19.51   \n",
       "45   g=accent; gq=accent; qq=accent   640.0/734.3     103.0/113.0  20.18   \n",
       "22   g=accent; gq=accent; qq=accent   693.2/734.3     114.0/118.0  19.86   \n",
       "24   g=accent; gq=accent; qq=accent   478.2/733.6      79.0/105.0  21.01   \n",
       "289  g=accent; gq=accent; qq=accent   562.7/728.5       54.0/71.0  10.46   \n",
       "318  g=accent; gq=accent; qq=accent   706.7/736.4       80.0/83.0   9.89   \n",
       "284  g=accent; gq=accent; qq=accent   706.3/727.2       76.0/78.0   9.72   \n",
       "181  g=accent; gq=accent; qq=accent   586.3/735.3     112.0/126.0   9.16   \n",
       "185  g=accent; gq=accent; qq=accent   685.4/736.8     134.0/139.0   8.58   \n",
       "200  g=accent; gq=accent; qq=accent   616.8/737.1     115.0/127.0   8.67   \n",
       "204  g=accent; gq=accent; qq=accent   508.3/733.8      95.0/117.0   9.13   \n",
       "206  g=accent; gq=accent; qq=accent   343.2/737.4       49.0/90.0   9.20   \n",
       "172  g=accent; gq=accent; qq=accent   493.5/735.0      80.0/106.0   8.46   \n",
       "176  g=accent; gq=accent; qq=accent   306.7/736.5       44.0/89.0   9.26   \n",
       "186  g=accent; gq=accent; qq=accent   683.7/736.5     119.0/125.0   8.36   \n",
       "111  g=accent; gq=accent; qq=accent   491.0/734.3       74.0/97.0  13.99   \n",
       "124  g=accent; gq=accent; qq=accent   695.4/735.6     113.0/117.0  13.24   \n",
       "119  g=accent; gq=accent; qq=accent   713.2/734.2     115.0/117.0  13.67   \n",
       "233  g=accent; gq=accent; qq=accent   438.6/733.5       61.0/93.0  15.65   \n",
       "242  g=accent; gq=accent; qq=accent   684.6/737.3     105.0/111.0  14.55   \n",
       "228  g=accent; gq=accent; qq=accent   674.3/732.3      96.0/102.0  15.03   \n",
       "150  g=accent; gq=accent; qq=accent   502.4/732.2       71.0/96.0  16.26   \n",
       "149  g=accent; gq=accent; qq=accent   567.6/737.4       74.0/92.0  15.53   \n",
       "269  g=accent; gq=accent; qq=accent   654.0/734.4      92.0/100.0  11.78   \n",
       "249  g=accent; gq=accent; qq=accent   722.3/736.9     102.0/104.0  11.58   \n",
       "\n",
       "     WER-r2  WER-r3  WER-mean  WER-stdev  \\\n",
       "5     20.33   19.88     20.09       0.19   \n",
       "8     20.61   20.12     20.42       0.21   \n",
       "16    22.74   22.81     22.80       0.05   \n",
       "31    19.83   20.28     19.92       0.26   \n",
       "38    19.55   19.42     19.49       0.05   \n",
       "45    19.66   19.56     19.80       0.27   \n",
       "22    20.24   19.84     19.98       0.18   \n",
       "24    21.92   21.14     21.36       0.40   \n",
       "289   10.50   10.51     10.49       0.02   \n",
       "318   10.08    9.93      9.97       0.08   \n",
       "284    9.68    9.66      9.69       0.02   \n",
       "181    9.07    9.05      9.09       0.05   \n",
       "185    8.66    8.60      8.61       0.03   \n",
       "200    8.74    8.66      8.69       0.04   \n",
       "204    9.14    9.14      9.14       0.00   \n",
       "206    9.16    9.20      9.19       0.02   \n",
       "172    8.50    8.45      8.47       0.02   \n",
       "176    9.24    9.26      9.25       0.01   \n",
       "186    8.38    8.32      8.35       0.02   \n",
       "111   14.06   14.19     14.08       0.08   \n",
       "124   13.41   13.34     13.33       0.07   \n",
       "119   13.91   13.73     13.77       0.10   \n",
       "233   15.59   15.43     15.56       0.09   \n",
       "242   14.59   14.65     14.60       0.04   \n",
       "228   15.15   14.93     15.04       0.09   \n",
       "150   16.50   16.58     16.45       0.14   \n",
       "149   15.51   15.28     15.44       0.11   \n",
       "269   11.81   11.76     11.78       0.02   \n",
       "249   11.52   11.64     11.58       0.05   \n",
       "\n",
       "                                              speakers  \n",
       "5                     {'ass': 111, 'tam': 4, 'guj': 1}  \n",
       "8    {'ass': 76, 'tam': 20, 'guj': 5, 'raj': 1, 'ka...  \n",
       "16   {'tam': 33, 'ass': 31, 'guj': 14, 'kan': 3, 'r...  \n",
       "31          {'ass': 88, 'tam': 14, 'guj': 3, 'kan': 1}  \n",
       "38                   {'ass': 100, 'tam': 11, 'raj': 1}  \n",
       "45                    {'ass': 103, 'tam': 9, 'kan': 1}  \n",
       "22                              {'ass': 114, 'tam': 4}  \n",
       "24   {'ass': 79, 'tam': 18, 'guj': 6, 'kan': 1, 'ma...  \n",
       "289  {'guj': 54, 'tam': 12, 'kan': 2, 'ass': 2, 'ra...  \n",
       "318                              {'guj': 80, 'tam': 3}  \n",
       "284                              {'guj': 76, 'tam': 2}  \n",
       "181  {'hin': 112, 'tam': 6, 'raj': 3, 'kan': 3, 'ma...  \n",
       "185  {'hin': 134, 'tam': 2, 'kan': 1, 'mal': 1, 'ra...  \n",
       "200  {'hin': 115, 'tam': 4, 'mal': 3, 'raj': 3, 'ka...  \n",
       "204  {'hin': 95, 'tam': 8, 'raj': 5, 'mal': 4, 'kan...  \n",
       "206  {'hin': 49, 'tam': 15, 'mal': 10, 'raj': 7, 'k...  \n",
       "172  {'hin': 80, 'raj': 15, 'tam': 6, 'kan': 3, 'ma...  \n",
       "176  {'hin': 44, 'raj': 23, 'tam': 12, 'kan': 6, 'm...  \n",
       "186         {'hin': 119, 'raj': 3, 'mal': 2, 'kan': 1}  \n",
       "111  {'kan': 74, 'tam': 14, 'guj': 3, 'mal': 3, 'ra...  \n",
       "124                   {'kan': 113, 'tam': 3, 'raj': 1}  \n",
       "119                             {'kan': 115, 'tam': 2}  \n",
       "233  {'mal': 61, 'kan': 15, 'tam': 10, 'guj': 3, 'r...  \n",
       "242                   {'mal': 105, 'tam': 3, 'kan': 3}  \n",
       "228          {'mal': 96, 'kan': 3, 'tam': 2, 'raj': 1}  \n",
       "150  {'raj': 71, 'tam': 11, 'hin': 10, 'kan': 2, 'm...  \n",
       "149  {'raj': 74, 'hin': 11, 'tam': 4, 'mal': 2, 'ka...  \n",
       "269  {'tam': 92, 'raj': 2, 'kan': 2, 'mal': 2, 'hin...  \n",
       "249                   {'tam': 102, 'kan': 1, 'hin': 1}  \n",
       "\n",
       "[29 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filtered_df.sort_values([\"other_accents\", \"fxn\", \"speaker\"])\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"report-21st-oct2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = list(range(1, len(df[\"etaScale\"]) + 1))\n",
    "plt.errorbar(x, df[\"WER-mean\"], df[\"WER-stdev\"], marker=\"o\", ecolor=\"red\")\n",
    "plt.xticks(x, df[\"etaScale\"])\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = list(range(1, len(df[\"etaScale\"]) + 1))\n",
    "plt.plot(x, df[\"WER-mean\"])\n",
    "plt.xticks(x, df[\"etaScale\"])\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# speakered selection: 76 76 76 -> 76.00\n",
    "# speakered duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "# Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"base_eta\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    # \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    # \"rajasthani_male_english\",\n",
    "    # \"hindi_male_english\",\n",
    "    # \"malayalam_male_english\",\n",
    "    # \"tamil_male_english\",\n",
    "    # \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        (func, base_eta, etaScale) = split_function(function)\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for accent_similarity in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for content_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}\"\n",
    "                    ):\n",
    "                        stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                        if not (os.path.isfile(stats_file_path)):\n",
    "                            continue\n",
    "                        stats_file = open(stats_file_path, \"r\")\n",
    "                        lines = stats_file.readlines()\n",
    "                        # print(\"lines length \", len(lines), lines)\n",
    "                        (\n",
    "                            total_selections,\n",
    "                            total_durations,\n",
    "                            speakered_selections,\n",
    "                            speakered_durations,\n",
    "                        ) = map(get_each_run, lines[:4])\n",
    "                        # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                        sample_frac = mean(\n",
    "                            [\n",
    "                                x[0] / x[1]\n",
    "                                for x in zip(speakered_selections, total_selections)\n",
    "                            ]\n",
    "                        )\n",
    "                        sample_total = mean(total_selections)\n",
    "                        duration_frac = mean(\n",
    "                            [\n",
    "                                x[0] / x[1]\n",
    "                                for x in zip(speakered_durations, total_durations)\n",
    "                            ]\n",
    "                        )\n",
    "                        duration_total = mean(total_durations)\n",
    "                        df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                            duration_total * duration_frac, duration_total\n",
    "                        )\n",
    "                        df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                            sample_total * sample_frac, sample_total\n",
    "                        )\n",
    "                        df_selections = get_selection_counts(lines[4])\n",
    "                        try:\n",
    "                            wers = [\n",
    "                                WER_test_file(\n",
    "                                    get_test_file_from_stats_path(i, stats_file)\n",
    "                                )\n",
    "                                for i in range(1, 4)\n",
    "                            ]\n",
    "                            df_wer_mean = round(mean(wers), 2)\n",
    "                            df_wer_stdev = round(variance(wers), 3) ** 0.5\n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            base_eta,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections,\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                        except:\n",
    "                            #                     continue\n",
    "                            print(\n",
    "                                \"no WER's in file\",\n",
    "                                get_test_file_from_stats_path(1, stats_file),\n",
    "                            )\n",
    "                            wers = [0, 0, 0]\n",
    "                            df_wer_mean = 0\n",
    "                            df_wer_stdev = 0\n",
    "                        # df = df.append(\n",
    "                        #     dict(\n",
    "                        #         zip(\n",
    "                        #             cols,\n",
    "                        #             [\n",
    "                        #                 speaker,\n",
    "                        #                 func,\n",
    "                        #                 base_eta,\n",
    "                        #                 etaScale,\n",
    "                        #                 target,\n",
    "                        #                 # accent_features,\n",
    "                        #                 # content_features,\n",
    "                        #                 # accent_similarity,\n",
    "                        #                 # content_similarity,\n",
    "                        #                 df_duration,\n",
    "                        #                 df_samples,\n",
    "                        #             ]\n",
    "                        #             + wers\n",
    "                        #             + [df_wer_mean, df_wer_stdev]\n",
    "                        #             + df_selections,\n",
    "                        #         )\n",
    "                        #     ),\n",
    "                        #     ignore_index=True,\n",
    "                        # )\n",
    "                        stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"base_eta\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            if not accent_features.endswith(\"_3rep\"):\n",
    "                continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mod_rep_csv_150_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            # if not accent_features.endswith(\"_3rep\"):\n",
    "            #     continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mix_query_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_percent(json_file):\n",
    "    with open(json_file) as file:\n",
    "        lines = file.readlines()\n",
    "    unq = set(lines)\n",
    "    return len(unq)/len(lines)\n",
    "\n",
    "\n",
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/speaker/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "# CMU_expts/speaker_without/ABA/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/run_1/\n",
    "# budget = 100\n",
    "\n",
    "## This cell is for the duplication report results\n",
    "\n",
    "\n",
    "budget = 150\n",
    "# target = 50\n",
    "ngram = 2\n",
    "target = 20\n",
    "# base_eta = \"423.28\"\n",
    "# etaScales = [\n",
    "#     \"0.1\",\n",
    "#     \"0.2\",\n",
    "#     \"0.3\",\n",
    "#     \"0.4\",\n",
    "#     \"0.5\",\n",
    "#     \"0.6\",\n",
    "#     \"0.7\",\n",
    "#     \"0.8\",\n",
    "#     \"0.9\",\n",
    "#     \"1.0\",\n",
    "#     \"2.0\",\n",
    "#     \"3.0\",\n",
    "#     \"4.0\",\n",
    "#     \"5.0\",\n",
    "#     \"6.0\",\n",
    "#     \"7.0\",\n",
    "#     \"8.0\",\n",
    "#     \"9.0\",\n",
    "#     \"10.0\",\n",
    "# ]\n",
    "\n",
    "# features = 'TRILL'\n",
    "csv_name = \"mod_report_{}_{}.csv\".format(budget, target)\n",
    "\n",
    "cols = [\n",
    "    \"speaker\",\n",
    "    \"function\",\n",
    "    \"etaScale\",\n",
    "    \"target\",\n",
    "    # \"accent_features\",\n",
    "    # \"content_features\",\n",
    "    # \"accent_similairty\",\n",
    "    # \"content_similarity\",\n",
    "    # \"g_kernel\",\n",
    "    # \"gq_kernel\",\n",
    "    # \"qq_kernel\",\n",
    "    \"duration\",\n",
    "    \"samples\",\n",
    "    \"WER-r1\",\n",
    "    \"WER-r2\",\n",
    "    \"WER-r3\",\n",
    "    \"WER-mean\",\n",
    "    \"WER-stdev\",\n",
    "    \"speakers\",\n",
    "    \"unique_percent\",\n",
    "]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "        continue\n",
    "    pick_from = \"all\"\n",
    "    if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "        if(len(function.split('_')) != 3): continue\n",
    "        # print(function, function.split('_'), len(function.split('_')))\n",
    "        (func, etaScale) = (function.split('_')[0], function.split('_')[2])\n",
    "        if etaScale != \"1.0\":\n",
    "            continue\n",
    "        for accent_features in get_dirs(\n",
    "            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "        ):\n",
    "            # if not accent_features.endswith(\"_3rep\"):\n",
    "            #     continue\n",
    "            for content_features in get_dirs(\n",
    "                f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "            ):\n",
    "                for kernel_type in get_dirs(\n",
    "                    f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "                ):\n",
    "                    for accent_similarity in get_dirs(\n",
    "                        f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}\"\n",
    "                    ):\n",
    "                        for content_similarity in get_dirs(\n",
    "                            f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}\"\n",
    "                        ):\n",
    "                            json_file = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/train.json\"\n",
    "                            unique_percent = get_unique_percent(json_file)\n",
    "                            stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{kernel_type}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "                            if not (os.path.isfile(stats_file_path)):\n",
    "                                continue\n",
    "                            stats_file = open(stats_file_path, \"r\")\n",
    "                            lines = stats_file.readlines()\n",
    "                            # print(\"lines length \", len(lines), lines)\n",
    "                            (\n",
    "                                total_selections,\n",
    "                                total_durations,\n",
    "                                speakered_selections,\n",
    "                                speakered_durations,\n",
    "                            ) = map(get_each_run, lines[:4])\n",
    "                            # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "                            sample_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_selections, total_selections)\n",
    "                                ]\n",
    "                            )\n",
    "                            sample_total = mean(total_selections)\n",
    "                            duration_frac = mean(\n",
    "                                [\n",
    "                                    x[0] / x[1]\n",
    "                                    for x in zip(speakered_durations, total_durations)\n",
    "                                ]\n",
    "                            )\n",
    "                            duration_total = mean(total_durations)\n",
    "                            df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "                                duration_total * duration_frac, duration_total\n",
    "                            )\n",
    "                            df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "                                sample_total * sample_frac, sample_total\n",
    "                            )\n",
    "                            df_selections = get_selection_counts(lines[4])\n",
    "\n",
    "                            wers = []\n",
    "                            for i in range(1, 4):\n",
    "                                try:\n",
    "                                    wers.append(WER_test_file(get_test_file_from_stats_path(i, stats_file)))\n",
    "                                except:\n",
    "                                    print(\n",
    "                                        \"no WER's in file\",\n",
    "                                        get_test_file_from_stats_path(1, stats_file),\n",
    "                                    )\n",
    "                            \n",
    "                            df_wer_mean = round(inf if len(wers) == 0  else mean(wers), 2)\n",
    "                            df_wer_stdev = round(inf if len(wers) <= 1 else variance(wers), 3) ** 0.5\n",
    "                            while(len(wers)<3): wers.append(0)\n",
    "                            # print(wers, speaker, func)\n",
    "                            \n",
    "                            df = df.append(\n",
    "                                dict(\n",
    "                                    zip(\n",
    "                                        cols,\n",
    "                                        [\n",
    "                                            speaker,\n",
    "                                            func,\n",
    "                                            etaScale,\n",
    "                                            target,\n",
    "                                            # accent_features,\n",
    "                                            # content_features,\n",
    "                                            # accent_similarity,\n",
    "                                            # content_similarity,\n",
    "                                            df_duration,\n",
    "                                            df_samples,\n",
    "                                        ]\n",
    "                                        + wers\n",
    "                                        + [df_wer_mean, df_wer_stdev]\n",
    "                                        + df_selections\n",
    "                                        + [unique_percent],\n",
    "                                    )\n",
    "                                ),\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                                # wers = [0, 0, 0]\n",
    "                                # df_wer_mean = 0\n",
    "                                # df_wer_stdev = 0\n",
    "                            # df = df.append(\n",
    "                            #     dict(\n",
    "                            #         zip(\n",
    "                            #             cols,\n",
    "                            #             [\n",
    "                            #                 speaker,\n",
    "                            #                 func,\n",
    "                            #                 base_eta,\n",
    "                            #                 etaScale,\n",
    "                            #                 target,\n",
    "                            #                 # accent_features,\n",
    "                            #                 # content_features,\n",
    "                            #                 # accent_similarity,\n",
    "                            #                 # content_similarity,\n",
    "                            #                 df_duration,\n",
    "                            #                 df_samples,\n",
    "                            #             ]\n",
    "                            #             + wers\n",
    "                            #             + [df_wer_mean, df_wer_stdev]\n",
    "                            #             + df_selections,\n",
    "                            #         )\n",
    "                            #     ),\n",
    "                            #     ignore_index=True,\n",
    "                            # )\n",
    "                            stats_file.close()\n",
    "df = df.sort_values(\n",
    "    by=[\n",
    "        \"speaker\",\n",
    "        # \"accent_features\",\n",
    "        # \"content_features\",\n",
    "        \"function\",\n",
    "        \"etaScale\",\n",
    "    ],\n",
    "    ascending=True,\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/\").is_dir()):\n",
    "#         continue\n",
    "#     pick_from = \"all\"\n",
    "#     if not (pathlib.Path(f\"./{speaker}/all/budget_{budget}/target_{target}/\").is_dir()):\n",
    "#         continue\n",
    "#     for function in get_dirs(f\"./{speaker}/all/budget_{budget}/target_{target}/\"):\n",
    "#         (func, base_eta, etaScale) = split_function(function)\n",
    "#         for accent_features in get_dirs(\n",
    "#             f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/\"\n",
    "#         ):\n",
    "#             for content_features in get_dirs(\n",
    "#                 f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}\"\n",
    "#             ):\n",
    "#                 for accent_similarity in get_dirs(\n",
    "#                     f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/\"\n",
    "#                 ):\n",
    "#                     for content_similarity in get_dirs(\n",
    "#                         f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}\"\n",
    "#                     ):\n",
    "#                         stats_file_path = f\"./{speaker}/all/budget_{budget}/target_{target}/{function}/{accent_features}/{content_features}/{accent_similarity}/{content_similarity}/stats.txt\"\n",
    "#                         if not (os.path.isfile(stats_file_path)):\n",
    "#                             continue\n",
    "#                         stats_file = open(stats_file_path, \"r\")\n",
    "#                         lines = stats_file.readlines()\n",
    "#                         # print(\"lines length \", len(lines), lines)\n",
    "#                         (\n",
    "#                             total_selections,\n",
    "#                             total_durations,\n",
    "#                             speakered_selections,\n",
    "#                             speakered_durations,\n",
    "#                         ) = map(get_each_run, lines[:4])\n",
    "#                         # print(total_selections, total_durations, speakered_selections, speakered_durations)\n",
    "#                         sample_frac = mean(\n",
    "#                             [\n",
    "#                                 x[0] / x[1]\n",
    "#                                 for x in zip(speakered_selections, total_selections)\n",
    "#                             ]\n",
    "#                         )\n",
    "#                         sample_total = mean(total_selections)\n",
    "#                         duration_frac = mean(\n",
    "#                             [\n",
    "#                                 x[0] / x[1]\n",
    "#                                 for x in zip(speakered_durations, total_durations)\n",
    "#                             ]\n",
    "#                         )\n",
    "#                         duration_total = mean(total_durations)\n",
    "#                         df_duration = \"{:.2f}/{:.2f}\".format(\n",
    "#                             duration_total * duration_frac, duration_total\n",
    "#                         )\n",
    "#                         df_samples = \"{:.2f}/{:.2f}\".format(\n",
    "#                             sample_total * sample_frac, sample_total\n",
    "#                         )\n",
    "#                         df_selections = get_selection_counts(lines[4])\n",
    "#                         try:\n",
    "#                             wers = [\n",
    "#                                 WER_test_file(\n",
    "#                                     get_test_file_from_stats_path(i, stats_file)\n",
    "#                                 )\n",
    "#                                 for i in range(1, 4)\n",
    "#                             ]\n",
    "#                             df_wer_mean = round(mean(wers), 2)\n",
    "#                             df_wer_stdev = round(variance(wers), 3) ** 0.5\n",
    "#                             df = df.append(\n",
    "#                                 dict(\n",
    "#                                     zip(\n",
    "#                                         cols,\n",
    "#                                         [\n",
    "#                                             speaker,\n",
    "#                                             func,\n",
    "#                                             base_eta,\n",
    "#                                             etaScale,\n",
    "#                                             target,\n",
    "#                                             # accent_features,\n",
    "#                                             # content_features,\n",
    "#                                             # accent_similarity,\n",
    "#                                             # content_similarity,\n",
    "#                                             df_duration,\n",
    "#                                             df_samples,\n",
    "#                                         ]\n",
    "#                                         + wers\n",
    "#                                         + [df_wer_mean, df_wer_stdev]\n",
    "#                                         + df_selections,\n",
    "#                                     )\n",
    "#                                 ),\n",
    "#                                 ignore_index=True,\n",
    "#                             )\n",
    "#                         except:\n",
    "#                             #                     continue\n",
    "#                             print(\n",
    "#                                 \"no WER's in file\",\n",
    "#                                 get_test_file_from_stats_path(1, stats_file),\n",
    "#                             )\n",
    "#                             wers = [0, 0, 0]\n",
    "#                             df_wer_mean = 0\n",
    "#                             df_wer_stdev = 0\n",
    "#                         # df = df.append(\n",
    "#                         #     dict(\n",
    "#                         #         zip(\n",
    "#                         #             cols,\n",
    "#                         #             [\n",
    "#                         #                 speaker,\n",
    "#                         #                 func,\n",
    "#                         #                 base_eta,\n",
    "#                         #                 etaScale,\n",
    "#                         #                 target,\n",
    "#                         #                 # accent_features,\n",
    "#                         #                 # content_features,\n",
    "#                         #                 # accent_similarity,\n",
    "#                         #                 # content_similarity,\n",
    "#                         #                 df_duration,\n",
    "#                         #                 df_samples,\n",
    "#                         #             ]\n",
    "#                         #             + wers\n",
    "#                         #             + [df_wer_mean, df_wer_stdev]\n",
    "#                         #             + df_selections,\n",
    "#                         #         )\n",
    "#                         #     ),\n",
    "#                         #     ignore_index=True,\n",
    "#                         # )\n",
    "#                         stats_file.close()\n",
    "# df = df.sort_values(\n",
    "#     by=[\n",
    "#         \"speaker\",\n",
    "#         # \"accent_features\",\n",
    "#         # \"content_features\",\n",
    "#         \"function\",\n",
    "#         \"base_eta\",\n",
    "#         \"etaScale\",\n",
    "#     ],\n",
    "#     ascending=True,\n",
    "#     ignore_index=True,\n",
    "# )\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [\n",
    "    \"assamese_female_english\",\n",
    "    \"manipuri_female_english\",\n",
    "    \"kannada_male_english\",\n",
    "    \"rajasthani_male_english\",\n",
    "    \"hindi_male_english\",\n",
    "    \"malayalam_male_english\",\n",
    "    \"tamil_male_english\",\n",
    "    \"gujarati_female_english\",\n",
    "]\n",
    "\n",
    "for speaker in speakers:\n",
    "    selection_json = f\"./{speaker}/selection.json\"\n",
    "    print(speaker, end=\" \")\n",
    "    with open(selection_json) as file:\n",
    "        print(len(file.readlines()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('error')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc33b118a8b882057d92ab3e840283c71bfc0408e638fa49ffb4a6668b810896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
