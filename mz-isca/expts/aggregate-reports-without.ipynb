{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import re, os, csv, pathlib\n",
    "import pandas as pd\n",
    "from statistics import mean, variance\n",
    "accents = ['african', 'indian', 'hongkong', 'philippines', \n",
    "           'england', 'scotland', 'ireland', 'australia', \n",
    "           'canada', 'us', \n",
    "           'bermuda', 'southatlandtic', 'wales', 'malaysia']\n",
    "accent_short_forms = {'hongkong':'hk', \"african\":\"afr\", \"philippines\":\"phil\", \"indian\":\"ind\",  \n",
    "                      \"england\":\"eng\", 'scotland':'sco', 'ireland':'ire',\n",
    "                      \"us\":\"us\", \"canada\":\"can\", \"australia\":\"aus\",\n",
    "                      'bermuda':'ber', 'southatlandtic':'satl', 'wales':'wal', 'malaysia':'mal'\n",
    "                     }\n",
    "\n",
    "def replace_with_short_forms(s):\n",
    "    for key, value in accent_short_forms.items():\n",
    "        s = s.replace(key, value)\n",
    "    return s\n",
    "\n",
    "def last_name(pth):\n",
    "    return pathlib.PurePath(pth).name\n",
    "\n",
    "def get_dirs(pth):\n",
    "    return [last_name(f.name) for f in os.scandir(pth) if f.is_dir()]\n",
    "\n",
    "def get_each_run(lne):\n",
    "    return list(map(float, re.findall(': (.+?) -> ', lne)[0].split(' ')))\n",
    "\n",
    "def get_selection_counts(s):\n",
    "    return list(map(replace_with_short_forms, re.findall('Counter\\\\((.+?)\\\\)', s)))\n",
    "\n",
    "def get_test_file_from_stats_path(run_number, stats_file_opened):\n",
    "    return stats_file_opened.name[:-9]+\"run_{}/output/test_infer_log.txt\".format(run_number)\n",
    "\n",
    "def WER_test_file(test_file):\n",
    "    txt_file = open(test_file, 'r')\n",
    "    lines = txt_file.readlines()\n",
    "    matched = \"\"\n",
    "    for line in lines:\n",
    "        if \"==========>>>>>>Evaluation Greedy WER: \" in line:\n",
    "            txt_file.close()\n",
    "            return float(line.rstrip().split(\": \")[1])\n",
    "    txt_file.close()\n",
    "    return \"\"\n",
    "\n",
    "def get_eta(func, eta):\n",
    "    if func != \"FL1MI\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return \"-n:\"+str(float(eta[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 500\n",
    "target = 20\n",
    "features = 'w2v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_path = 'Error-Driven-ASR-Personalization/CMU_expts/accent/hindi/manifests/TSS_output/all/budget_100/target_50/FL1MI/eta_1.0/euclidean/39/stats.txt'\n",
    "cols = ['accent', 'ground', 'function', 'feature', 'duration', 'samples', \n",
    "        'WER-r1', 'WER-r2', 'WER-r3', 'WER-mean', 'WER-var', 'accents-run_1', 'accents-run_2', 'accents-run_3']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "# accents = ['canada', 'england', 'australia', 'us', 'indian', 'philippines', 'african']\n",
    "accents = ['african', 'indian', 'hongkong', 'philippines', \n",
    "           'england', 'scotland', 'ireland', 'australia', \n",
    "           'canada', 'us', \n",
    "           'bermuda', 'southatlandtic', 'wales', 'malaysia']\n",
    "cnt = 0\n",
    "pretrain_added = dict([(accent, False) for accent in accents])\n",
    "for accent in accents:\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/'.format(accent)).is_dir()):\n",
    "        continue\n",
    "    pretrain_log = '{}/manifests/TSS_output/test_infer_log.txt'.format(accent)\n",
    "    pick_from = 'without'\n",
    "    if not(pathlib.Path('./{}/manifests/TSS_output/{}/budget_{}/target_{}/'.format(accent, pick_from, budget, target)).is_dir()):\n",
    "        continue\n",
    "    for function in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/'.format(accent, pick_from, budget, target)):\n",
    "        if function == \"random\":\n",
    "            stats_file_path = './{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/stats.txt'.format(accent, pick_from, budget, \n",
    "                                                                                                                target, function)                                                                                  \n",
    "            if not(os.path.isfile(stats_file_path)):\n",
    "                continue\n",
    "            stats_file = open(stats_file_path, 'r')\n",
    "            lines = stats_file.readlines()\n",
    "            total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "            sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "            sample_total = mean(total_selections)\n",
    "            duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "            duration_total = mean(total_durations)\n",
    "            df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "            df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "            df_selections = get_selection_counts(lines[5])\n",
    "            try:\n",
    "                wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "                df_wer_mean = round(mean(wers), 2)\n",
    "                df_wer_var = round(variance(wers), 3)\n",
    "            except:\n",
    "                continue\n",
    "                print(\"no WER's in file\", get_test_file_from_stats_path(1, stats_file))\n",
    "                wers = [0,0,0]\n",
    "                df_wer_mean = 0\n",
    "                df_wer_var = 999\n",
    "            df = df.append(dict(zip(cols, [accent, pick_from, function, \"NA\", df_duration, df_samples]+wers\n",
    "                                    +[df_wer_mean, df_wer_var] + df_selections)), ignore_index=True)\n",
    "            stats_file.close()\n",
    "            continue\n",
    "#         for eta in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/'.format(accent, pick_from, budget, target, function)):\n",
    "#             for similarity in get_dirs('./{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/{}/'.format(accent, pick_from, \n",
    "#                                                                                                     budget, target, function, eta)):\n",
    "#                 cnt += 1\n",
    "#                 stats_file_path = './{}/manifests/TSS_output/{}/budget_{}/target_{}/{}/{}/{}/{}/stats.txt'.format(accent, pick_from, budget, \n",
    "#                                                                                                             target, function, eta, similarity, \n",
    "#                                                                                                             features)                                                                                  \n",
    "#                 if not(os.path.isfile(stats_file_path)):\n",
    "#                     continue\n",
    "#                 stats_file = open(stats_file_path, 'r')\n",
    "#                 lines = stats_file.readlines()\n",
    "#                 total_selections, total_durations, accented_selections, accented_durations = map(get_each_run, lines[:4])\n",
    "#                 sample_frac = mean([x[0]/x[1] for x in zip(accented_selections, total_selections)])\n",
    "#                 sample_total = mean(total_selections)\n",
    "#                 duration_frac = mean([x[0]/x[1] for x in zip(accented_durations, total_durations)])\n",
    "#                 duration_total = mean(total_durations)\n",
    "#                 df_duration = \"{:.2f}/{:.2f}\".format(duration_total*duration_frac, duration_total)\n",
    "#                 df_samples = \"{:.2f}/{:.2f}\".format(sample_total*sample_frac, sample_total)\n",
    "#                 df_selections = get_selection_counts(lines[5])\n",
    "#                 try:\n",
    "#                     wers = [WER_test_file(get_test_file_from_stats_path(i, stats_file)) for i in range(1,4)]\n",
    "#                     df_wer_mean = round(mean(wers), 2)\n",
    "#                     df_wer_var = round(variance(wers), 3)\n",
    "#                 except:\n",
    "#                     continue\n",
    "#                     print(\"no WER's in file\", get_test_file_from_stats_path(1, stats_file))\n",
    "#                     wers = [0,0,0]\n",
    "#                     df_wer_mean = 0\n",
    "#                     df_wer_var = 999\n",
    "#                 df = df.append(dict(zip(cols, [accent, pick_from, function+get_eta(function, eta), features, df_duration, df_samples]+wers\n",
    "#                                         +[df_wer_mean, df_wer_var] + df_selections)), ignore_index=True)\n",
    "#                 if not(pretrain_added[accent]) and os.path.isfile(pretrain_log):\n",
    "#                     pretrain_added[accent] = True\n",
    "#                     pre_WER = WER_test_file(pretrain_log)\n",
    "#                     df = df.append(dict(zip(cols, [accent, \"pretrain\", '', '', '', '']+['-']*3+[pre_WER, ''])), ignore_index=True)\n",
    "#                 stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accent</th>\n",
       "      <th>ground</th>\n",
       "      <th>function</th>\n",
       "      <th>feature</th>\n",
       "      <th>duration</th>\n",
       "      <th>samples</th>\n",
       "      <th>WER-r1</th>\n",
       "      <th>WER-r2</th>\n",
       "      <th>WER-r3</th>\n",
       "      <th>WER-mean</th>\n",
       "      <th>WER-var</th>\n",
       "      <th>accents-run_1</th>\n",
       "      <th>accents-run_2</th>\n",
       "      <th>accents-run_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [accent, ground, function, feature, duration, samples, WER-r1, WER-r2, WER-r3, WER-mean, WER-var, accents-run_1, accents-run_2, accents-run_3]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sort_values(by=['accent', 'feature', 'ground', 'function'], ascending=True, ignore_index=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total selection : 100 100 100 -> 100.00\n",
    "# total selection duration: 357.0149433106577 357.0149433106577 357.0149433106577 -> 357.01\n",
    "# accented selection: 76 76 76 -> 76.00\n",
    "# accented duration: 254.74947845804974 254.74947845804974 254.74947845804974 -> 254.75\n",
    "\n",
    "# all selections: [Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3}), Counter({'hindi': 76, 'korean': 8, 'spanish': 7, 'arabic': 3, 'chinese': 3, 'vietnamese': 3})]\n",
    "\n",
    "#Evaluation Greedy WER: 16.19\n",
    "\n",
    "df.to_csv(\"without_{}_{}_{}.csv\".format(budget, target, features), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dss",
   "language": "python",
   "name": "dss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
